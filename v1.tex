\documentclass{article}

\input{preamble}

\begin{document}

\title{Notes on various topics}

\author{Zijun Zhao}

\date{\today}
\maketitle
\tableofcontents
\section{Odds and ends}
\begin{definition}[The antisymmetry of $n$-body operator elements]
    A normal-ordered $n$-body operator $\hat{O}$ is defined, in the most general way, as 
    \begin{equation}
        \hat{O}=\frac{1}{n!}\sum_{pqr\dots}\sum_{srt\dots}O^{pqr\dots}_{srt\dots}\{\hat{a}^{srt\dots}_{prs\dots} \}.
    \end{equation}
    In this definition, the prefactor of $1/n!$ arises from the fact that the integral
    \begin{equation}
        \int \phi_s^*(1)\phi_r^*(2)\phi_t^*(3)\dots \hat{O} \phi_p(1)\phi_q(2)\phi_r(3)\dots\dl \tau
    \end{equation}
    must be invariant with respect to exchange of dummy indices. This is simply a universal property of integrals, and has nothing to do with normal ordering or Fermionic antisymmetry.
\end{definition}

\section{Gaussian Processes}
\subsection{Multivariate normal distribution}
(Adapted from \href{http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote15.html}{here}.) A multivariate Gaussian random variable with $k$ variables is denoted by $\bvec{F}\sim\mathcal{N}_k(\bvec{\mu},\bvec{\Sigma})$, with $\bvec{\mu}$ being the vector of means and $\bvec{\Sigma}$ being the covariance matrix, which is yet to be defined. The probability density function is given by
\begin{equation}
P(\bvec{f};\bvec{\mu},\bvec{\Sigma})=\frac{1}{\sqrt{(2\pi)^{k}\det(\bvec{\Sigma})}}\exp\left( -\frac{1}{2}(\bvec{f}-\bvec{\mu})^{\mathrm{T}}\bvec{\Sigma}^{-1}(\bvec{f}-\bvec{\mu}) \right).
\end{equation}
Henceforth the vectors and matrices will no longer be boldface.

The problem Gaussian processes try to solve is fitting continuous functions to a finite set of data points. Instead of giving one possible interpolation, as is done with classical polynomial/spline interpolations, Gaussian processes conduct regression by defining a distribution over \textit{all possible} continuous functions that fit the input data points.

Some properties of the multivariate normal distribution are highly desirable, they can be summarized as `once Gaussian, always Gaussian':
\begin{enumerate}
    \item \textbf{Normalization}:
    \begin{equation}
        \int_y P(y;\mu,\Sigma)\dl y=1
    \end{equation}
    \item \textbf{Marginalization}: The marginal distributions are Gaussian:
    \begin{align}
        &P(y_a)=\int_{y_b}P(y_a,y_b;\mu,\Sigma)\dl y_b,\ y_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})\\
        &P(y_b)=\int_{y_a}P(y_a,y_b;\mu,\Sigma)\dl y_a,\ y_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})
    \end{align}
    \item \textbf{Summation}: If $y\sim\mathcal{N}(\mu,\Sigma)$ and $y'\sim\mathcal{N}(\mu',\Sigma')$ then
    \begin{equation}
        y+y'\sim\mathcal{N}(\mu+\mu',\Sigma+\Sigma')
    \end{equation}
    \item \textbf{Conditioning}: The conditional distribution is also Gaussian:
    \begin{align}
        P(y_a|y_b)=\frac{P(y_a,y_b;\mu,\Sigma)}{\int_{y_a}P(y_a,y_b;\mu,\Sigma)\dl y_a}\\
        y_a|y_b\sim\mathcal{N}(\mu_a+\Sigma_{ab}\Sigma_{bb}^{-1}(y_b-\mu_b),\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}).
    \end{align}
    For derivation of the above result, see \cite[Ch. 2.3.1]{bishopPatternRecognitionMachine2006}
\end{enumerate}

\subsection{Models, estimation, and Bayes' rule}
For a model parameterized by $\theta$ that attempts to explain data $\mathcal{D}$, Bayes' rule is given by
\begin{equation}
\underbrace{P(\theta|\mathcal{D})}_{\mathrm{posterior}}=\frac{P(\mathcal{D}|\theta)P(\theta)}{P(\mathcal{D})}\propto \underbrace{P(\mathcal{D}|\theta)}_{\mathrm{likelihood}}\underbrace{P(\theta)}_{\mathrm{prior}},
\end{equation}
where the denominator can be seen as a normalization factor and is not usually important.

There are two types of estimations:
\begin{enumerate}
    \item Maximum likelihood estimation (MLE): choose value that maximizes the probability of observed data:
    \begin{equation}
        \hat{\theta}_{\mathrm{MLE}}=\argmax_{\theta} P(\mathcal{D}|\theta).
    \end{equation}
    This is a frequentist estimation method, where likelihood `speaks for itself'.
    \item Maximum \textit{a posteriori} (MAP) estimation: choose value that is most probable given observed data \textit{and prior belief}:
    \begin{equation}
        \hat{\theta}_{\mathrm{MAP}}=\argmax_{\theta}P(\theta|\mathcal{D})=\argmax_{\theta} P(\mathcal{D}|\theta)P(\theta).
    \end{equation}
    This is a Bayesian estimation method, where prior beliefs, usually informed by domain knowledge of parameters, regularize the point estimate.
\end{enumerate}

\subsection{Motivating Gaussian processes}
The flow of MLE/MAP is $\mathcal{D}\rightarrow \theta\rightarrow y=\theta^{\mathrm{T}}x$. Why do we go through the model again? Well, we don't have to. We can model the prediction of a text point $x$ directly, by modelling $P(y|x,\mathcal{D})$:
\begin{equation}
P(y|x,\mathcal{D})=\int_{\theta}P(y|x,\theta)P(\theta|\mathcal{D})\dl\theta,
\end{equation}
where we marginalized out the dependence of the model. If $P(\theta)$ is Gaussian (this is a choice of prior!), then each term in the integrand is Gaussian, and consequently the LHS is also Gaussian. 

What this shows is profound - now we don't have to commit to any single model, we can `integrate' over an infinite number of them, but only the sensible ones make significant contributions.

Now, since we know that $P(y|x,\mathcal{D})$ is Gaussian, we don't have to explicitly do the integration, we can just fit the distribution of $P(y|x,\mathcal{D})\sim\mathcal{N}(\mu,\Sigma)$. 

It is \textbf{very important to emphasize} here that this Gaussian will be of $(M+N)$ variables, where $M$ is the size of the test set $x$, and $N$ is the size of the training set $\mathcal{D}$. This means that given the data, we can do an $N$-dimensional slice on the Gaussian, giving an $M$-dimensional distribution on the estimates $y$.

The covariance matrix is a measure of correlation. If two inputs are close: say my neighbor's house and my house, then the two labels/outputs should be close: say the selling price. The diagonals of the covariance matrix reflect the inherent uncertainty in the label of each input, in this case, the house prices are each subject to some fluctuations. The mean can be set to $0$ wlog, so the covariance function is the central object here.

The covariance function should return a large value if two inputs are close: this is satisfied by kernel functions.

\subsection{Gaussian process regression}
\begin{equation}
P(y|x,\mathcal{D})\sim\mathcal{N}(0,\Sigma),\ \Sigma=K,\ K_{ij}=k(x_i,x_j)
\end{equation}
Prediction:
\begin{equation}
    P(y_+|\mathcal{D},x_+)\sim\mathcal{N}(K_*^{\mathrm{T}}K^{-1}y,K_{**}-K_*K^{-1}K_*),
\end{equation}
where
\begin{equation}
    \Sigma=\ttmat{K}{K_*}{K_*^{\mathrm{T}}}{K_{**}},
\end{equation}
sorted training-test superblocks.

\section{Linear algebra}
\subsection{Singular value decomposition}
The SVD can be motivated by a geometric observation:
\begin{theorem}
    The image of the unit sphere $S$ under any $m\times n$ matrix $A$ is a hyperellipse. 
\end{theorem}
This means that the unit sphere will be stretched by some factors $\sigma_1,\dots,\sigma_m$ (if $m\geq n$ then at most $n$ of these will be nonzero) in some orthonormal directions $u_1,\dots,u_m$. The vectors $\{ \sigma_iu_i \}$ are then the principal semiaxes of the hyperellipse.
\begin{definition}
    Assuming $m\geq n$, and that $A$ has full rank $n$, we now define the following
    \begin{itemize}
        \item The lengths of the principal semiaxes, $\sigma_1,\dots,\sigma_n$, are the $n$ \textbf{singular values} of $A$, numbered in descending order, $\sigma_1\geq\sigma_2\geq\dots\geq\sigma_n>0$.
        \item The unit vectors in the directions of the principal semiaxes of $AS$ are the $n$ \textbf{left singular vectors} of $A$.
        \item The preimages of the principal semiaxes of $AS$ such that $Av_j=\sigma_ju_j$, are the $n$ \textbf{right singular vectors} of $A$.
    \end{itemize}
\end{definition}

Since the left and right singular vectors can be related by
\begin{equation}
Av_j=\sigma_ju_j,\ \ 1\leq j\leq n,
\end{equation}
we can collect all $n$ of these equations to write
\begin{equation}
AV=\hat{U}\hat{\Sigma},\ A\in\mathbb{R}^{m\times n},V\in\mathbb{R}^{n\times n},\hat{U}\in\mathbb{R}^{m\times n},\hat{\Sigma}\in\mathbb{R}^{n\times n},
\end{equation}
the hats represent the fact that these are the `reduced' form of SVD. Similarly, we can also write $A\adj U=V\Sigma\tp$ Since $V$ has orthonormal columns, it is unitary, and we can obtain the \textit{reduced singular value decomposition}:
\begin{definition}[Reduced SVD]
\label{def:reduced-svd}
\begin{equation}
    A=\hat{U}\hat{\Sigma}V\adj.
\end{equation}
\end{definition}

The full SVD can be defined by extending the columns of $\hat{U}$ with $m-n$ additional orthonormal columns, making it unitary as well. At the same time, $\Sigma$ needs to be extended downwards with zeros to fit the dimensions without changing the product. So we reach the definition of the full SVD:
\begin{definition}[Full SVD]
\begin{equation}
\label{eq:full-svd}
    A=U\Sigma V\adj,
\end{equation}
where 
\begin{align*}
    &A\in\mathbb{C}^{m\times n}\text{ not necessarily of full rank},\\
    &V\in\mathbb{C}^{n\times n}\text{ is unitary},\\
    &U\in\mathbb{C}^{m\times m}\text{ is unitary},\\
    &\Sigma\in\mathbb{R}^{m\times n}\text{ is diagonal}.
\end{align*}
\end{definition}
We have relaxed the requirement that $A$ be of full rank, as the singular values can be $0$, and the singular vectors can then be arbitrary as long as they are still orthonormal.
\begin{theorem}
    Every matrix $A\in\mathbb{C}^{m\times n}$ has a SVD, and the singular values are uniquely defined.
\end{theorem}
\begin{proof}
    
\end{proof}

We now see that this definition suits the geometrical observation at the start: $V\adj$ rotates the sphere, $\Sigma$ stretches the sphere along the directions of the canonical basis into a hyperellipse, and $U$ again rotates or reflects the hyperellipse.

For matrices where $m<n$, we can just compute the SVD of $A\tp$:
\begin{equation}
    A\tp=U\Sigma V\adj\Rightarrow A=V\Sigma\tp U\adj.
\end{equation}

\begin{corollary}
    ``Every matrix is diagonal''.
\end{corollary}
\begin{proof}
    For any two vectors $b\in\mathbb{C}^m$ and $x\in\mathbb{C}^n$ related by $b=Ax$, we can expand them in the left and right singular vectors of $A$ respectively, which gives
    \begin{equation}
        b'=U\adj b,\ \ x'=V\adj x.
    \end{equation}
    So we can re-express the relation $b=Ax$ as
    \begin{equation}
    b=Ax \Leftrightarrow U\adj b=U\adj Ax=U\adj U\Sigma V\adj x \Leftrightarrow b'=\Sigma x'.
    \end{equation}
\end{proof}
The above means we can solve a linear system $Ax=b$ as
\begin{equation}
    \Sigma x'=b',\ x=Vx',
\end{equation}
the first is a diagonal system and can be trivially solved in $\mathcal{O}(N)$ time.

The importance of the SVD can be further appreciated by its deep connections to some fundamental topics of linear algebra, some of which we prove below. In the following, we assume $A$ is a $m\times n$ matrix, $p=\min(m,n)$, $r\leq p$ is the number of nonzero singular values of $A$, and $\bra x,y,\dots,z\ket$ is the space spanned by $x,y,\dots,z$.

\begin{theorem}
    The rank of $A$ is equal to the number of nonzero singular values of $A$, \textit{i.e.}, $r$.
\end{theorem}
\begin{proof}
    The rank of $\Sigma$ is $r$, and since $U$ and $V$ are orthonormal, and therefore of full rank, $\rank(A)=\rank(\Sigma)=r$.
\end{proof}

\begin{theorem}
\label{theo:norm-and-singval}
$||A||_2=\sigma_1$ and $||A||_{\mathrm{F}}=\sqrt{\sigma_1^2+\sigma_2^2+\dots+\sigma_r^2}$.
\end{theorem}
\begin{proof}
    \hl{Come back when you understand it}\cite[p. 34]{trefethenNumericalLinearAlgebra2022}
\end{proof}

\begin{theorem}
    The nonzero singular values of $A$ are the square roots of the the nonzero eigenvalues of $A\adj A$ or $AA\adj$ (despite having different dimensions, they have the same nonzero eigenvalues). 
\end{theorem}
\begin{proof}
    Since
    \begin{equation}
        A\adj A=(U\Sigma V\adj)\adj(U\Sigma V\adj)=V\Sigma\tp U\adj U\Sigma V\adj=V(\Sigma\tp\Sigma)V\adj,
    \end{equation}
    we conclude that $A\adj A$ is similar to $\Sigma\adj\Sigma$ and hence has the same $n$ eigenvalues. The eigenvalues of $\Sigma\adj\Sigma$, which is diagonal, are simply the diagonal entries $\sigma_1^2,\dots,\sigma_p^2$, with $n-p$ additional zero eigenvalues. Similarly,
    \begin{equation}
        AA\adj=U(\Sigma\Sigma\tp) U\adj.
    \end{equation}
\end{proof}
Some of the main differences between eigendecomposition and SVD are
\begin{itemize}
    \item An eigenedecomposition 
    \begin{equation}
        A = SDS^{-1}
    \end{equation}
    exists only for all \href{https://en.wikipedia.org/wiki/Defective_matrix}{non-defective matrices}, whereas an SVD exists for all matrices.
    \item Eigenvectors are linearly independent (orthogonal when Hermitian/symmetric/normal), whereas the left and right SVD vectors are strictly orthonormal.
    \item Eigenvalues are not sorted, whereas singular values are sorted
    \item EVD are usually important in studying matrix powers, whereas SVD is more relevant for studying the behaviour of the matrix itself.
    \end{itemize}
The commonality is in
\begin{theorem}
    For $A\in\mathbb{R}^{m\times m}$ is symmetric, then singular values of $A$ are absolute values of eigenvalues of $A$.
\end{theorem}
\begin{proof}
    The EVD of $A$ is
    \begin{equation}
        A=Q\Lambda Q\tp,
    \end{equation}
    where $Q$ is orthogonal and $\Lambda=\diag{\lambda_1,\dots,\lambda_n}$, so we can write
    \begin{equation}
        A=Q\begin{bmatrix}
            |\lambda_1| & & \\
            &          \ddots & \\
            & & |\lambda_n|
        \end{bmatrix}
        \begin{bmatrix}
            \sign(\lambda_1) & & \\
            &          \ddots & \\
            & & \sign(\lambda_n)
        \end{bmatrix}Q\tp=Q\Sigma V\tp.
        ,
    \end{equation}
\end{proof}

\begin{theorem}
    If $A$ is unitary, then its singular values are the absolute values of the eigenvalues of $A$.
\end{theorem}
\begin{proof}
    We can write the eigendecomposition of the unitary $A$ as 
    \begin{equation}
        A=Q\Lambda Q\adj=Q|\Lambda|\sign(\Lambda)Q\adj\equiv U\Sigma V\adj.
    \end{equation}
    Since $\sign(\Lambda)Q\adj$ is unitary, the last equivalence means that this is a valid SVD.
\end{proof}

\begin{theorem}
    For $A\in\mathbb{C}^{m\times m}$, $|\det(A)|=\prod_{i=1}^m\sigma_i$.
\end{theorem}
\begin{proof}
    The determinant of a unitary matrix is always $1$, since
    \begin{equation}
        |\det(U\adj U)|=|\det(U\adj)||\det(U)|=|\det(U)|^*|\det(U)|=||\det(U)||^2\equiv 1.
    \end{equation}
    So we can write,
    \begin{equation}
        |\det(A)|=|\det(U\Sigma V\adj)|=|\det(U)||\det(\Sigma)||\det(V)|^*=|\det(\Sigma)|=\prod_{i=1}^m\sigma_i.
    \end{equation}
\end{proof}

\begin{theorem}[Low-rank representation]
    $A$ is the sum of $r$ rank-one matrices:
    \begin{equation}
        A=\sum_{j=1}^r\sigma_j u_jv_j\adj,
    \end{equation}
    noting that $u_jv_j\adj$ are outer products, and each has rank $1$.
\end{theorem}
\begin{proof}
    If we write $\Sigma$ as a sum of $r$ matrices $\Sigma_j=\diag(0,\dots,\sigma_j,\dots,0)$, then \Cref{eq:full-svd} gives the above expression.
\end{proof}
This is one of the many possible ways in which $A$ can be expressed in terms of rank-one matrices, but this specific way ensures that each partial sum captures as much of the `energy' of $A$ as possible, either in the $2$-norm sense or the Frobenius norm sense. To make it precise, we introduce the problem of best approximation of a matrix $A$ by matrices of lower rank:
\begin{theorem}[Eckart-Young]
    For any $\nu$ with $0\leq\nu\leq r$, define
    \begin{equation}
        A_{\nu}=\sum_{j=1}^{\nu}\sigma_j u_jv_j\adj.
    \end{equation}
    If $\nu=p$, define $\sigma_{\nu+1}=0$. Then
    \begin{align}
        &||A-A_{\nu}||_2=\inf_{\substack{B\in\mathbb{C}^{m\times n}\\ \rank(B)\leq\nu}}||A-B||_2=\sigma_{\nu+1}\\
        &||A-A_{\nu}||_{\mathrm{F}}=\inf_{\substack{B\in\mathbb{C}^{m\times n}\\ \rank(B)\leq\nu}}||A-B||_{\mathrm{F}}=\sqrt{\sigma_{\nu+1}^2+\dots+\sigma_r^2}.
    \end{align}
    That is to say, the truncated SVD approximation is the best approximation to a matrix at a given (truncated) rank.
\end{theorem}
\begin{proof}
From \Cref{theo:norm-and-singval}, we can write
    \begin{equation}
        ||A-A_{\nu}||_{\mathrm{F}}=\sqrt{\sigma_{\nu+1}^2+\dots+\sigma^2_{\min(m,n)}}
    \end{equation}
    \hl{Come back when you understand it}\cite[p. 36]{trefethenNumericalLinearAlgebra2022}
\end{proof}
We can therefore proceed to define low-rank approximations by progressively omitting terms with small $\sigma_i$'s from the summation.

\begin{theorem}
    Every SVD is `essentially unique'.
\end{theorem}
\begin{proof}
    Suppose
    \begin{equation}
        A=U\Sigma V\adj\ \ \mathrm{and}\ \ A=U'\Sigma' (V')\adj
    \end{equation}
    be SVDs of $A$. Then
    \begin{equation}
        \Sigma=\Sigma'\ \ \mathrm{and}\ \ V'=VQ,
    \end{equation}
    where $Q$ is orthogonal and $\sigma_i\neq\sigma_j$ implies $q_{ij}=0$. A similar statement holds for $U$. This means that the singular values are unique, but the singular value $\sigma_i$ does not repeat, then $v_i=\pm v_i$ and $u_i=\pm u_i$. \hl{this is not a proof}.
\end{proof}

\begin{theorem}[Fundamental theorem of linear algebra]
    Let $A\in\mathbb{R}^{m\times n}$, then
    \begin{align}
        \nulll(A)=\range(A\tp)^{\perp}&\ \ \ \\
        \dim(\range(A\tp))+\dim(\nulll(A))=n&\ \ \ \\\
        \mathbb{R}^n=\range(A\tp)\oplus\nulll(A)&\ \ \ 
    \end{align}
\end{theorem}
\begin{proof}
    Let's perform an SVD on $A$:
    \begin{equation}
        A = U\Sigma V\adj,
    \end{equation}
    then $\{v_i\}_{i=1}^r$ forms an orthonormal basis for $\range(A\tp)$, $\{v_i\}_{i=r+1}^{n}$ forms an ONB for $\nulll(A)$, $\{u_i\}_{i=1}^r$ forms an orthonormal basis for $\range(A)$, $\{u_i\}_{i=r+1}^{m}$ forms an ONB for $\nulll(A\tp)$.

\end{proof}

\begin{theorem}
    The Frobenius norm of $A$ is the sum of squared singular values.
\end{theorem}
\begin{proof}
\begin{equation}
    ||A||^2_{\mathrm{F}}=\Tr(A\adj A)=\Tr(V\Sigma\tp\Sigma V\adj)=\Tr(\Sigma\tp\Sigma)=\sum_i^r\sigma_i^2.
\end{equation}
\end{proof}

\begin{theorem}
    The $2$-norm of $A$ is the square of the largest singular value.
\end{theorem}
\begin{proof}
    \begin{align}
        ||A||_2^2&=||U\Sigma V\adj||_2^2=||\Sigma||_2^2=\max_{x\neq 0}\frac{||\Sigma x||_2^2}{||x||_2^2}\\
        &=\max_{x\neq 0}\frac{\sum_i\sigma_i^2x_i^2}{||x||_2^2}\\
        &\leq\max_{x\neq 0}\sigma_1^2\left( \frac{||x||_2^2}{||x||_2^2} \right)=\sigma_1^2,
    \end{align}
    this is attained when $x_1=1$ and $x_{i\neq1}=0$
\end{proof}

\subsubsection{Orthogonal Procrustes problem}
The orthogonal Procrustes problem aims to approximate matrix $A$ to $B$ using an orthogonal matrix:
\begin{equation}
R=\argmin_{\Omega}||\Omega A-B||_{\mathrm{F}},\ \Omega^{\mathrm{T}}\Omega=I.
\end{equation}

To solve this problem, it is easier to first solve a related problem - the nearest orthogonal matrix problem, which aims to find the orthogonal matrix $R$ that is closest to a given $B$ in the Frobenius norm sense:
\begin{equation}
R = \argmin_{\Omega}||\Omega-B||_{\mathrm{F}},\ \Omega^{\mathrm{T}}\Omega=I.
\end{equation}
We proceed by first performing a singular value decomposition (SVD) on the target matrix:
\begin{equation}
    B = U\Sigma V\adj,
\end{equation}
and recall that orthogonal matrices have all-one singular values, and so we can immediately write
\begin{equation}
    R= UV\adj.
\end{equation}

We now show how the orthogonal Procrustes problem can be reduced to the nearest orthogonal matrix problem:
\begin{align}
R=&\argmin_{\Omega}||\Omega A-B||_{\mathrm{F}},\ \Omega^{\mathrm{T}}\Omega=I\\
=& \argmin_{\Omega}||\Omega A-B||^2_{\mathrm{F}}\\
=&\argmin_{\Omega}\bra \Omega A-B,\Omega A-B\ket_{\mathrm{F}}\\
=&\argmin_{\Omega}||\Omega A||^2_{\mathrm{F}} + ||B||^2_{\mathrm{F}}-2\bra \Omega A,B\ket_{\mathrm{F}}\\
=&\argmin_{\Omega}||A||^2_{\mathrm{F}} + ||B||^2_{\mathrm{F}}-2\bra \Omega A,B\ket_{\mathrm{F}}\\
=&\argmax_{\Omega}\bra \Omega A,B\ket_{\mathrm{F}}\\
=&\argmax_{\Omega}\Tr((\Omega A)\tp B)\\
=&\argmax_{\Omega}\Tr(\Omega BA\tp)\\
=&\argmax_{\Omega}\bra \Omega,BA\tp\ket_{\mathrm{F}}\\
=&\argmax_{\Omega}\bra \Omega,U\Sigma V\tp\ket_{\mathrm{F}}\\
=&\argmax_{\Omega}\bra U\tp\Sigma V,\Sigma\ket_{\mathrm{F}}.
\end{align}
The last expression will be maximized if $ U\tp\Sigma V$, an orthogonal matrix, is the identity matrix, hence
\begin{align}
I&=U\tp RV\\
R&=UV\tp.
\end{align}

\subsection{Linear algebra review}
\begin{definition}[Column definition of matrix-vector multiplication]
Viewing the matrix $A$ as a collection of column vectors $\bvec{a}_i$, the matrix-vector multiplication can be written as
    \begin{equation}
        A\bvec{x}=\sum_i^nx_i\bvec{a}_i.
    \end{equation}

    The matrix-matrix multiplication is defined likewise:
    \begin{equation}
        AC=[A\bvec{c}_1,A\bvec{c}_2,\dots,A\bvec{c}_p].
    \end{equation}
\end{definition}

\begin{definition}[Fundamental subspaces]
    Let $A\in\mathbb{C}^{m\times n}$, the fundamental subspaces of $A$ are
    \begin{itemize}
        \item Range or column space: $\range(A)=R(A)=\{A\bvec{x}\}=\spann\{ \bvec{a}_i \}$
        \item Row space: $R(A\adj)$
        \item Kernel or null space: $\nulll(A)=\{ A\bvec{x}=\bvec{0} \}$. 
        \item Left-hand null space: $\nulll(A\adj)$.
    \end{itemize}
\end{definition}

\begin{theorem}[Fundamental theorem of linear algebra]
    Let $A$ be an $m\times n$ matrix, then
    \begin{align}
        &\nulll(A)=\range(A\adj)^{\perp}\\
        &\dim(\range(A\adj))+\dim(\nulll(A))=n\\
        &\mathbb{C}^n=\range(A\adj)\oplus\nulll(A)
    \end{align}
\end{theorem}

\begin{definition}[Rank]
If $A\in\mathbb{R}^{m\times n}$
\begin{itemize}
    \item Column rank is dimension of space spanned by coloumns
    \item Row rank...
    \item Column rank = row rank = $\rank(A)$
    \item $\rank(A)\leq \min(m,n)$
    \item Full column rank: $\rank(A)=n$
    \item Full row rank:...
\end{itemize}
\end{definition}


\begin{definition}[Matrix inverse]
If A is invertible, then
\begin{itemize}
    \item $\rank(A)=n$
    \item $\range(A)=\mathbb{R}^n$
    \item $\nulll(A)=\{\bvec{0}\}$
    \item $0$ is not an eigenvalue of $A$
    \item $0$ is not a singular value of $A$
    \item $\det(A)\neq 0$
    \item $(AB)^{-1}=B^{-1}A^{-1}$
\end{itemize}
\end{definition}

\begin{definition}[Definiteness]
    \begin{equation}
        \bvec{x}\tp A\bvec{x}\geq 0,\ \ \forall\bvec{x}\neq\bvec{0}.
    \end{equation}
\end{definition}

\begin{definition}[Diagonally dominant]
    \begin{equation}
        |a_{ii}|\geq{\sum_{j\neq i}}|a_{ij}|.
    \end{equation}
\end{definition}

\begin{definition}[Matrix norms]
For square matrices,
\begin{align}
    &||A||_1=\max\{ \sum_{i=1}^n|a_{i,1}|,\dots, \sum_{i=1}^n|a_{i,n}|\},\\
    &||A||_{\infty}=\max\{ \sum_{i=1}^n|a_{1,i}|,\dots, \sum_{i=1}^n|a_{n,i}|\},\\
    &||A||_2=\sqrt{\rho(A\tp A)},
\end{align}
where $\rho(A)=\max|\lambda_i|$ is the spectral radius.
\end{definition}

\begin{theorem}[Kantorovich inequality]
Let $A\in\mathbb{R}^{n\times n}$ be SPD with eigenvalues ordered in non-decreasing order ($0<\lambda_1\leq\dots\leq\lambda_n$), then
\begin{equation}
    \frac{(\bvec{x}\tp \bvec{x})^2}{(\bvec{x}\tp A\bvec{x})(\bvec{x}\tp A^{-1}\bvec{x})}\geq 4\frac{\lambda_1\lambda_n}{(\lambda_1+\lambda_n)^2},
\end{equation}
for all $\bvec{x}\in\mathbb{R}^n\setminus\{\bvec{0}\}$. This is a special case of the Cauchy--Schwartz inequality.
\end{theorem}

\subsubsection{Vector norms}
A vector norm is any function $||\cdot||:\mathbb{C}^m\rightarrow\mathbb{R}^+\cup \{0\}$ that satisfies
\begin{enumerate}
    \item $||x||\geq 0$, and $||x||=0$ only if $x=0$ (positive definiteness),
    \item $||\alpha x||=|\alpha|||x||$ (homogeneity),
    \item $||x+y||\leq||x||+||y||$, (triangle inequality).
\end{enumerate}
\begin{definition}[H{\" o}lder/$p$-norms]
    \begin{align}
        &||x||_p=\left( \sum_{i=1}^m|x_i|^p \right)^{1/p},\ \ (1\leq p<\infty),\\
        &||x||_{\infty}=\max_{1\leq i\leq m}|x_i|.
    \end{align}
\end{definition}

\begin{definition}[Cauchy--Schwartz inequatlity]
    \begin{equation}
        |\bvec{x}\tp\bvec{y}|\leq ||\bvec{x}||_2||\bvec{y}||_2
    \end{equation}
\end{definition}

The triangle inequality can be derived from the Cauchy--Schwartz inequality:
\begin{align}
    ||\bvec{x}+\bvec{y}||_2^2&=(\bvec{x}+\bvec{y})\tp(\bvec{x}+\bvec{y})\\
    &\leq||\bvec{x}||_2^2+2||\bvec{x}||_2||\bvec{y}||_2+||\bvec{y}||_2^2\\
    &=(||\bvec{x}||_2+||\bvec{y}||_2)^2.
\end{align}
The Pythagorean theorem in $\mathbb{R}^n$ can be derived for $\bvec{x}$ and $\bvec{y}$ orthogonal.

\begin{definition}[H{\"o}lder inequality]
    \begin{equation}
        |(\bvec{x},\bvec{y})|\leq ||\bvec{x}||_p||\bvec{y}||_q,\ \ \frac{1}{p}+\frac{1}{q}=1.
    \end{equation}
    The Cauchy--Schwartz inequality is a special case of the H{\"o}lder inequality.
\end{definition}

\begin{theorem}
    All norms are `equivalent'.
\end{theorem}
\begin{proof}
    If $\phi_1$ and $\phi_2$ are two norms, then there is a constant $\alpha>0 $ such that
    \begin{equation}
        \phi_1(\bvec{x})\leq\alpha\phi_2(\bvec{x}),
    \end{equation}
    analogously, there exists $\beta>0$ such that $\phi_2(x)\leq\beta\phi_1(\bvec{x})$. This means we can always bound one norm in terms of another:
    \begin{equation}
        \beta\phi_2(\bvec{x})\leq\phi_1(\bvec{x})\leq\alpha\phi_2(\bvec{x}).
    \end{equation}
\end{proof}
For example, we can show that for any $\bvec{x}$,
\begin{equation}
    ||\bvec{x}||_1^2=\sum_{i}|x_i|^2+2\sum_{ij}|x_i||x_j|\geq ||\bvec{x}||_2^2.
\end{equation}
At the same time, we note that 
\begin{equation}
    ||\bvec{x}||_1=\bra\bvec{1},|\bvec{x}|\ket\leq ||\bvec{1}||_2||\bvec{x}||_2=\sqrt{n}||\bvec{x}||_2.
\end{equation}
We have therefore shown that
\begin{equation}
    \frac{1}{\sqrt{n}}||\bvec{x}||_1\leq||\bvec{x}||_2\leq||\bvec{x}||_1.
\end{equation}

\subsubsection{Matrix norms}
We can stack matrices together and take its matrix norm. The $||\cdot||_2$ case leads to the Frobenius norm:
\begin{definition}[Frobenius norm]
    \begin{equation}
        ||A||_{\mathrm{F}}=\left( \sum_{ij}|a_{ij}|^2 \right)^{1/2}=\sqrt{\Tr(A\adj A)}=\sqrt{\Tr(A A\adj)}.
    \end{equation}
\end{definition}

\begin{definition}[Mutual consistency]
    A matrix norm is called mutually consistent if
    \begin{equation}
        ||AB||\leq ||A||||B||\ \ \forall A \in \mathbb{C}^{m\times n},\ B\in\mathbb{C}^{n\times p}.
    \end{equation}
\end{definition}
For example $||A||_{\mathrm{max}}=\max_{ij}|a_{ij}|$ is not mutually consistent:
\begin{equation}
    A=B=\ttmat{1}{1}{1}{1},\ ||A||_{\mathrm{max}}=||B||_{\mathrm{max}}=1,\ ||AB||_{\mathrm{max}}=2.
\end{equation}
The Frobenius norm is mutually consistent.
\begin{definition}[Induced matrix norms]
    \begin{equation}
        ||A||=\sup_{\bvec{x}\neq\bvec{0}}\frac{||A\bvec{x}||}{||\bvec{x}||}.
    \end{equation}
    This satisfies 
    \begin{equation}
        ||AB||\leq||A||||B||,\ \text{and}\ ||A\bvec{x}||\leq||A||||\bvec{x}||.
    \end{equation}
\end{definition}

Let's define 
\begin{equation}
    ||A||_1=\max_{1\leq j\leq n}\sum_{i=1}^n|a_{ij}|,
\end{equation}
we claim that $||A||_1$ is induced by vector $1$-norm.
\begin{proof}
    Let's partition $A$ into column vectors $\{\bvec{a}_i\}$, then
    \begin{align}
        ||A\bvec{x}||_1&=||\sum_ix_i\bvec{a}_i||_1\\
        &\leq\sum_{i=1}^n||x_1\bvec{a}_i||_1\\
        &=\sum_{i=1}^n|x_i|||\bvec{a}_i||_1\\
        &\leq\sum_{i=1}^n|x_i|\left(\max_{1\leq k \leq n}||\bvec{a}_k||_1\right)\\
        &=||\bvec{x}||_1||A||_1.
    \end{align}
    Therefore, 
    \begin{equation}
        \max_{||\bvec{x}||_1=1}||A\bvec{x}||_1\leq ||A||_1.
    \end{equation}
    Now choose $\bvec{x}=\bvec{e}_k$, then
    \begin{equation}
        \max_{||\bvec{x}||_1=1}||A\bvec{x}||_1\geq \max_{1\leq k \leq n}||\bvec{a}_k||_1=||A||_1.
    \end{equation}
\end{proof}

We can also prove likewise that
\begin{equation}
    ||A||_{\infty}=\max_{1\leq i \leq n}\sum_{j=1}^n|a_{ij}|,
\end{equation}
and
\begin{equation}
    ||A||_2=\sigma_1(A)=\sqrt{\lambda_{\mathrm{max}}(A\adj A)}=\sqrt{\lambda_{\mathrm{max}}(A A\adj)}.
\end{equation}

\subsubsection{Orthogonal and unitary matrices}
\begin{definition}
    A matrix $A\in\mathbb{C}^{n\times n}$ is
    \begin{itemize}
        \item orthogonal if $A\tp=A^{-1}$, or $AA\tp=A\tp A=I$, its columns form an orthonormal basis for $\mathbb{R}^n$,
        \item unitary if $A\adj=A^{-1}$, or $AA\adj=A\adj A=I$, its columns form an orthonormal basis for $\mathbb{C}^n$.
    \end{itemize}
    If $A$ is real, then the two notions are equivalent.
\end{definition}
\begin{theorem}
    $U$ is unitary iff $||U\bvec{x}||_2=||\bvec{x}||_2$ for all $\bvec{x}\in\mathbb{C}^n$.
\end{theorem}
\begin{proof}
We first prove the in the forward direction:
\begin{equation}
    ||U\bvec{x}||_2^2=(U\bvec{x})\adj(U\bvec{x})=\bvec{x}\adj U\adj U \bvec{x}=\bvec{x}\adj\bvec{x}=||\bvec{x}||_2^2.
\end{equation}
Now in the backward direction: define $A=U\adj U$, and let $\bvec{x}=\bvec{z}+\bvec{w}$ where $\bvec{z},\bvec{w}\in\mathbb{C}^n$, so
\begin{equation}
    \bvec{x}\adj\bvec{x}=\bvec{z}\adj\bvec{z}+\bvec{w}\adj\bvec{w}+2\Re(\bvec{z}\adj\bvec{w}),
\end{equation}
and
\begin{align}
    \bvec{x}\adj A\bvec{x}&=(\bvec{z}+\bvec{w})\adj(A\bvec{z}+A\bvec{w})\\
    &=\bvec{z}\adj A\bvec{z}+\bvec{w}\adj A\bvec{w}+2\Re(\bvec{z}\adj A\bvec{w})\\
    &=\bvec{z}\adj\bvec{z}+\bvec{w}\adj\bvec{w}+2\Re(\bvec{z}\adj A\bvec{w}).
\end{align}
Then, since $\bvec{x}\adj\bvec{x}=\bvec{x}\adj A\bvec{x}$, we have
\begin{equation}
    2\Re(\bvec{z}\adj\bvec{w})=2\Re(\bvec{z}\adj A\bvec{w}),
\end{equation}
for all $\bvec{z},\bvec{w}$. This implies that $A=I$, so $U$ is unitary.
\end{proof}
Linear transformation $T:\mathbb{C}^n\rightarrow\mathbb{C}^m$ is a Euclidean isometry if 
\begin{equation}
    ||T\bvec{x}||_2=||\bvec{x}||_2\ \forall\ \bvec{x}\in\mathbb{C}^n,
\end{equation}
and a square matrix $U$ is a Euclidean isometry iff $U$ is unitary.

\subsubsection{Projectors}
\begin{definition}
    Let $S$ be a subspace of $\mathbb{R}^n$. $P\in\mathbb{R}^{n\times n}$ is an orthogonal projector onto $S$ if
    \begin{itemize}
        \item $\range(P)=S$,
        \item $P^2=P$,
        \item $P\tp=P$.
    \end{itemize}
\end{definition}

\begin{definition}
    If $\{\bvec{a}_i\}$ is a linearly independent set and $S=\spann\{\bvec{a}_i\}$, the projector onto $\range{A}=\spann\{\bvec{a}_i\}$ is
    \begin{equation}
        P=A(A\tp A)^{-1}A\tp.
    \end{equation}
    If they are orthonormal instead, 
    \begin{equation}
        P=AA\tp.
    \end{equation}
    The application of the projector can be made cheaper by performing a series of matrix-vector multiplications.
\end{definition}
\begin{corollary}
    $I-2P$ gives the reflection in the null space of $P$.
\end{corollary}

\begin{theorem}
    Given orthogonal projector $P$ and vector $\bvec{x}$. $P\bvec{x}$ uniquely solves optimization problem
    \begin{equation}
        \min_{\bvec{z}\in\range(P)}||\bvec{z}-\bvec{x}||_2
    \end{equation}
\end{theorem}
\begin{proof}
    We will show that for any $\bvec{z}\in\range(P)\setminus P\bvec{x}$ that
    \begin{equation}
        ||\bvec{z}-\bvec{x}||_2>||P\bvec{x}-\bvec{x}||_2.
    \end{equation}
    We observe that
    \begin{equation}
        \bvec{z}-\bvec{x}=\bvec{z}-P\bvec{x}+P\bvec{x}-\bvec{x}
    \end{equation}
    let $\bvec{z}=P\bvec{w}$ for some $\bvec{w}\in\mathbb{R}^n$:
    \begin{equation}
        (\bvec{z}-P\bvec{x})\tp(P\bvec{x}-\bvec{x})=0
    \end{equation}
    By Pythagorean law
    \begin{equation}
        ||\bvec{z}-\bvec{x}||_2^2=||\bvec{z}-P\bvec{x}||_2^2+||P\bvec{x}-\bvec{x}||_2^2>||P\bvec{x}-\bvec{x}||_2^2
    \end{equation}
    since $\bvec{z}\neq P\bvec{x}$.
\end{proof}

\subsection{QR factorization}
Let $A\in\mathbb{R}^{m\times n}$, $m\geq n$, we have
\begin{itemize}
    \item Full QR factorization $A=QR$ where $Q\in\mathbb{R}^{m\times m}$ is orthogonal, and $R\in\mathbb{R}^{m\times n}$ is upper triangular,
    \item Reduced (thin, economy) QR factorization $A=\hat{Q}\hat{R}$ where $\hat{Q}\in\mathbb{R}^{m\times n}$ has orthonormal columns, and $\hat{R}\in\mathbb{R}^{n\times n}$ is upper triangular.
\end{itemize}
The two versions are related by
\begin{equation}
    A=QR=
    \begin{bmatrix}
        \hat{Q} & \ast
    \end{bmatrix}
    \tvec{\hat{R}}{0}=\hat{Q}\hat{R}
\end{equation}
There are many methods of obtaining the QR factorization, we will go through some of them below.
\subsubsection{Gram--Schmidt method}
We'd like to transform a set of linearly independent vectors $\{\bvec{a}_i\}$ into an orthonormal set $\{\bvec{q}_i\}$, such that $\spann(\bvec{q}_1)=\spann(\bvec{a}_1)$, $\spann(\bvec{q}_1,\bvec{q}_2)=\spann(\bvec{a}_1,\bvec{a}_2)$, and so on. This can be done as follows, remembering that $\bvec{q}_i\tp\bvec{q}_i=1$:
\begin{enumerate}
    \item $\spann(\bvec{q}_1)=\spann(\bvec{a}_1)$ gives
    \begin{equation}
        \bvec{q}_1=\frac{\bvec{a}_1}{r_{11}}=\frac{\bvec{a}_1}{\lVert\bvec{a}_1\rVert}.
    \end{equation}
    \item $\spann(\bvec{q}_1,\bvec{q}_2)=\spann(\bvec{a}_1,\bvec{a}_2)$, $\bvec{q}_1\tp\bvec{q}_2=0$, let the component of $\bvec{a}_2$ orthogonal to $\bvec{q}_1$ be $\bvec{z}=\bvec{a}_2-r_{12}\bvec{q}_1$. The orthogonality condition gives
    \begin{equation}
        \bvec{q}_1\tp(\bvec{a}_2-r_{12}\bvec{q}_1)=\bvec{q}_1\tp\bvec{a}_2-r_{12}\bvec{q}_1\tp\bvec{q}_1=0\Rightarrow r_{12}=\bvec{q}_1\tp\bvec{a}_2.
    \end{equation}
    The normalization condition gives
    \begin{equation}
        \bvec{q}_2=\frac{\bvec{z}}{r_{22}}=\frac{\bvec{z}}{\lVert\bvec{z}\rVert}
    \end{equation}
    \item Let $\bvec{u}\in\spann(\bvec{q}_1,\bvec{q}_2)$ be the projection of $\bvec{a}_3$ onto the plane spanned by $\bvec{q}_1$ and $\bvec{q}_2$, $\bvec{u}=r_{13}\bvec{q}_1+r_{23}\bvec{q}_2$, so the orthogonal component is $\bvec{z}=\bvec{a}_3-\bvec{u}$. Using the orthogonality conditions we have $r_{13}=\bvec{q}_1\tp\bvec{a}_3$ and $r_{23}=\bvec{q}_2\tp\bvec{a}_3$. The normalization condition again implies $r_{33}=\lVert\bvec{z}\rVert$.
    \item To compute $\bvec{q}_k$, we do the following 
    \begin{align}
        &r_{ik}=\bvec{q}_i\tp\bvec{a}_k,\ \ i=1,2,\dots,k-1,\\
        &\bvec{z}=\bvec{a}_k-\sum_{i=1}^{k-1}r_{ik}\bvec{q}_i\\
        &r_{kk}=\lVert\bvec{z}\rVert\\
        &\bvec{q}_k=\frac{\bvec{z}}{r_{kk}}.
    \end{align}
\end{enumerate}
The pseudocode for this would look like
\begin{verbatim}
R(1,1) = norm2(A(:,1))
Q(:,1) = A(:,1)/R(1,1)
for k=2:n
    R(1:k-1, k) = Q(:, 1:k-1)' * A(:,k)
    z = A(:,k) - Q(:, 1:k-1) * R(1:k-1, k)
    R(k,k) = norm2(z)
    Q(:,k) = z/R(k,k)
end
\end{verbatim}
This is the algorithm to compute $A=\hat{Q}\hat{R}$.

\subsubsection{Modified Gram--Schmidt}
The algorithm is mathematically equivalent to classical Gram--Schmidt, but is more numerically stable. It relies on the following observation:
\begin{equation}
    \left(A-\sum_{i=1}^{k-1}\bvec{q}_i\bvec{r}_i\tp\right)_k=r_{kk}\bvec{q}_k,
\end{equation}
so that we can gradually overwrite the columns of $A$, revealing $\bvec{q}_kr_{kk}$ in the $k$-th iteration at the $k$-th iteration, giving the following algorithm:
\begin{verbatim}
for k=1:n
    R(k,k) = norm2(A(:,k))
    Q(:,k) = A(:,k)/R(k,k)
    for j=k+1:n
        R(k,j) = Q(:,k)' * A(:,j)
        A(:,j) = A(:,j) - Q(:,k) * R(k,j)
    end
end
\end{verbatim}

\subsubsection{Householder transformations}
\begin{definition}[Householder transformation]
    A Householder transformation is a matrix of the form 
    \begin{equation}
        Q=I_n-2\bvec{uu}\tp,
    \end{equation}
    with $\lVert\bvec{u}\rVert=1$, and $\bvec{u}$ is called the Householder vector.
\end{definition}
We can see that $Q$ is orthogonal: $Q\tp Q=QQ\tp=I$. $Q\tp\bvec{x}$ is a reflection on $\bvec{x}$ on the hyperplane $\spann(\bvec{u})^{\perp}$. Householder transformations are useful in getting the QR factorization as the $Q$ matrices can be used to sequentially give the upper triangular structure:
\begin{equation}
    \begin{bmatrix}
        \ast & \ast & \ast \\
        \ast & \ast & \ast \\
        \ast & \ast & \ast 
    \end{bmatrix}\xrightarrow{Q_1}
    \begin{bmatrix}
        \ast & \ast & \ast \\
        0    & \ast & \ast \\
        0    & \ast & \ast 
    \end{bmatrix}\xrightarrow{Q_2}
    \begin{bmatrix}
        \ast & \ast & \ast \\
        0    & \ast & \ast \\
        0    & 0    & \ast 
    \end{bmatrix}\Rightarrow Q_2Q_1A=R\Rightarrow A=Q_1\tp Q_2\tp R\equiv QR.
\end{equation}
We want to find $\bvec{u}$ such that
\begin{equation}
    Q\bvec{x}=(I-2\bvec{uu}\tp)\bvec{x}=
    \begin{bmatrix}
        \ast\\ 0 \\ \vdots \\0
    \end{bmatrix}=c\bvec{e}_1,
\end{equation}
so we can write
\begin{align}
    \bvec{x}-2\bvec{u}(\bvec{u}\tp\bvec{x})&=c\bvec{e}_1\\
    \bvec{u}&=\frac{1}{2\bvec{u}\tp\bvec{x}}(\bvec{x}-c\bvec{e}_1).
\end{align}
Since $\lVert\bvec{x}\rVert=\lVert Q\bvec{x}\rVert=|c|$, so we can say $\bvec{u}$ is parallel to $\bvec{x}\pm\lVert\bvec{x}\rVert\bvec{e}_1$, and we can normalize $\bvec{u}$. Therefore, we have found the required Householder vector
\begin{equation}
    \tilde{\bvec{u}}=\bvec{x}\pm\lVert\bvec{x}\rVert\bvec{e}_1,\ \ \bvec{u}=\frac{\tilde{\bvec{u}}}{\lVert\tilde{\bvec{u}}\rVert},
\end{equation}
then $Q\tp\bvec{x}=\mp\lVert\bvec{x}\rVert\bvec{e}_1$. The sign should be chosen so that there is no catastrophic cancellation in $\tilde{\bvec{u}}$, so 
\begin{equation}
    \tilde{\bvec{u}}=\bvec{x}+\sign(x_1)\lVert\bvec{x}\rVert\bvec{e}_1.
\end{equation}
The algorithm for Householder reflection is
\begin{verbatim}
function u = house(x)
    u = x + sign(x(1)) * norm2(x) * e1
    u = u/norm2(u)
end
\end{verbatim}
To compute QR by Householder transformations, we work on successively smaller submatrices of $A$:
\begin{enumerate}
    \item Let $\bvec{x}=A(1:m,1)$, compute $\bvec{u}_1=\bvec{x}+\sign(x_1)\lVert\bvec{x}\rVert\bvec{e}_1$, and normalize $\bvec{u}_1$. Then multiply and overwrite $A=(I-2\bvec{u}_1\bvec{u}_1\tp)A$,
    \item Let $\bvec{x}=A(2:m,2)$, compute $\bvec{u}_2=\bvec{x}+\sign(x_1)\lVert\bvec{x}\rVert\bvec{e}_1$, and normalize $\bvec{u}_2$. Then multiply and overwrite $A=(I-2\bvec{u}_2\bvec{u}_2\tp)A$. Note that we're working in the $A(2:m,2:n)$ submatrix,
\end{enumerate}
and so on. We can write the pseudocode below
\begin{verbatim}
for k=1:n
    x = A(k:m, k)
    u_k = house(x)
    A(k:m, k:n) = A(k:m, k:n) - 2*u_k * (u_k' * A(k:m, k:n))
end
\end{verbatim}
A few remarks are in order
\begin{itemize}
    \item We don't have to explicitly generate the $Q_k$ matrices, but we can save the Householder vectors $\bvec{u}_i$:
    \begin{equation}
        Q_k=\ttmat{I}{0}{0}{I-2\bvec{u}_k\bvec{u}_k\tp}
    \end{equation}
    \item At the end of the algorithm, $A$ contains the upper triangular matrix $R$.
    \item To compute $Q\tp\bvec{b}=Q_{k-1}\tp\dots Q_1\tp\bvec{b}$, we observe that $Q_k\tp\bvec{b}'$ is
    \begin{equation}
        \ttmat{I_{k-1}}{0}{0}{I-2\bvec{u}_k\bvec{u}_k\tp}\tvec{b(1:k-1)}{b(k:m)}=\tvec{b(1:k-1)}{b(k:m)-2\bvec{u}_k\bvec{u}_kb(k:m)},
    \end{equation}
    only the bottom part changes, so we can write the pseudocode
    \begin{verbatim}
for k=1:n
    b(k:m) = b(k:m) - 2*u_k*(u_k' * b(k:m))
end
    \end{verbatim}
    \item To compute $Q\bvec{x}$, go in reverse order:
    \begin{verbatim}
for k=n:-1:1
    x(k:m) = x(k:m) - 2*u_k*(u_k' * x(k:m))
end
    \end{verbatim}
    \item To explicitly compute $Q$, we compute each column of $Q$ is $\bvec{q}_k=Q\bvec{e}_k$, so we can use the above algorithm to compute columns of $Q$.
\end{itemize}

\subsubsection{Givens rotation}
A Givens rotation is defined as 
\begin{equation}
    R(\theta)=\ttmat{\cos\theta}{-\sin\theta}{\sin\theta}{\cos\theta},
\end{equation}
which rotates a vector $\bvec{x}\in\mathbb{R}^2$ counter-clockwise by $\theta$ degrees. We can use it to zero out the bottom element of a two dimensional vector:
\begin{equation}
    \ttmat{c}{-s}{s}{c}\tvec{x_1}{x_2}=\tvec{\sqrt{x_1^2+x_2^2}}{0},
\end{equation}
where $c=\cos\theta=x_1/\sqrt{x_1^2+x_2^2}$ and $s=\sin\theta=-x_2/\sqrt{x_1^2+x_2^2}$. In $m$ dimensions, to clear an arbitrary element $x_1$ located at the $j$-th row, we choose a pivot in the same column at the $i$-th row.

\subsection{Rank-deficient least squares problem}
In the rank-deficient least squares problem, the $A$ matrix is not of full rank, which is to say, $R$ is not full rank, or at least one of the $r_{ii}$'s is zero. We can solve it either using QR factorization with column pivoting or with SVD.

\subsubsection{QR factorization with pivoting}
The decomposition is given by
\begin{equation}
    AP=QR=Q\ttmat{R_{11}}{R_{12}}{0}{0},
\end{equation}
where $R_{11}$ is upper triangular and nonsingular, and has the rank of $A$, and $P$ is a permutation matrix that does the column pivoting. The formulation is as follows
\begin{enumerate}
    \item Find column of $A$ with the largest $2$-norm. Swap this column with the first column of $A$, and do one step of Householder to zero out the first column except the first element.
    \item Overwrite $A=Q_1\tp AP_1$, and we work on the $A(2:m,2:n)$ submatrix. Repeat step one, noting that we swap \textit{entire} columns, not just in the submatrix.
    \item Stop when the largest column of $A(k:m,k:n)$ has $2$-norm equal to zero.
\end{enumerate}
Assuming we have obtained the decomposition $AP=QR$, we can solve the rank-deficient LS problem,
\begin{equation}
    \min_{\bvec{x}}\lVert \bvec{b}-A\bvec{x}\rVert_2^2
\end{equation}
as follows:
\begin{align}
    &\lVert \bvec{b}-A\bvec{x}\rVert_2^2\\
    =&\lVert \bvec{b}-APP\tp\bvec{x}\rVert_2^2\\
    =&\lVert \bvec{b}-QRP\tp\bvec{x}\rVert_2^2\\
    =&\lVert Q\tp\bvec{b}-RP\tp\bvec{x}\rVert_2^2,
\end{align}
where
\begin{equation}
    Q\tp\bvec{b}\equiv\tvec{\bvec{c}}{\bvec{d}},\ \ P\tp\bvec{x}\equiv\tvec{\bvec{y}}{\bvec{z}},
\end{equation}
where $\bvec{c}, \bvec{y}\in\mathbb{R}^r$, and $\bvec{d}\in\mathbb{R}^{m-r}$, $\bvec{z}\in\mathbb{R}^{n-r}$, as appropriate. We then have
\begin{align}
    &\lVert \bvec{b}-A\bvec{x}\rVert_2^2\\
    =&\left\lVert\tvec{\bvec{c}}{\bvec{d}}-\ttmat{R_{11}}{R_{22}}{0}{0}\tvec{\bvec{y}}{\bvec{z}}\right\rVert_2^2\\
    =&\lVert\bvec{c}-R_{11}\bvec{y}-R_{12}\bvec{z}\rVert_2^2+\lVert\bvec{d}\rVert^2_2.
\end{align}
The second term doesn't depend on $\bvec{x}$, and can be ignored in the minimization. We essentially need to find
\begin{equation}
    \bvec{c}-R_{11}\bvec{y}-R_{12}\bvec{z}=\bvec{0}.
\end{equation}
This can be set to zero because we have a full-rank sub-problem. $\bvec{z}$ is arbitrary here and are usually set to $\bvec{0}$ to obtain the minimium-norm LS solution, which we can obtain by back substitution:
\begin{equation}
    R_{11}\bvec{y}=\bvec{c}-R_{12}\bvec{z},
\end{equation}
giving finally 
\begin{equation}
    \bvec{x}_{\mathrm{LS}}=P\tvec{\bvec{y}}{\bvec{z}}.
\end{equation}

\subsubsection{SVD for rank-deficient LS}
We take the (reduced) SVD $A=U\Sigma V\tp$, where $U$ and $V$ are orthogonal, and $\Sigma\in\mathbb{R}^{m\times n}$ contains $r=\rank(A)$ non-zero sorted singular values. The LS problem can be solved as follows:
\begin{align}
    &\lVert \bvec{b}-A\bvec{x}\rVert_2^2\\
    =&\lVert \bvec{b}-U\Sigma V\tp\bvec{x}\rVert_2^2\\
    =&\lVert U\tp\bvec{b}-\Sigma V\tp\bvec{x}\rVert_2^2\\
    \equiv&\lVert \bvec{z}-\Sigma \bvec{y}\rVert_2^2\\
    =&\sum_{i=1}^r(z_i-\sigma_iy_i)^2+\sum_{i=r+1}^mz_i^2.
\end{align}
Again the second summation can be dropped in the minimization, and we have minimizer if $y_i=z_i/\sigma$ for $i=1,2,\dots,r$, and again $y_{r+1},\dots,y_n$ are arbitrary, and are usually set to zero. The solution can be obtained as 
\begin{equation}
    \bvec{x}_{\mathrm{LS}}=V\bvec{y}.
\end{equation}

\subsubsection{Tikhonov regularization for ill-posed LS}
For matrices $A$ with continuous singular value spectra, no clear truncation of $SVD$ is possible, resulting in higher computational cost. We motivate the Tikhonov regularization through Bayesian statistics.
\begin{definition}[Multivariate Gaussian]
    A Gaussian random variable $\bvec{X}$ with mean $\bvec{\mu}$ and covariance matrix $\Sigma$ has probability density
    \begin{equation}
        \Pi(\bvec{x})=\left(\frac{1}{2\pi\det(\Sigma)}\right)^{n/2}\exp\left( -\frac{1}{2}(\bvec{x}-\bvec{\mu})\tp\Sigma^{-1}(\bvec{x}-\bvec{\mu}) \right),
    \end{equation}
    and is compactly denoted as 
    \begin{equation}
        \bvec{x}\sim\mathcal{N}(\bvec{\mu},\Sigma).
    \end{equation}
\end{definition}
\begin{theorem}[Bayes Law]
Bayes relates the posterior function $\Pi(\bvec{x}|\bvec{b})$ with the likelihood function $\Pi(\bvec{b}|\bvec{x})$, the prior function $\Pi(\bvec{x})$, and the marginal function $\Pi(\bvec{b})$:
    \begin{equation}
        \Pi(\bvec{x}|\bvec{b})=\frac{\Pi(\bvec{b}|\bvec{x})\Pi(\bvec{x})}{\Pi(\bvec{b})}
    \end{equation}
\end{theorem}
We consider both $\bvec{x}$ and $\bvec{b}$ random variables, with
\begin{equation}
    \bvec{b}=A\bvec{x}+\bvec{e},\ \ \bvec{e}\sim\mathcal{N}(\bvec{0},I),
\end{equation}
where $\bvec{e}$ is a Gaussian noise, and we can immediate write the likelihood function as 
\begin{align}
    \Pi(\bvec{b}|\bvec{x})&=\Pi(\bvec{b}-A\bvec{x})\\
    &\propto\exp\left(-\frac{1}{2}(\bvec{b}-A\bvec{x})\tp(\bvec{b}-A\bvec{x})\right)\\
    &=\exp\left(-\frac{1}{2}\lVert\bvec{b}-A\bvec{x}\rVert_2^2\right).
\end{align}
To maximize the probability of $\bvec{x}$ given $\bvec{b}$, which is our LS problem, we can simply only maximize the the likelihood function, and we can minimize the negative log function:
\begin{equation}
    \argmax_{\bvec{x}}\Pi(\bvec{b}|\bvec{x})=\argmin_{\bvec{x}}-\log\Pi(\bvec{b}|\bvec{x})=\argmin_{\bvec{x}}\frac{1}{2}\lVert\bvec{b}-A\bvec{x}\rVert_2^2.
\end{equation}
This is known as the maximum likelihood estimate (MLE).

We can also look at the maximum \textit{a posteriori} estimate (MAP). For the prior function, let's assume $\bvec{x}\sim\mathcal{N}(\bvec{\mu},\Sigma)$, and maximize the posterior as a whole:
\begin{align}
    \bvec{x}_{\mathrm{MAP}}&=\argmax_{\bvec{x}}\Pi(\bvec{b}|\bvec{x})\Pi(\bvec{x})\\
    &=\argmax_{\bvec{x}}\exp\left(-\frac{1}{2}\lVert\bvec{b}-A\bvec{x}\rVert_2^2-\frac{1}{2}(\bvec{x}-\bvec{\mu})\tp\Sigma^{-1}(\bvec{x}-\bvec{\mu})\right)\\
    &=\argmin_{\bvec{x}}\frac{1}{2}\lVert\bvec{b}-A\bvec{x}\rVert_2^2+\frac{1}{2}\lVert R(\bvec{x}-\bvec{\mu})\rVert_2^2\equiv J(\bvec{x}),
\end{align}
where we have performed a Cholesky factorization $\Sigma^{-1}=R\tp R$, and the final LS problem is called a regularized LS problem. 

Let's assume $\bvec{e}\sim\mathcal{N}(\bvec{0},I)$ and $\bvec{x}\sim\mathcal{N}(\bvec{0},\lambda^{-2}I)$, so we have
\begin{align}
    J(\bvec{x})&=\frac{1}{2}\lVert\bvec{b}-A\bvec{x}\rVert_2^2+\frac{1}{2}\lambda^2\lVert \bvec{x}\rVert_2^2\\
    &=\frac{1}{2}\left\lVert\tvec{A}{\lambda I}\bvec{x}-\tvec{\bvec{b}}{\bvec{0}}\right\rVert_2^2.
\end{align}
We can write the corresponding normal equations as 
\begin{equation}
    \begin{bmatrix}
        A\tp & \lambda I
    \end{bmatrix}
    \tvec{A}{\lambda I}\bvec{x}=\begin{bmatrix}
        A\tp & \lambda I
    \end{bmatrix}\tvec{\bvec{b}}{\bvec{0}}\Rightarrow(A\tp A+\lambda^2I)\bvec{x}=A\tp\bvec{b}.
\end{equation}
Having computed the SVD $A=U\Sigma V\tp$, $A\tp A=V\Sigma\tp\Sigma V\tp$, we can solve the normal equations easily:
\begin{align}
    \bvec{x}_{\mathrm{MAP}}&=(V\Sigma\tp\Sigma V\tp+\lambda^2I)^{-1}V\Sigma\tp U\tp\bvec{b}\\
    &=(V(\Sigma\tp\Sigma+\lambda^2I)V\tp)^{-1}V\Sigma\tp U\tp\bvec{b}\\
    &=V(\Sigma\tp\Sigma+\lambda^2I)^{-1}\Sigma\tp U\tp\bvec{b}\\
    &=\sum_{i=1}^r\phi_i\frac{\bvec{u}_i\tp\bvec{b}}{\sigma_i}\bvec{v}_i,\ \phi_i=\frac{\sigma_i^2}{\sigma_i^2+\lambda^2},
\end{align}
where the $\phi_i$'s are known as filter factors.

\subsection{Eigenvalue Preliminaries}
The eigenvalue problem can be set up quickly:
\begin{equation}
    A\bvec{x}=\lambda\bvec{x}\Rightarrow(\lambda I-A)\bvec{x}=\bvec{0},
\end{equation}
which implies that $\bvec{x}$ is a nonzero vector in the null space of $(\lambda I-A)$, which implies that it has a non-empty null space, and therefore is singular, and has determinant zero, giving rise to the characteristic polynomial:
\begin{equation}
    \det(\lambda I-A)=0,
\end{equation}
the roots of which are the eigenvalues of $A$.
\begin{theorem}
    Let $A\in\mathbb{C}^{n\times n}$, $B\in\mathbb{C}^{k\times k}$, $X\in\mathbb{C}^{n\times k}$, and suppose $AX=XB$. If $X$ is of full column rank, then $\lambda(B)\subseteq\lambda(A)$.
\end{theorem}

\begin{proof}
    Let $B\bvec{y}=\lambda\bvec{y}$ be an eigenpair. Now consider
    \begin{equation}
        AX\bvec{y}=XB\bvec{y}=X\lambda\bvec{y},
    \end{equation}
    since $X$ is full rank, and $\bvec{y}\neq\bvec{0}$, then $\bvec{z}\equiv X\bvec{y}\neq\bvec{0}$, so $A\bvec{z}=\lambda\bvec{z}$, so $\lambda$ is an eigenvalue of $A$.
\end{proof}

\begin{property}[Decoupling property]
    If $A\in\mathbb{C}^{n\times n}$ is partitioned as 
    \begin{equation}
        A=\ttmat{A_{11}}{A_{22}}{0}{A_{22}},
    \end{equation}
    then $\lambda(A)=\lambda(A_{11})\cup\lambda(A_{22})$
\end{property}

\begin{definition}[Eigenvalue multiplicity]
Let $\lambda_1,\lambda_2,\dots,\lambda_q$ be distinct eigenvalues of $A\in\mathbb{C}^{n\times n}$, then
    \begin{itemize}
        \item The number of times $\lambda_i$ repeats, $n_i$, is called the algebraic multiplicity of $\lambda_i$. $\sum_in_i=n$.
        \item The maximum number of linearly independent eigenvectors associated with $\lambda_i$, $d_i$, is called the geometric multiplicity of $\lambda_i$. $d_i=\dim(\nulll(\lambda_i I-A))$.
    \end{itemize}
    We have $1\leq d_i\leq n_i$ for all $i$. If $d_i<n_i$, then $\lambda_i$ is known as defective. A single defective eigenvalue makes $A$ defective.
\end{definition}

\begin{definition}[Diagonizability]
A matrix $A\in\mathbb{C}^{n\times n}$ is diagonalizable if and only if $A$ is nondefective, if and only if there exists a nonsingular matrix $X$ such that
\begin{equation}
    X^{-1}AX=\Lambda,\ \ A=X\Lambda X^{-1},
\end{equation}
where $\Lambda$ is a diagonal matrix containing the eigenvalues.
\end{definition}

\begin{definition}[Jordan form]
    Let $A\in\mathbb{C}^{n\times n}$. There is a nonsingular matrix $X$ such that
    \begin{equation}
        X^{-1} AX=\diag(J_1,J_2,\dots,J_n),
    \end{equation}
    where $J_i$ is $m_i\times m_i$, and contains on the diagonal $\lambda_i$'s, and a superdiagonal of $1$'s, with zeros in the other entries. $\sum_im_i=n$. Each Jordan black $J_i$ has a single eigenvalue $\lambda_i$ with a single independent eigenvector, $\bvec{e}_1$.
\end{definition}

\begin{definition}[Schur decomposition]
    Let $A\in\mathbb{C}^{n\times m}$, there is a unitary matrix $Q$ such that
    \begin{equation}
        Q\adj AQ=T,
    \end{equation}
    where $T$ is upper triangular. The columns $\bvec{q}_n$ of $Q$ are Schur vectors. Let $T=D+N$, where $D=\diag(\lambda_1,\lambda_2,\dots,\lambda_n)$ and $N$ is a strictly upper triangular matrix, then $\bvec{q}_k$ is an eigenvector of $A$ if and only if the $k$-th column of $N$ is $\bvec{0}$. If $A\adj A=AA\adj$, then we can show that $N=0$, that is, $T$ is diagonal.
\end{definition}

\begin{definition}[Normal matrix]
    If a matrix satisfies $A\adj A=AA\adj$ then $A$ is a normal matrix.
\end{definition}

\begin{theorem}[Spectral theorem]
    $A$ is unitarily diagonalizable if and only if $A$ is normal. That is, there exists a unitary matrix $Q$ such that $A=QDQ\adj$.
\end{theorem}
If $A$ is not normal, $Q\adj AQ=T=D+N$, then
\begin{align}
    \lVert Q\adj AQ\rVert_{\mathrm{F}}^2&=\lVert D+N \rVert_{\mathrm{F}}^2\\
    \lVert A\rVert_{\mathrm{F}}^2&=\lVert D\rVert_{\mathrm{F}}^2+\lVert N\rVert_{\mathrm{F}}^2\\
    \lVert N\rVert_{\mathrm{F}}^2&=\lVert A\rVert_{\mathrm{F}}^2-\sum_{i=1}^n|\lambda_i|^2,
\end{align}
which shows that the norm of $N$ is a measure of departure from normality.

\subsection{Algorithms for nonsymmetric eigenvalue problems}
We want to compute the Schur decomposition of a matrix $A$, $Q\adj AQ=T$, and since $T$ is upper triangular, the eigenvalues of $T$ are the diagonal entries, and since $T$ and $A$ are similar, those are the eigenvalues of $A$ as well. We need iterative algorithms. The aim is to reduce the matrix to Hessenberg (if $A$ non-Hermitian) or tridiagonal form (if $A$ Hermitian).

\subsubsection{Power method}
The Rayleigh quotient of a vector $\bvec{x}\in\mathbb{R}^n$ is 
\begin{equation}
    r(\bvec{x})=\frac{\bvec{x}\adj A\bvec{x}}{\bvec{x}\adj\bvec{x}},
\end{equation}
if $A\bvec{x}=\lambda\bvec{x}$, then $r(\bvec{x})=\lambda$. The algorithm is given below:
\begin{verbatim}
Given A, x_0!=0, k=0
while not converged
    y_{k+1} = A*x_k                        % cost dominated by M-V mult
    x_{k+1} = y_{k+1} / norm2(y_{k+1})     % normalize
    lambda_{k+1} = x_{k+1}' * A * x_{k+1} % Rayleigh quotient
    k = k+1
end
\end{verbatim}

\subsubsection{Shifted power method/inverse iteration}
We can make the observation that the eigenvalues of $(A-\sigma I)^{-1}$ is $1/(\lambda-\sigma)$ for $\sigma\neq\lambda$:
\begin{align}
    A\bvec{x}&=\lambda\bvec{x}\\
    A\bvec{x}-\sigma\bvec{x}&=\lambda\bvec{x}-\sigma\bvec{x}\\
    (A-\sigma I)^{-1}\bvec{x}&=\frac{1}{\lambda-\sigma}\bvec{x},\ \forall\sigma\neq\lambda.
\end{align}
Therefore, eigenvalues close to $\sigma$ are magnified, and becomes extremal, therefore we can apply the power iteration to $(A-\sigma I)^{-1}$, with $\sigma$ known as the shift. The algorithm is given below
\begin{verbatim}
Given A, x_0!=0, k=0, sigma
while not converged
    y_{k+1} = (A - sigma*I)^(-1) * x_k    % solve the system instead of inverting
    x_{k+1} = y_{k+1} / norm2(y_{k+1})
    lambda_{k+1} = x_{k+1}' * A * x_{k+1}
    k = k+1
end
\end{verbatim}
A simple and obvious improvement on this algorithm is by using the Rayleigh quotient as the shift, known as the Rayleigh quotient iteration:
\begin{verbatim}
Given A, x_0!=0, norm2(x0)=1, k=0, lambda_0=x_0' * A * x_0
while not converged
    y_{k+1} = (A - lambda_k*I)^(-1) * x_k    % solve the system instead of inverting
    x_{k+1} = y_{k+1} / norm2(y_{k+1})
    lambda_{k+1} = x_{k+1}' * A * x_{k+1}
    k = k+1
end
\end{verbatim}
This is one of the few algorithms that has cubic convergence.

\subsubsection{Orthogonal iteration/subspace iteration}
Given $A$, $Z_0\in\mathbb{R}^{n\times p}$ with orthonormal columns, $k=1$
\begin{verbatim}
while not converged
    y_k = A * Z_{k-1}
    [Z_k, R_k] = qr(y_k) % reduced QR
    k = k+1
end
\end{verbatim}
This reduces to the power method if $p=1$. A reasonable starting $Z_0$ is with diagonal entries of $1$'s.

\subsubsection{Basic and shifted QR iteration}
Basic QR iteration proceeds as follows:

Given $A$, $k=0$, $A_0=A$
\begin{verbatim}
while not converged
    [Q_k, R_k] = qr(A_k)
    A_{k+1} = R_k * Q_k
    k = k+1
end
\end{verbatim}
This algorithm seems like magic at first, but will reveal itself to be a generalization of the power method upon analysis. First we note that $A_k$ and $A_{k+1}$ are orthogonally similar:
\begin{equation}
    A_{k+1}=R_kQ_k=Q_k\tp Q_kR_kQ_k=Q_k\tp A_kQ_k.
\end{equation}
Recursing this all the way back to $A_0=A$, we have
\begin{equation}
    A_{k+1}=\underbrace{Q_k\tp Q_{k-1}\tp\dots Q_0\tp}_{\equiv V_k\tp} A \underbrace{Q_0Q_1\dots Q_k}_{\equiv V_k}.
\end{equation}
So we can conclude that
\begin{align}
    AV_k&=V_kA_{k+1}=V_kQ_{k+1}R_{k+1}\\
    AV_k\bvec{e}_1&=V_kQ_{k+1}R_{k+1}\bvec{e}_1=V_{k+1}r_{11}\bvec{e}_1\\
    V_{k+1}\bvec{e}_1&=\frac{1}{r_{11}}AV_k\bvec{e}_1,
\end{align}
which is to say, the power iteration is performed on the first column of $V_i$. This is never used directly in practice. The first modification is the shifted QR iteration:

Given $A$, $k=0$, $A_0=A$, shift $\sigma_0$
\begin{verbatim}
while not converged
    choose shift sigma_k near an eigenvalue of A
    [Q_k, R_k] = qr(A_k - sigma_k*I_n)
    A_{k+1} = R_k * Q_k + sigma_k*I_n
    k = k+1
end
\end{verbatim}
We can show that $A_k$ and $A_{k+1}$ are orthogonally similar:
\begin{equation}
    A_{k+1}=R_kQ_k+\sigma_kI=Q_k\tp Q_kR_kQ_k+\sigma_kQ_k\tp Q_k=Q_k\tp(Q_kR_k+\sigma_kI)Q_k=Q_k\tp A_kQ_k.
\end{equation}
If $\sigma_k$ is an exact eigenvalue of $A$, $\sigma_k=\lambda_i$, then $A_k-\lambda_iI_n$ is singular, so $R_k$ is singular ($Q_k$ cannot be sinuglar as it is orthogonal), so some diagonal entries of $R_k$ needs to be zero. We can assume that $[R_k]_{n,n}= 0$, so the last row of $R_k$ is zero, and the last row of $A_{k+1}=R_kQ_k+\lambda_iI_n$ is $\lambda\bvec{e}_n\tp$. So we can apply the algorithm to the $A_{k+1}(1:n-1,1:n-1)$ submatrix.

\subsubsection{Shifted Hessenberg QR iteration}
This algorithm is commonly used in practical computation of the eigendecomposition of non-symmetric matrices. It first proceeds by reduction to Hessenberg or tridiagonal form, and the performs QR iteration on the reduced matrix. The Hessenberg reduction proceeds for $A\in\mathbb{C}^{n\times n}$ as follows:
\begin{enumerate}
    \item Find Householder transformation $Q_1$ such that $Q_1\adj A$ has the correct Hessenberg form:
    \begin{equation}
        \hat{Q}_1\adj
        \begin{bmatrix}
            \ast \\ \ast \\ \vdots \\ \ast
        \end{bmatrix}=
        \begin{bmatrix}
            \bar{\ast} \\ 0 \\ \vdots \\ 0
        \end{bmatrix},
    \end{equation}
    and 
    \begin{equation}
        Q_1\adj=\ttmat{1}{\bvec{0}\tp}{\bvec{0}}{\hat{Q}_1\adj}.
    \end{equation}
    This ensures that $Q_1\adj AQ_1$ preserves the zeros we just created.
    \item Repeat the step with ${Q}_2\adj$:
    \begin{equation}
        Q_2\adj=\ttmat{I_2}{0}{0}{\hat{Q}_2\adj},
    \end{equation}
    where $\hat{Q}_2\adj$ sets the second column into the Hessenberg form.
    \item We need $n-2$ Householder transformations to reduce an $n\times n$ matrix $A$ to upper Hessenberg form:
    \begin{equation}
        \tilde{A}=Q_{n-2}\adj\dots Q_1\adj AQ_1\dots Q_{n-2}
    \end{equation}
\end{enumerate}
The pseudocode is
\begin{verbatim}
If Q is desired, set Q=I
for i = 1:n-2
    u_i = house(A(i+1:n, i))
    P_i = I - 2*u_i*u_i'
    A(i+1:n, i:n) = P_i * A(i+1:n, i:n)
    A(1:n, i+1:n) = A(1:n, i+1:n) * P_i
    if Q is desired
        Q(i+1:n, i:n) = P_i * Q(i+1:n, i:n)
    end
end
\end{verbatim}
The total FLOPs: $\frac{10}{3}n^3+\mathcal{O}(n^2)$ for reduction, $\frac{14}{3}n^3+\mathcal{O}(n^2)$ if product $Q=Q_{n-1}\dot Q_1$ is computed too. Given $A$, perform Hessenberg reduction $H=Q\adj AQ$, set $A_0=H$
\begin{verbatim}
while not converged
    pick shift sigma_k
    A - sigma_k*I = Q_k * R_k
    A_{k+1} = R_k * Q_k + sigma_k*I_n
    k = k+1
end
\end{verbatim}
If at least one of the subdiagonals of an upper Hessenberg matrix $H$ is zero, it is called reduced. If $H$ is reduced, we can deflate the problem as 
\begin{equation}
    A_k = \ttmat{A_{11}}{A_{12}}{0}{A_{22}},
\end{equation}
we can decouple the problem into two smaller subproblems $A_{11}$ and $A_{22}$. For Hessenberg matrices, a further theorem allows the QR algorithm to be applied implicitly:
\begin{theorem}[Implicit Q theorem]
    Suppose $Q\tp AQ=H$ is unreduced upper Hessenberg matrix, then columns $2$ to $n$ of $Q$ are determined uniquely (up to signs) by the first column of $Q$.
\end{theorem}
The implication of this theorem is that, to compute $A_{k+1}=Q_k\tp A_k Q_k$ for $A_k$ in the QR algorithm, we just need to
\begin{itemize}
    \item Compute the first column of $Q_k$
    \item Choose other columns so $Q_k$ is unitary and $A_{k+1}$ is upper Hessenberg. We can do this using Givens rotations.
\end{itemize}

We show an example of how to use the implicit Q theorem to compute $A_1$ from $A_0=1$.

\subsection{Symmetric eigenvalue problem}
The Schur form of a real symmetric matrix $A$ is
\begin{equation}
    A=QRQ\adj
\end{equation},
since $A\adj=A$, $R\adj=R$ being upper triangular, so $R$ is diagonal, and eigenvalues are real.

\subsubsection{Tridiagonal QR iteration}
We proceed by finding the $Q$ that makes $A$ tridiagonal:
$T=QAQ\adj$, and apply the QR iteration to $T$ to get a sequence of tridiagonal matrices converging to diagonal form. A shift is needed.

The divide-and-conquer algorithm can be used. Given a tridiagonal matrix $T$, we can write it as 
\begin{align}
    T&=\ttmat{T_1}{0}{0}{T_2}+b_m\begin{bmatrix}
        0\\\vdots\\0\\1\\1\\0\\\vdots\\0
    \end{bmatrix}
    \begin{bmatrix}
        0&\dots&0&1&1&0&\dots&0
    \end{bmatrix}\\
    &=\ttmat{T_1}{0}{0}{T_2}+b_m\bvec{v}\bvec{v}\tp.
\end{align}

\section{Numerical Optimization}
Notes from the Math 571 class taught by Prof. Matthias Chung in Fall 2023, Emory University. The overall aim of this course is to optimize a real-valued objective function $f(\bvec{x})$ with $\bvec{x}\subset\mathbb{R}^{n}$ with $n$ being finite, in the normed, real vector space (feasible set) $\mathcal{X}\subset\mathcal{V}$:
\begin{equation}
    \min_{\bvec{x}\in\mathcal{X}}f(\bvec{x}).
\end{equation}
If $\mathcal{X}=\mathbb{R}^n$, then we have an unconstrained optimization problem, otherwise, we have a constrained optimization problem. The former is typically easier to solve, and can be denoted simply as 
\begin{equation}
    \min f(\bvec{x}).
\end{equation}

\begin{theorem}
    Let $\mathcal{X}\in\mathbb{R}^n$ be non-empty and compact, $f:\mathbb{R}^n\rightarrow\mathbb{R}$ continuous, then there exists a global minimum of $f$ on $\mathcal{X}$.
\end{theorem}
\begin{proof}
    There exists a sequence $\{x_k\} \in \mathcal{X}$ where $f(x_k)\rightarrow\inf f(x)$. Since $\mathcal{X}$ is compact, therefore there exists a subsequence $x_{k_j}\rightarrow\hat{x}\in\mathcal{X}$ for $j\rightarrow\infty$. $f$ is continuous, we have $f(x_{k_j})\rightarrow f(\hat{x})$ \hl{fill in}
\end{proof}

\begin{theorem}[Necessary conditions for optimality]
There are two necesary conditions:
    \begin{itemize}
        \item \textbf{First order necessary condition.} Let $f\in\mathcal{C}^1(\mathbb{R}^n,\mathbb{R})$ and $\hat{x}\in\mathbb{R}^n$ local minimizer of $f$, then $\nabla f(\hat{\bvec{x}})=\bvec{0}$.
        \item \textbf{Second order necessary condition.} Let $f\in\mathcal{C}^2(\mathbb{R}^n,\mathbb{R})$ and $\hat{x}\in\mathbb{R}^n$ local minimizer of $f$, then $\nabla^2 f(\hat{\bvec{x}})$ is symmetric positive semi-definite.
    \end{itemize}
\end{theorem}
\begin{proof}
    We prove by contradiction.
    \begin{itemize}
        \item Assume $\hat{\bvec{x}}$ is local minimizer, and that $\nabla f(\hat{\bvec{x}})\neq\bvec{0}$. Define $\phi(\alpha)=f(\hat{\bvec{x}}+\alpha\bvec{d})$ with $\alpha>0$ and $\bvec{d}$ arb. Taylor's gives
        \begin{equation}
            \phi(\alpha)=\phi(0)+\alpha\phi'(0)+\mathcal{O}(\alpha^2),
        \end{equation}
        where
        \begin{equation}
            \phi'(0)=\bvec{d}\tp\nabla f(\hat{\bvec{x}}),
        \end{equation}
        so
        \begin{equation}
            \phi(\alpha)=f(\hat{\bvec{x}})+\alpha \bvec{d}\tp\nabla f(\hat{\bvec{x}})+\mathcal{O}(\alpha^2).
        \end{equation}
        Let's pick $\bvec{d}=-\nabla f(\hat{\bvec{x}})$ so
        \begin{equation}
            \bvec{d}\tp\nabla f(\hat{\bvec{x}})=-||\nabla f(\hat{\bvec{x}})||_2^2<0.
        \end{equation}
        Finally,
        \begin{align}
            f(\hat{\bvec{x}}+\alpha\bvec{d})&=f(\hat{\bvec{x}})-\alpha ||\nabla f(\hat{\bvec{x}})||_2^2 +\mathcal{O}(\alpha^2)\\
            \frac{ f(\hat{\bvec{x}}+\alpha\bvec{d})-f(\hat{\bvec{x}})}{\alpha}&=- ||\nabla f(\hat{\bvec{x}})||_2^2 +\frac{\mathcal{O}(\alpha^2)}{\alpha}
        \end{align}
        Limit process $\alpha\rightarrow0$ gives the directional derivative in the direction $\bvec{d}$:
        \begin{equation}
            \lim_{\alpha\rightarrow0}\frac{ f(\hat{\bvec{x}}+\alpha\bvec{d})-f(\hat{\bvec{x}})}{\alpha}=- ||\nabla f(\hat{\bvec{x}})||_2^2 +\lim_{\alpha\rightarrow0}\frac{\mathcal{O}(\alpha^2)}{\alpha}<0.
        \end{equation}
        This means $\hat{\bvec{x}}$ is not a minimizer.
        \item Assume $\hat{\bvec{x}}$ is local minimizer, with $\nabla^2 f(\hat{\bvec{x}})$ symmetric (has to be since $\mathcal{C}^2$) but not positive semi-definite. If that is the case, there must exist $\bvec{d}\in\mathbb{R}^n\setminus\bvec{0}$ that $\bvec{d}\tp\nabla^2 f(\hat{\bvec{x}})\bvec{d}<0$. Taylor's gives
        \begin{align}
            \phi(\alpha)&=\phi(0)+\alpha\phi'(0) +\frac{1}{2}\alpha^2\phi''(0) + \mathcal{O}(\alpha^3)\\
            &=\phi(0)+\frac{1}{2}\alpha^2\phi''(0) + \mathcal{O}(\alpha^3),
        \end{align}
        where
        \begin{equation}
            \phi''(0)=\bvec{d}\tp\nabla^2 f(\hat{\bvec{x}})\bvec{d}<0\Rightarrow\alpha^2\phi''(0)+\mathcal{O}(\alpha^3)<0,
        \end{equation}
        for $\alpha$ sufficiently small.
.    \end{itemize}
\end{proof}

\begin{theorem}[Sufficient condition for optimality]
    Let $f\in\mathcal{C}^2(\mathbb{R}^n,\mathbb{R})$ with $\nabla f(\hat{\bvec{x}})=\bvec{0}$, and $\nabla^2 f(\hat{\bvec{x}})$ be symmetric positive definite, then $\hat{x}\in\mathbb{R}^n$ is a strict local minimizer of $f$.
\end{theorem}
\begin{proof}
    Let the condition hold for $\hat{\bvec{x}}$ and $\bvec{d}\neq\bvec{0}$:
    \begin{equation}
        f(\hat{\bvec{x}}+\bvec{d})=f(\hat{\bvec{x}})+\nabla f(\hat{\bvec{x}})\tp\bvec{d}+\frac{1}{2}\bvec{d}\tp\nabla^2f(\tilde{\bvec{x}})\bvec{d},
    \end{equation}
    where $\tilde{\bvec{x}}=\hat{\bvec{x}}+\hat{\theta}\bvec{d}$ for $0<\hat{\theta}<1$. We add and subtract $\frac{1}{2}\bvec{d}\tp\nabla^2f(\hat{\bvec{x}})\bvec{d}$ to get
    \begin{equation}
        f(\hat{\bvec{x}}+\bvec{d})=f(\hat{\bvec{x}})+\frac{1}{2}\bvec{d}\tp\nabla^2f(\hat{\bvec{x}})\bvec{d}+\frac{1}{2}\bvec{d}\tp\nabla^2[f(\tilde{\bvec{x}})-f(\hat{\bvec{x}})]\bvec{d}.
    \end{equation}
    The Rayleigh principle (the variational principle) gives that
    \begin{equation}
        \frac{1}{2}\bvec{d}\tp\nabla^2f(\hat{\bvec{x}})\bvec{d}\geq \lambda_{\mathrm{min}}||\bvec{d}||^2,
    \end{equation}
    since $\nabla^2 f$ is S.P.D., so $\lambda_{\min}>0$ also.
    
    Since $f\in\mathcal{C}^2$, the Hessian is continuous, so $\nabla^2f(\tilde{\bvec{x}})$ and $\nabla^2f(\hat{\bvec{x}})$ need to become arbitrarily close for $\tilde{\bvec{x}}$ and $\hat{\bvec{x}}$ being close. In other words, there exists $\bar{\theta}$ for $0<\bar{\theta}<\hat{\theta}$ such that
    \begin{equation}
        |\bvec{d}\tp\nabla^2[f({\bvec{x}})-f(\hat{\bvec{x}})]\bvec{d}|<\lambda_{\mathrm{min}}||\bvec{d}||^2
    \end{equation}
    for all $0<\theta\leq\bar{\theta}$. We therefore have
    \begin{align}
        f(\hat{\bvec{x}}+\bvec{d})&\geq f(\hat{\bvec{x}})+\frac{1}{2}\lambda_{\mathrm{min}}||\bvec{d}||^2-\frac{1}{2}|\bvec{d}\tp\nabla^2[f({\bvec{x}})-f(\hat{\bvec{x}})]\bvec{d}|\\
        &> f(\hat{\bvec{x}})+\frac{1}{2}\lambda_{\mathrm{min}}||\bvec{d}||^2-\frac{1}{2}\lambda_{\mathrm{min}}||\bvec{d}||^2=f(\hat{\bvec{x}})
    \end{align}
    for any $\bvec{d}\neq\bvec{0}$.
\end{proof}

\begin{definition}[Convexity]
    Let $\mathcal{X}\in\mathbb{R}^n$ be convex.
    \begin{enumerate}
        \item A function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is called \textbf{convex} on $\mathcal{X}$, if
        \begin{equation}
            f((1-\lambda)\bvec{x}+\lambda\bvec{y})\leq(1-\lambda)f(\bvec{x})+\lambda f(\bvec{y}),
        \end{equation}
        for all $\bvec{x},\bvec{y}\in\mathcal{X}$ and for all $\lambda\in[0,1]$.
        \item A function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is called \textbf{strictly convex} on $\mathcal{X}$, if
        \begin{equation}
            f((1-\lambda)\bvec{x}+\lambda\bvec{y}) < (1-\lambda)f(\bvec{x})+\lambda f(\bvec{y}),
        \end{equation}
        for all $\bvec{x},\bvec{y}\in\mathcal{X}$ and for all $\lambda\in[0,1]$.
        \item A function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is called \textbf{strongly convex} on $\mathcal{X}$, if there exists a $\mu>0$ such that
        \begin{equation}
            f((1-\lambda)\bvec{x}+\lambda\bvec{y}) +\mu\lambda(1-\lambda)||\bvec{x}-\bvec{y}||_2^2 \leq(1-\lambda)f(\bvec{x})+\lambda f(\bvec{y}),
        \end{equation}
        for all $\bvec{x},\bvec{y}\in\mathcal{X}$ and for all $\lambda\in[0,1]$.
    \end{enumerate}
\end{definition}

\begin{property}[Properties of convexity]
    \begin{itemize}
        \item Let $f_1$, $f_2$ be convex functions and $a_1,a_2\geq0$, then $f(\bvec{x})=a_1f_1(\bvec{x})+a_2f_2(\bvec{x})$ is convex.
        \item Let $\bvec{r}:\mathbb{R}^n\rightarrow\mathbb{R}^m$ be an affine linear mapping $\bvec{r}(\bvec{x})=A\bvec{x}-\bvec{b}$. Further let $f:\mathbb{R}^m\rightarrow\mathbb{R}$ be convex, then $g(\bvec{x})=f(A\bvec{x}-\bvec{b})$ is convex.
        \item All norms $||\cdot||:\mathbb{R}^n\rightarrow\mathbb{R}$ are convex.
        \hl{one more}
    \end{itemize}
\end{property}

\begin{theorem}[Uniqueness]
    
\end{theorem}

\section{DMRG}
\subsection{Motivation}
(Adapted from \cite[Ch. 16]{pavariniEmergentPhenomenaCorrelated2013}.) Consider a quantum system that lives on $L$ lattice sites with $d$ local state $\{\sigma_i\}$ on each site. The Hilbert space is then $L^d$-dimensional:
\begin{equation}
    \mathcal{H}=\bigotimes_{i=1}^L\mathcal{H}_i,\ \ \mathcal{H}_i=\{|1_i\ket,\dots,|d_i\ket\}.
\end{equation}
So the most general state is then
\begin{equation}
    |\psi\ket=\sum_{\sigma_1,\dots,\sigma_L}c^{\sigma_1,\dots,\sigma_L}|{\sigma_1,\dots,\sigma_L}\ket.
\end{equation}
We now abbreviate ${\sigma_1,\dots,\sigma_L}$ as $\{\sigma\}$. The exponential growth of the Hilbert space is most commonly tackled with the mean-field approximation:
\begin{equation}
\label{eq:meanfield-tensor}
    c^{\sigma_1,\dots,\sigma_L}\approx c^{\sigma_1}c^{\sigma_2}\dots c^{\sigma_L}.
\end{equation}
This represents a reduction from $d^L$ coefficients to $dL$ coefficients. However, the mean-field approximation do not capture entanglement.
\begin{example}[Entanglement]
    A simple example of entanglement comes from a system of $2$ spin-$\frac{1}{2}$ particles. We have $\mathcal{H}_i=\{|\uparrow_i\ket,|\downarrow_i\ket\}$, and $\mathcal{H}=\mathcal{H}_1\otimes\mathcal{H}_2$. A general state in this system reads
    \begin{equation}
        |\psi\ket=c^{\uparrow\uparrow}|\uparrow\uparrow\ket+c^{\uparrow\downarrow}|\uparrow\downarrow\ket+c^{\downarrow\uparrow}|\downarrow\uparrow\ket+c^{\downarrow\downarrow}|\downarrow\downarrow\ket,
    \end{equation}
    subject to normalization. We can show that some states can be factorized:
    \begin{equation}
        |\psi_1\ket=|\uparrow\uparrow\ket
    \end{equation}
    can be exactly factorized into $c^{\uparrow_1}=c^{\uparrow_2}=1$, while $c^{\downarrow_1}=c^{\downarrow_2}=0$. However, this factorization is plainly impossible for the simple singlet state:
    \begin{equation}
        |\psi_2\ket=\frac{1}{\sqrt{2}}(|\uparrow\downarrow\ket+|\downarrow\uparrow\ket),
    \end{equation}
    as we have to choose one of $\{c^{\uparrow_1},c^{\uparrow_2}\}$ and one of $\{c^{\downarrow_1},c^{\downarrow_2}\}$ to be zero, but that would also mean that the mixed states will have to be zero.
\end{example}

States that factorize are called product states, and all others are called entangled states. Entangled states carry non-local information and superclassical correlations. The goal of DMRG is to generalize the mean-field product state in \Cref{eq:meanfield-tensor} to describe (at least certain) entangled states, while remaining numerically convenient. The most simple generalization would be replacing the $c$'s with matrices:
\begin{equation}
    c^{\sigma_1}c^{\sigma_2}\dots c^{\sigma_L}\rightarrow M^{\sigma_1}M^{\sigma_2}\dots M^{\sigma_L}.
\end{equation}
Here the $M^{\sigma_i}$ are $(2\times 2)$ matrices, except on sites $1$ and $L$, where they have to be $(1\times 2)$ and $(2\times 1)$ vectors respectively, such that the entire thing is still a scalar.

\subsection{Schmidt decomposition}
(Adapted from \cite{schollwockDensitymatrixRenormalizationGroup2011}.) Imagine a bipartite universe AB. An arbitrary state (`pure state') on AB can be written as
\begin{equation}
    \psi\ket =\sum_{ij}\Psi_{ij}|i\ket_{\mathrm{A}}|j\ket_{\mathrm{B}},
\end{equation}
where $\{|i\ket_{\mathrm{A}}\}$ and $\{|j\ket_{\mathrm{B}}\}$ are orthonormal bases of $A$ and $B$ with dimensions $N_{\mathrm{A}}$ and $N_{\mathrm{B}}$. The reduced density operators are defined as $\hat{\rho}_{\mathrm{A}}=\Tr_{\mathrm{B}}|\psi\ket\bra\psi|$ and $\hat{\rho}_{\mathrm{B}}=\Tr_{\mathrm{A}}|\psi\ket\bra\psi|$. We can work the expression of $\hat{\rho}_{\mathrm{A}}$:
\begin{equation}
    \hat{\rho}_{\mathrm{A}}=\Tr_{\mathrm{B}}|\psi\ket\bra\psi|=\sum_{k_{\mathrm{B}}}\bra k_{\mathrm{B}}|\psi\ket\bra\psi|k_{\mathrm{B}}\ket=\sum_{i_{\mathrm{A}}}\Psi_{i_{\mathrm{A}}k_{\mathrm{B}}}\Psi^*_{i_{\mathrm{A}}'k_{\mathrm{B}}}|i_{\mathrm{A}}\ket\bra i'_{\mathrm{A}}|,
\end{equation}
which is to say, in matrix form:
\begin{equation}
    \rho_{\mathrm{A}}=\Psi\Psi\adj,\ \ \rho_{\mathrm{B}}=\Psi\adj\Psi.
\end{equation}

Carrying out an SVD of $\Psi$, we obtain
\begin{align}
    |\psi\ket&=\sum_{ij}\sum_{a=1}^{\min(N_{\mathrm{A}},N_{\mathrm{B}})}U_{ia}\sigma_{\mathrm{A}}V_{ja}^*|i\ket_{\mathrm{A}}|j\ket_{\mathrm{B}}\\
    &=\sum_{a=1}^{\min(N_{\mathrm{A}},N_{\mathrm{B}})}\left(\sum_iU_{ia}|i\ket_{\mathrm{A}}\right)\sigma_{\mathrm{A}}\left(\sum_jV_{ja}^*|j\ket_{\mathrm{B}}\right)\\
    &\equiv\sum_{a=1}^{\min(N_{\mathrm{A}},N_{\mathrm{B}})}\sigma_{\mathrm{A}}|a\ket_{\mathrm{A}}|a\ket_{\mathrm{B}},
\end{align}
If we so wish, we could extend the sets $\{ |a\ket_{\mathrm{A}}\}$ and $\{|a\ket_{\mathrm{B}}\}$ to span the whole spaces A and B. But for now, we restrict the sum to run only over the $r\leq\min(N_{\mathrm{A}},N_{\mathrm{B}})$ positive singular values to obtain the Schmidt decomposition of the pure state:
\begin{equation}
    |\psi\ket=\sum_{a=1}^rs_{\mathrm{A}}|a\ket_{\mathrm{A}}|a\ket_{\mathrm{B}}.
\end{equation}
The $r=1$ and $r>1$ cases correspond to product and entangled states. The expressions of the reduced density operators are now simple:
\begin{equation}
    \hat{\rho}_{\mathrm{A}}=\sum_{a=1}^rs_{\mathrm{A}}^2|a_{\mathrm{A}}\ket \bra a_{\mathrm{A}}|,\ \ \hat{\rho}_{\mathrm{B}}=\sum_{a=1}^rs_{\mathrm{A}}^2|a_{\mathrm{B}}\ket \bra a_B|.
\end{equation}
The von Neumann entropy of entanglement is also easily obtained:
\begin{equation}
    S_{A|B}(|\psi\ket)=-\Tr\hat{\rho}_{\mathrm{A}}\log_2\hat{\rho}_{\mathrm{A}}=-\sum_{a=1}^rs_{\mathrm{A}}^2\log_2s_{\mathrm{A}}^2.
\end{equation}

The use of SVD in the Schmidt decomposition naturally leads to the approximation problem in view of potentially large Hilbert spaces: we first observe that if the orignal basis sets are orthonormal (which they are here), then the $2$-norm of the vector $|\psi\ket$ is identical to the Frobenius norm of the matrix $\Psi$. The approximation can then be written as
\begin{equation}
    |\tilde{\psi}\ket=\sum_{a=1}^{r'}s_a|a\ket_{\mathrm{A}}|a\ket_{\mathrm{B}},
\end{equation}
where $r'<r$, and $s_a$ needs to be rescaled to preserve normalization.

\subsection{The MPS Ansatz}
(Adapted from \cite{schollwockDensitymatrixRenormalizationGroup2011}.)
\begin{definition}[MPS Ansatz]
    The general matrix product state Ansatz is given by
    \begin{equation}
    \label{eq:mps-ansatz}
        |\psi\ket=\sum_{\sigma_1,\dots,\sigma_L}M^{\sigma_1}M^{\sigma_2}\dots M^{\sigma_L}|\sigma_1\sigma_2\dots\sigma_L\ket,
    \end{equation}
    where we have introduced $dL$ matrices, $d$ at each of the $L$ sites. The dimensions are $(1\times D_1),(D_1\times D_2),\dots,(D_{L-1}\times 1)$, and dimensions $\{D_i\}$ are free hyperparameters for now.
\end{definition}

\begin{theorem}[Gauge freedom of the MPS Ansatz]
    A given state $|\psi\ket$ does not have a unique decomposition into matrices $M^{\sigma_i}$.
\end{theorem}
\begin{proof}
    Consider an arbitrary invertible matrix $X$ of dimension $(D_i\times D_i)$. The MPS state does not change under the insertion of $XX^{-1}=1$ between matrices $M^{\sigma_i}$ and $M^{\sigma_{i+1}}$. This implies a Gauge transformation
    \begin{equation}
        M^{\sigma_i}\rightarrow M^{\sigma_i}X,\ \ M^{\sigma_{i+1}}\rightarrow X^{-1}M^{\sigma_{i+1}}.
    \end{equation}
\end{proof}

\begin{theorem}[Universality of the MPS Ansatz]
    Every state can be, in principle, represented as an MPS.
\end{theorem}
\begin{proof}
    We accomplish this proof by successively `peeling off' site after site, starting from the left. Consider the $d^L$-dimensional coefficient vector containing $c^{\{\sigma\}}$: we can reshape it into a $(d\times d^{L-1})$-dimensional matrix $\Psi$, and then perform a reduced SVD (see \Cref{def:reduced-svd}) on it (commas separate ranks/(composite) indices, so one comma indicate a rank-$2$ tensor, for example):
    \begin{equation}
        c_{\sigma_1\sigma_2\dots\sigma_L}=\Psi_{\sigma_1,\sigma_2\dots\sigma_L}=\sum^{r_1}_{a_1}U_{\sigma_1,a_1}\Sigma_{a_1,a_1}(V\adj)_{a_1,\sigma_2\dots\sigma_L}\equiv \sum^{r_1}_{a_1} U_{\sigma_1,a_1}c_{a_1\sigma_2\dots\sigma_L},
    \end{equation}    
    where $r_1\leq d$ is the rank of $\Psi$. Now, we further slice the matrix $U$ into a collection of $d$ row vectors $A^{\sigma_1}$ with entries $A_{a_1}^{\sigma_1}=U_{\sigma_1,a_1}$. Then, we reshape $c_{a_1\sigma_2\dots\sigma_L}$ into a matrix $\Psi_{(a_1\sigma_2),(\sigma_3\dots\sigma_L)}$ of dimension $(r_1d\times d^{L-2})$, such that 
    \begin{equation}
        c_{\sigma_1\sigma_2\dots\sigma_L}=\sum_{a_1}^{r_1}A_{a_1}^{\sigma_1}\Psi_{(a_1\sigma_2),(\sigma_3\dots\sigma_L)}.
    \end{equation}
    We then perform another SVD on $\Psi$ to give
    \begin{equation}
        c_{\sigma_1\sigma_2\dots\sigma_L}=\sum_{a_1}^{r_1}\sum_{a_2}^{r_2}A_{a_1}^{\sigma_1}U_{(a_1\sigma_2),a_2}\Sigma_{a_2,a_2}(V\adj)_{a_2,(\sigma_3,\dots,\sigma_L)}=\sum_{a_1}^{r_1}\sum_{a_2}^{r_2}A_{a_1}^{\sigma_1}A_{a_1,a_2}^{\sigma_2}\Psi_{(a_2\sigma_3),(\sigma_4\dots\sigma_L)},
    \end{equation}
    where $U$ is replaced by a set of $d$ \textit{matrices} $A^{\sigma_2}$ of dimension $(r_1\times r_2)$ with entries $A_{a_1,a_2}^{\sigma_2}=U_{(a_1\sigma_2),a_2}$, and $r_2\leq r_1d\leq d^2$.

    We can see now that we could repeat this process with further SVDs until all sites have been `peeled off', to finally obtain
    \begin{equation}
        c_{\sigma_1\sigma_2\dots\sigma_L}\sum_{a_1,\dots,a_{L-1}}A_{a_1}^{\sigma_1}A_{a_1,a_2}^{\sigma_2}\dots A_{a_{L-2},a_{L-1}}^{\sigma_{L-1}}A^{\sigma_L}_{a_{L-1}}.
    \end{equation}
    We have therefore demonstrated \Cref{eq:mps-ansatz} can represent a general wavefunction. This is called the \textit{left-canonical matrix product state}.
\end{proof}

Now we look at the $A$ matrices a little closer: assuming that all of the $\Psi$ tensors have full rank, \textit{i.e.}, $r_i=d$, then the maximal dimensions of the $A$ matrices are $(1\times d),(d\times d^2),\dots,(d^{L/2-1}\times d^{L/2}),(d^{L/2}\times d^{L-/2-1}),\dots,(d^2\times d),(d\times 1)$, assuming $L$ is even. This shows that the exact decomposition is infeasible in realistic calculations. The condition enforced by $U\adj U$ in an SVD also ensures that 
\begin{equation}
    \sum_{a_{l-1}\sigma_l}(U\adj)_{a_l,(a_{l-1}\sigma_l)}U_{(a_{l-1}\sigma_l),a_l'}=\sum_{a_{l-1}\sigma_l}(A^{\sigma_l,\dagger})_{a_l,a_{l-1}}A^{\sigma_l}_{a_{l-1},a_l'}=\sum_{\sigma_l}(A^{\sigma_l,\dagger}A^{\sigma_l})_{a_l,a_l'}=\delta_{a_l,a_l'},
\end{equation}
or equivalently,
\begin{equation}
    \sum_{\sigma_l}A^{\sigma_l,\dagger}A^{\sigma_l}=I.
\end{equation}
Matrices that obey this condition is known as being \textit{left-normalized}, and MPSes that consist only of left-normalized matrices are called \textit{left-canonical}.

If we split up the lattice into A and B blocks, where A contains sites $1$ to $l$, and B sites $l+1$ to $L$. We can introduce states
\begin{align}
    &|a_l\ket_{\mathrm{A}}=\sum_{\sigma_1\dots\sigma_l}(A^{\sigma_1}A^{\sigma_2}\dots A^{\sigma_l})_{1,a_l}|\sigma_1,\dots,\sigma_l\ket,\\
    &|a_l\ket_{\mathrm{B}}=\sum_{\sigma_{l+1}\dots\sigma_L}(A^{\sigma_{l+1}}A^{\sigma_{l+2}}\dots A^{\sigma_L})_{a_l,1}|\sigma_{l+1},\dots,\sigma_L\ket,
\end{align}
such that the MPS can be written as
\begin{equation}
    |\psi\ket=\sum_{a_l}|a_l\ket_{\mathrm{A}}|a_l\ket_{\mathrm{B}}.
\end{equation}
This looks like a Schmidt decomposition of $|\psi\ket$ but is not, because while $\{|a_l\ket_{\mathrm{A}}\}$ are orthonormal due to the left-normalization of the $A$ matrices, $\{|a_l\ket_{\mathrm{B}}\}$ aren't, since $\sum_{\sigma}A^{\sigma}A^{\sigma,\dagger}\neq I$.

A completely equivalent procedure can be used to obtain the \textit{right-canonical MPS}, by starting from the right, and sequentially decomposing $U\Sigma$ instead of $\Sigma V\adj$, we can obtain
\begin{equation}
    |\psi\ket=\sum_{\sigma_1\dots\sigma_L}B^{\sigma_1}B^{\sigma_2}\dots B^{\sigma_L}|\sigma_1,\dots,\sigma_L\ket,
\end{equation}
where the $B$ matrices are of the same dimensions as the $A$ matrices, but are \textit{right-normalized}:
\begin{equation}
    \sum_{\sigma_l}B^{\sigma_l}B^{\sigma_l,\dagger}=I.
\end{equation}

Finally, we can peel off from both ends, and meet in the middle, to obtain the \textit{mixed-canonical MPS}:
\begin{equation}
    c_{\sigma_1\sigma_2\dots\sigma_L}=A^{\sigma_1}\dots A^{\sigma_l}\Sigma B^{\sigma_{l+1}}\dots B^{\sigma_L}.
\end{equation}
In this form, we can obtain the Schmidt decomposition of $|\psi\ket$. If we introduce states
\begin{align}
    &|a_l\ket_{\mathrm{A}}=\sum_{\sigma_1\dots\sigma_l}(A^{\sigma_1}A^{\sigma_2}\dots A^{\sigma_l})_{1,a_l}|\sigma_1,\dots,\sigma_l\ket,\\
    &|a_l\ket_{\mathrm{B}}=\sum_{\sigma_{l+1}\dots\sigma_L}(B^{\sigma_{l+1}}B^{\sigma_{l+2}}\dots B^{\sigma_L})_{a_l,1}|\sigma_{l+1},\dots,\sigma_L\ket,
\end{align}
then we have
\begin{equation}
    |\psi\ket=\sum_{a_l}\sigma_a|a_l\ket_A|a_l\ket_B.
\end{equation}

\section{Time-dependent methods}
\subsection{Basic time-dependent quantum mechanics}
For time dependent Hamiltonian $H(t)$ that satisfies $[H(t_1), H(t_2)]=0$, the time-evolution operator is 
\begin{equation}
    U(t)=e^{\frac{1}{i} \int_{t_0}^{t}H(t')\dl t'}.
\end{equation}
If $[H(t_1), H(t_2)]\neq 0$,
\begin{equation}
    U(t,t_0)=U^{(0)}(t,t_0)+U^{(1)}(t,t_0)+\dots,
\end{equation}
\begin{equation}
    U(t,t_0)=1+\frac{1}{i}\int_{t_0}^tH(t_1)U(t,t_0)\dl t_1.
\end{equation}
where $U^{(0)}=1$. Plugging in $U^{(0)}$ into the integral gives $U^{[1]}$:
\begin{equation}
    U^{[1]}=1+\frac{1}{i}\int_{t_0}^tH(t_1)\dl t_1.
\end{equation}
We can proceed to plug in $U^{[1]}$ into the integral:
\begin{equation}
    U^{[2]}=1+\frac{1}{i}\int_{t_0}^tH(t_1)\dl t_1+\frac{1}{i}\int_{t_0}^t\dl t_1 H(t_1)\frac{1}{i}\int_{t_0}^{t_1}\dl t_2 H(t_2).
\end{equation}
The $U^{(2)}$ term can be re-written as 
\begin{equation}
    \frac{1}{i^2}\int_{t_0}^t\dl t_2 \int^{t_2}_{t_0}\dl t_1 H(t_2)H(t_1).
\end{equation}
We can generalize and see that
\begin{equation}
    U^{(m)}=\frac{1}{i^m}\int_{t_0}^t\dl t_m\int^{t_m}_{t_0}\dl t_{m-1}\dots\int_{t_0}^{t_2}\dl t_1 H(t_m)\dots H(t_1).
\end{equation}
Introducing the time-ordering operator $\mathcal{T}$:
\begin{equation}
    \mathcal{T}A(5)B(3)=A(5)B(3),\ \mathcal{T}A(2)B(7)=B(7)A(2).
\end{equation}
Furthermore,
\begin{align}
    \mathcal{T}\left(\frac{1}{i}\int_{t_0}^t\dl t_1H(t_1)\right)^2&=\frac{1}{i^2}\mathcal{T}\int_{t_0}^t\dl t_1\int_{t_0}^t\dl t_2 H(t_1)H(t_2)\\
    &=\frac{2}{i^2}\mathcal{T}\int_{t_0}^t\dl t_2\int_{t_0}^{t_2}\dl t_1\mathcal{T}H(t_1)H(t_2).
\end{align}
We can now show that
\begin{align}
    U(t,t_0)&=\sum_{m=0}^{\infty}U^{(m)}(t,t_0)=\sum_{m=0}^{\infty}\frac{1}{m!}\left(\frac{1}{i}\int_{t_0}^t\dl t_1H(t_1)\right)^m\\
    &=\mathcal{T}\exp\left(\frac{1}{i}\int_{t_0}^t\dl t_1H(t_1)\right).
\end{align}
This is known as the Dyson series.
\subsection{The Heisenberg picture}
(Adapted from the \href{https://en.wikipedia.org/wiki/Heisenberg_picture}{Wikipedia page}) The familiar Sch{\" o}dinger picture of quantum mechanics says that the state of a system evolves with time, and this evolution is brought by a unitary time-evolution operator:
\begin{equation}
|\psi(t)\ket =U(t,t_0)|\psi(t_0)\ket.
\end{equation}

For a time-independent Hamiltonian, the time-evolution operator is given by
\begin{equation}
U(t,t_0)=e^{-i{H}(t-t_0)/\hbar},
\end{equation}
via the operator equation
\begin{equation}
    i\hbar\diff{}{t} U(t,t_0)={H}{U}(t,t_0).
\end{equation}

The Sch{\" o}dinger picture is useful when dealing with such time-independent Hamiltonians. For time-dependent or relativistic Hamiltonians, the equivalent Heisenberg picture is sometimes more useful. In the Heisenberg picture, the states do not change with time, while the observables do, via the Heisenberg equation:
\begin{equation}
\label{eq:heisenberg_eq}
\diff{}{t}A_{\mathrm{H}}(t)=\frac{i}{\hbar}[H_{\mathrm{H}},A_{\mathrm{H}}(t)]+\left( \diffp{A_{\mathrm{S}}}{t} \right)_{\mathrm{H}},
\end{equation}
where `H' and `S' label observables in the Heisenberg and Schr{\" o}dinger picture respectively.

In the Schr{\" o}dinger picture, the expectation value of an observable $A$ for a given Schr{\" o}dinger state $|\psi(t)\ket$ is
\begin{equation}
\bra A\ket_t=\bra\psi(t)|A|\psi(t)\ket.
\end{equation}

In the Heisenberg picture, all state vectors remain constant, whereas the operators evolve with time according to
\begin{equation}
    A(t)=U^{\dagger}(t)AU(t).
\end{equation}

We can then write the Schr{\" o}dinger equation for the time-evolution operator $U$ as
\begin{equation}
\diff{}{t} U(t)=\frac{i}{\hbar}HU(t).
\end{equation}

We can now perform the time derivative on the LHS of the Heisenberg equation, \Cref{eq:heisenberg_eq}:
\begin{align}
\diff{}{t} A(t) &=\frac{i}{\hbar}U^{\dagger}(t)HAU(t)+U^{\dagger}(t)\left( \diffp{A}{t} \right)U(t)+\frac{i}{\hbar}U^{\dagger}(t)A(-H)U(t)\\
&=\frac{i}{\hbar}U^{\dagger}(t)HU(t)U^{\dagger}(t)AU(t)+U^{\dagger}(t)\left( \diffp{A}{t} \right)U(t)-\frac{i}{\hbar}U^{\dagger}(t)AU(t)U^{\dagger}(t)HU(t)\\
&=\frac{i}{\hbar}(H(t)A(t)-A(t)H(t))+U^{\dagger}(t)\left( \diffp{A}{t} \right)U(t),
\end{align}
which is the RHS of the Heisenberg equation.

If the Hamiltonian is time-independent, then $U(t)=\exp(-iHt/\hbar)$, and we can see the expectation values agrees with the Schr{\" o}dinger picture:
\begin{equation}
\bra A\ket_t=\bra\psi(0)|e^{iHt/\hbar}Ae^{-iHt/\hbar}|\psi(0)\ket\equiv\bra\psi(t)|A|\psi(t)\ket,
\end{equation}
and for an $A(0)$ with no time dependence,
\begin{equation}
    \diff{A}{t}=\frac{i}{\hbar}[H,A(t)].
\end{equation}

The interaction picture starts from a partition of the time-independent Hamiltonian into a soluble part and a fluctuation potential:
\begin{equation}
    H=H_0+H_1(t),
\end{equation}
one can define the \textit{interaction state vector} as
\begin{equation}
    |\Psi_{\mathrm{I}}(t)\ket\equiv e^{iH_0t/\hbar}|\Psi_{\mathrm{S}}(t)\ket,
\end{equation}
we can obtain the equation of motion for this state vector by taking its time derivative:
\begin{align}
    i\hbar\diff{}{t}|\Psi_{\mathrm{I}}(t)\ket&=-H_0e^{iH_0t/\hbar}|\Psi_{\mathrm{S}}(t)\ket+e^{iH_0t/\hbar}i\hbar\diff{}{t}|\Psi_{\mathrm{S}}(t)\ket\\
    &=e^{iH_0t/\hbar}[-H_0+H_0+H_1]e^{-iH_0t/\hbar}|\Psi_{\mathrm{I}}(t)\ket,
\end{align}
which gives us the interaction picture equation:
\begin{align}
    i\hbar\diff{}{t}|\Psi_{\mathrm{I}}(t)\ket=H_1(t)|\Psi_{\mathrm{I}}(t)\ket \\
    H_1(t)\equiv e^{iH_0t/\hbar}H_1e^{-iH_0t/\hbar}.
\end{align}
The time-propagation operation is 
\begin{equation}
    U_{\mathrm{I}}(t,t_0)=\mathcal{T}\exp\left(\frac{1}{i}\int_{t_0}^tV_{\mathrm{I}}(t_1)\right).
\end{equation}

\subsection{Two-level systems}
Given a two-level system described by basis $\{|0\ket,|1\ket\}$, with Hamiltonian given by
\begin{equation}
    H=\ttmat{E_0}{V(t)}{V^*(t)}{E_1}.
\end{equation}
We'd like to know the time evolution of a general state $|\Psi\ket=c_0|0\ket+c_1|1\ket$. We can divide up the Hamiltonian as per the interaction picture:
\begin{equation}
    H=H_0+V=\ttmat{E_0}{0}{0}{E_1}+\ttmat{0}{V}{V^*}{0}.
\end{equation}
The interaction picture perturbation is given by
\begin{equation}
    V_{\mathrm{I}}(t)=e^{-itH_0}V(t)e^{itH_0}=V(t)+[itH_0,V]+\frac{1}{2!}[itH_0,[itH_0,V]]+\dots,
\end{equation}
or equivalently, by directly matrix multiplication
\begin{equation}
    V_{\mathrm{I}}(t)=\ttmat{0}{Ve^{i\omega t}}{V^*e^{-i\omega t}}{0}.
\end{equation}
Plugging $|\Psi(t)\ket$ into the interaction picture Schrodinger equation gives
\begin{align}
    i\diffp{}{t}|\Psi(t)\ket &= i\dot{c_0}(t)|0\ket+i\dot{c_1}(t)|1\ket=V_{\mathrm{I}}(t)|\Psi(t)\ket\\
    &=c_0(t)V^*e^{-i\omega t}|1\ket + c_1(t)Ve^{i\omega t}|0\ket.
\end{align}
We obtain the coupled ODEs:
\begin{align}
    &i\dot{c_0}(t)=c_1(t)Ve^{i\omega t}\\
    &i\dot{c_1}(t)=c_0(t)V^*e^{-i\omega t}
\end{align}
The solutions for the initial state $|\Psi(0)\ket=|0\ket$ are
\begin{align}
    c_0(t)&=\frac{\gamma-\omega}{2\gamma}e^{it(\omega+\gamma)/2}+\frac{\gamma+\omega}{2\gamma}e^{it(\omega-\gamma)/2}\\
    c_1(t)&=\frac{-1}{4\gamma V}(\omega+\gamma)(\gamma-\omega)\left[e^{it(\gamma-\omega)/2} - e^{-it(\omega+\gamma)/2}  \right],
\end{align}
where $\gamma=\sqrt{\omega^2+4|V|^2}$. Furthermore,
\begin{align}
    |c_0(t)|^2&=\frac{\gamma^2+\omega^2+(\gamma^2-\omega^2)\cos(\gamma t)}{2\gamma^2},\\
    |c_1(t)|^2&=\frac{4|V|^2}{\omega^2+4|V|^2}\sin^2\left( \frac{t}{2}\sqrt{ \omega^2+4|V|^2 } \right).
\end{align}

\subsection{Configuration interaction singles}
The CIS Ansatz is simply \cite{dreuwSingleReferenceInitioMethods2005}
\begin{equation}
    |\Psi^{\mathrm{CIS}}\ket=\hat{C}_1|\Phi_0\ket,
\end{equation}
notice the lack of the Hartree--Fock determinant in the Ansatz, due to Brillouin's theorem. Projecting the Ansatz onto the singles manifold, we can write the energy equation
\begin{equation}
\sum_{ia}\bra \Phi_j^b|\hat{H}|\Phi_i^a\ket c_i^a=E_{\mathrm{CIS}}\sum_{ia}c_i^a\delta_{ij}\delta_{ab},
\end{equation}
where
\begin{equation}
    \bra \Phi_j^b|\hat{H}|\Phi_i^a\ket=(E_0+\epsilon_a-\epsilon_i)\delta_{ij}\delta_{ab}+ (ia||jb).
\end{equation}
Subtracting the Hartree--Fock energy, we obtain
\begin{equation}
\sum_{ia}[(\epsilon_a-\epsilon_i)\delta_{ij}\delta_{ab}+ (ia||jb)]c_i^a=\omega_{\mathrm{CIS}}\sum_{ia}c_j^b.
\end{equation}
This can be cast into matrix form:
\begin{equation}
AX=\omega X,
\end{equation}
where
\begin{equation}
A_{ia,jb}=(\epsilon_a-\epsilon_i)\delta_{ij}\delta_{ab}+ (ia||jb),
\end{equation}
and
\begin{equation}
    X_{ia,k}=c_i^{a,(k)},
\end{equation}
collect the coefficients $k$-th eigenstate.

CIS tend to overestimate excitation energies, but provide a well-defined approximation.

\subsection{Time-dependent Hartree--Fock}
TDHF usually, and in this case, refer to the Hartree--Fock equations obtained through a linear-response formalism. It is also known as the random phase approximation (RPA) \cite{dreuwSingleReferenceInitioMethods2005}. Starting from the TDSE
\begin{equation}
\hat{H}\Psi(\bm{r},t)=i\diffp{}{t}\Psi(\bm{r},t),
\end{equation}
where the Hamiltonian is the usual time-independent electronic Hamiltonian plus an arbitrary one-body time-dependent potential:
\begin{equation}
\hat{H}(\bm{r},t)=\hat{H}(\bm{r})+\hat{V}(\bm{r},t).
\end{equation}
The Hartree--Fock approximation reads that $\Psi\rightarrow\Phi$, where
\begin{equation}
\Phi(\bm{r},t)=|\phi_1(\bm{r},t)\phi_2(\bm{r},t)\dots\phi_N(\bm{r},t)|.
\end{equation}
We can then write the time-dependent Hartree--Fock equation as
\begin{equation}
\hat{F}(\bm{r},t)\Phi(\bm{r},t)=i\diffp{}{t}\Phi(\bm{r},t)
\end{equation}

(Adapted from \href{https://joshuagoings.com/2013/05/03/derivation-of-time-dependent-hartree-fock-tdhf-equations/}{here}.) In matrix form, we can write
\begin{align}
i\diffp{}{t}\bvec{C}&=\bvec{F}\bvec{C}\\
-i\diffp{}{t}\bvec{C}^{\dagger}&=\bvec{C}^{\dagger}\bvec{F},
\end{align}
and chain rule gives
\begin{equation}
    i\diffp{}{t}\left( \bvec{C}\bvec{C}^{\dagger} \right)=i\left( \diffp{}{t}\bvec{C} \right)\bvec{C}^{\dagger}+i\bvec{C}\left( \diffp{}{t}\bvec{C}^{\dagger} \right).
\end{equation}
Noting that the density matrix is $\bvec{P}=\bvec{C}\bvec{C}^{\dagger}$, we can obtain the Dirac form of the TD-HF equations:
\begin{equation}
\label{eq:dirac-td-hf}
    [\bvec{F},\bvec{P}]=i\diffp{}{t}\bvec{P}.
\end{equation}
Denoting the unperturbed quantities with $^{(0)}$, we have
\begin{align}
    F_{pq}^{(0)}&=H^{\mathrm{core}}_{pq}+\sum_{rs}P_{rs}(pq||sr)\\
                &=\delta_{pq}\epsilon_p\\
    P_{ij}^{(0)}&=\delta_{ij}\\
    P_{pq}^{(0)}&=0,\ \forall\{p,q\}\notin\mathrm{occ}.
\end{align}

We now expand $\bvec{P}$ and $\bvec{F}$ to first order:
\begin{equation}
    \bvec{P}=\bvec{P}^{(0)}+\bvec{P}^{(1)}\\
    \bvec{F}=\bvec{F}^{(0)}+\bvec{F}^{(1)}.
\end{equation}
\Cref{eq:dirac-td-hf} now reads, after collecting the first-order terms:
\begin{equation}
    [\bvec{F}^{(0)},\bvec{P}^{(1)}]+[\bvec{F}^{(1)},\bvec{P}^{(0)}]=i\diffp{}{t}\bvec{P}^{(1)}.
\end{equation}

For reasons beyond my understanding, the time-dependent perturbation is given by
\begin{equation}
    g_{pq}(t)=\frac{1}{2}(f_{pq}e^{-i\omega t}+f_{qp}^*e^{i\omega t}),
\end{equation}
where $g$ and $f$ are not to be confused with the two electron integrals and the Fock matrix. This perturbation contributes to the first-order change in the Fock matrix. The Fock matrix also changes to first order due to the first-order change in the density matrix, and this is given by
\begin{equation}
\Delta F_{pq}^{(0)}=\sum_{st}\diffp{F_{pq}^{(0)}}{P_{st}}P_{st}^{(1)},
\end{equation}
which together with the perturbation gives the first-order change in the Fock matrix as 
\begin{equation}
    F^{(1)}_{pq}=g_{pq}+\Delta F_{pq}^{(0)}.
\end{equation}
The first-order density response, is similarly given by
\begin{equation}
    P_{pq}^{(1)}=\frac{1}{2}(d_{pq}e^{-i\omega t}+d_{qp}^*e^{i\omega t}).
\end{equation}
Here, $d$ and $g$ are simply unknowns, which will be solved for in a diagonalization problem later.

Plugging in the definitions for the first-order quantities, and collecting the terms containing $e^{-i\omega t}$ gives:
\begin{equation}
\sum_{q}\left[F_{pq}^{(0)}d_{qr}-d_{pq}F_{qr}^{(0)} +\left( f_{pq}+\sum_{st}\diffp{F_{pq}^{(0)}}{P_{st}}d_{st} \right)P_{qr}^{(0)}-P_{pq}^{(0)}\left( f_{qr}+\sum_{st}\diffp{F_{qr}^{(0)}}{P_{st}}d_{st} \right) \right]=\omega d_{pr}.
\end{equation}
Since the density matrix is idempotent ($\bvec{C}\bvec{C}^{\dagger}=\bvec{I}$), we can write 
\begin{align}
(\bvec{P}^{(0)}+\lambda \bvec{P}^{(1)}+\dots)(\bvec{P}^{(0)}+\lambda \bvec{P}^{(1)}+\dots)&=(\bvec{P}^{(0)}+\lambda \bvec{P}^{(1)}+\dots)\\
\bvec{P}^{(0)}\bvec{P}^{(1)}+\bvec{P}^{(1)}\bvec{P}^{(0)}&=\bvec{P}^{(1)},
\end{align}
where the second line collected the terms to first order in $\lambda$. We can hence show that the diagonal terms $d_{ii}\propto P_{ii}^{(1)}$ and $d_{aa}\propto P_{aa}^{(1)}$ must vanish. Since the zeroth order Fock matrix is diagonal, we can write the following set of coupled equations
\begin{align}
&F_{aa}^{(0)}x_{ai}-x_{ai}F_{ii}^{(0)}+\left( f_{ai}+\sum_{bj}\left[ \diffp{F_{ai}^{(0)}}{P_{bj}}x_{bj}+\diffp{F_{ai}^{(0)}}{P_{jb}}y_{bj} \right] \right)P_{ii}^{(0)}=\omega x_{ai}\\
&F_{ii}^{(0)}y_{ai}-y_{ai}F_{aa}^{(0)}-\left( f_{ia}+\sum_{bj}\left[ \diffp{F_{ia}^{(0)}}{P_{bj}}x_{bj}+\diffp{F_{ia}^{(0)}}{P_{jb}}y_{bj} \right] \right)P_{ii}^{(0)}=\omega y_{ai},
\end{align}
where $x_{ai}=d_{ai}$ and $y_{ai}=d_{ia}$ due to convention. The derivatives are
\begin{equation}
\diffp{F_{pq}}{P_{rs}}=\diffp{}{P_{rs}}(H^{\mathrm{core}}_{pq}+\sum_{tu}P_{tu}(pq||ut))=(pq||sr).
\end{equation}
Taking the `zero-frequency limit', which is apparently to assume that transitions occur for a infinitesimal perturbation, \textit{i.e.}, resonance (?), we get to the final TD-HF equations:
\begin{equation}
\ttmat{\bvec{A}}{\bvec{B}}{\bvec{B}^*}{\bvec{A}^*}\tvec{\bvec{X}}{\bvec{Y}}=\omega\ttmat{1}{0}{0}{-1}\tvec{\bvec{X}}{\bvec{Y}},
\end{equation}
where
\begin{align}
A_{ia,jb}&=\delta_{ij}\delta_{ab}(\epsilon_a-\epsilon_i)+(ai||jb)\\
B_{ia,jb}&=(ai||bj).
\end{align}

\subsection{Real-time TDHF}
Starting from the Dirac form of the TDHF equation
\begin{equation}
    i\diff{\bvec{P}}{t}=[\hat{F}(t),\bvec{\Gamma}(t)],
\end{equation}
where the one-particle density operator $\bvec{\Gamma}(t)=\sum_i^{\mathrm{occ}}\phi_i^*(t)\phi_i(t)$, and the associated density matrix elements in the AO basis are given by
\begin{equation}
    P'_{\mu\nu}(t)=\sum_i^{\mathrm{occ}}c_{\mu i}^*(t)c_{\nu i}(t).
\end{equation}
In an orthonormal basis, the TDHF equation is given by
\begin{equation}
    i\diff{\bvec{P}(t_i)}{t}=[\bvec{F}(t_i),\bvec{P}(t_i)].
\end{equation}
A laser field can be well approximated by a linearly polarized and spatially homogeneous (notice the lack of $\bvec{r}$ dependence, as the field has the same intensity everywhere in space) external field:
\begin{equation}
    \bvec{e}(\bvec{r},t)\approx\bvec{E}(t)\sin(\omega t+\phi).
\end{equation}
The AO basis Fock matrix element can then be separated into the field-free and dipole moment contributions:
\begin{equation}
    F'_{\mu\nu}(t)=F'_{0,\mu\nu}(t)+\bra\chi_{\mu}|\bvec{r}|\chi_{\nu}\ket e(t)\equiv F'_{0,\mu\nu}(t)+d'_{\mu\nu} e(t).
\end{equation}
The field-free contribution is time dependent \textit{via} the density matrix $\bvec{P}'(t)$.

The TDHF equation can be directly integrated with integrators such as Verlet or Runge--Kutta, but they require small time steps to maintain idempotency contraint on the density matrix $\bvec{PP}=\bvec{P}$. Symplectic integrators also require small step sizes. The `relax and drive' method is able to stably integrate the TDHF equation with larger time steps. The relax step propagates the density matrix forward using the unitary time propagator:
\begin{equation}
    \bvec{P}(t_i+\Delta t)=\bvec{UP}(t_i)\bvec{U}\adj=\exp(i\Delta t\bvec{F}(t_i))\bvec{P}(t_i)\exp(-i\Delta t\bvec{F}(t_i)),
\end{equation}
which can be tractable if the Fock matrix is expressed in its eigenspace
\begin{equation}
    \bvec{U}(t_i)=\exp(i\Delta t\bvec{F}(t_i))=\bvec{C}(t_i)\exp(i\Delta t\bvec{\epsilon}(t_i))\bvec{C}\adj(t_i).
\end{equation}
As this is a unitary transformation, the idempotency constraint is preserved for any time step, but the Fock matrix actually changes during the time step due to interaction with the external field. The `drive step' gives a correction to the density matrix $\Delta\bvec{P}$:
\begin{equation}
    \Delta\bvec{P}=\bvec{P\Delta}'\bvec{U}\adj,
\end{equation}
where
\begin{equation}
    \bvec{\Delta}'=\int[\bvec{F}(t')-\bvec{F}(t_i),\bvec{UP}(t_i)\bvec{U}\adj ]\dl t'.
\end{equation}

The modified mid-point unitary transformation (MMUT) algorithm approximates the drive step by replacing the relax step as 
\begin{equation}
{P}(t_{i+1})={U_{\mathrm{MMUT}}(t_i)P}(t_{i-1}){U_{\mathrm{MMUT}}(t_i)}\adj,\ \ {U_{\mathrm{MMUT}}(t_i)}=\exp(2i\Delta t{F}(t_i))
\end{equation}
This is a leapfrog integrator, and requires a bootstrapping step, plus frequent seed-and-restart steps to avoid energy drift. This can be achieved by the explicit second-order (trapezoidal) Magnus (EM2) integrator:
\begin{equation}
    P(t_{i+1})=U_{\mathrm{EM2}}(t_i)P(t_i)U_{\mathrm{EM2}}\adj(t_i),\ \ U_{\mathrm{EM2}}(t_i)=\exp(-i\Delta tF_m(t_i,t_{i+1})))
\end{equation}
where $F_m$ is an average Fock matrix given by
\begin{equation}
    F_m(t_i,t_{i+1})=\frac{1}{2}[F(P(t_i),t_i)+F(P_{\mathrm{FE}}(t_{i+1}),t_{i+1})],
\end{equation}
where the density matrix is computed by a forward Euler (FE) step:
\begin{equation}
    P_{\mathrm{FE}}(t_{i+1})=U_{\mathrm{FE}}(t_i)P(t_i)U\adj_{\mathrm{FE}}(t_i),\ \ U_{\mathrm{FE}}(t_i)=\exp(-i\Delta tF(P(t_i),t_i)).
\end{equation}

\subsection{MCTDHF}
Adapted from \cite{hochstuhlTimedependentMulticonfigurationMethods2014} unless otherwise noted.
\subsubsection{The Ansatz}
The MCTDHF method can be motivated from the TD-FCI method, whose Ansatz is given by
\begin{equation}
    |\Psi(t)\ket=\sum_{1\leq i_1<\dots<i_N\leq 2N_b} C_I(t)|\psi_{i_1}\dots\psi_{i_N}\ket.
\end{equation}
Its insertion into the TDSE and subsequent left-projection gives the matrix form of the equation of motion of the TD-FCI method:
\begin{equation}
    i\bvec{S}\dot{\bvec{C}}(t)=\bvec{H}(t)\bvec{C}(t).
\end{equation}
Many observations have been made about how sparse the FCI Hamiltonian is. Indeed, if the Hamiltonian consists only of a sum of single-particle operators, then the exact wavefunction.
can be given by a single Slater determinant, with time-dependent orbitals. Therefore, the MCTDHF Ansatz takes the idea of using time-dependent orbitals and a restricted sum of Slater determinants:
\begin{equation}
    |\Psi(t)\ket=\sum_{1\leq i_1<\dots<i_N\leq 2M} C_I(t)|\psi_{i_1}(t)\dots\psi_{i_N}(t)\ket,
\end{equation}
where $2M$ is usually much smaller than the number of the time-independent basis functions $2N_b$. In practice, $M$ is usually set to $N_e/2$ first, giving a single SD at the beginning, and gradually increased to reach convergence.

At this point, we can introduce the spin-restricted and spin-unrestricted Ansatzes. Spin-restriction here just refers to the number of spatial orbitals used. The spin-restricted Ansatz uses the same set of $M$ spatial orbitals for $N_{\alpha}$ spin-up electrons and $N_{\beta}$ spin-down ones, whereas the spin-unrestricted Ansatz uses $M_{\alpha}$ and $M_{\beta}$ different spatial orbitals for the spin-up and -down electrons.

\subsubsection{Spin-restricted MCTDHF equations}
The Lagrange formulation of the time-dependent variational principle gives us the action functional to minimize w.r.t. variational parameters:
\begin{equation}
    S=\int \dl t\left\{ \left\bra\Psi\left|\hat{H}(t)-i\diffp{}{t}\right|\Psi\right\ket-\sum_{kl}\mu_{kl}(t)(\bra\phi_k|\phi_l\ket-\delta_{kl} \right\}.
\end{equation}
The Lagrange multipliers $\mu_{kl}(t)$ are introduced to enforce orthonormality within the time-dependent orbitals. The first braket above can be written as
\begin{equation}
\left\bra\Psi\left|\hat{H}(t)-i\diffp{}{t}\right|\Psi\right\ket=\left\{ \sum_{pq}D_{pq}\left[ h_{pq}-\left(i\diffp{}{t}\right)_{pq} \right] +\frac{1}{2}\sum_{pqrs}d_{pqrs}g_{pqrs} \right\},
\end{equation}
note that the only difference with time-independent theories is the appearance of the time-derivative matrix elements. Taking the orbital derivatives gives us the \textit{orbital equations}:
\begin{equation}
\label{eq:mctdhf-orb-eq}
    0\equiv \frac{\delta}{\delta\bra\phi_p|}S=\left\{ \sum_q D_{pq}\left[ \hat{h}(t)-i\diffp{}{t} \right]|\phi_q\ket+\sum_{qrs}d_{pqrs}\hat{g}_{rs}|\phi_q\ket \right\}-\sum_m\mu_{pm}(t)|\phi_m\ket,
\end{equation}
where 
\begin{equation}
    \hat{g}_{rs}(\bvec{r})=\int\dl\bvec{r}'\phi_r^*(\bvec{r}')\frac{1}{|\bvec{r}-\bvec{r'}|}\phi_s(\bvec{r}')
\end{equation}
are the mean-field integrals (we used the symmetry of it and the TPDM to get rid of the half factor). No derivatives of the TPDM occur as in an orthogonal basis set (as is the case here), expectation values of fractional rank (say two creation and one annihilation) strings vanish. It is now easy to solve for the Lagrange multipliers:
\begin{equation}
    \mu_{pm}(t)=\sum_q D_{pq}\left\bra\phi_m\left|\hat{h}(t)-i\diffp{}{t}\right|\phi_q\right\ket+\sum_{qrs}d_{pqrs}\bra\phi_m|\hat{g}_{rs}|\phi_q\ket.
\end{equation}
Re-insertion of the multipliers into \Cref{eq:mctdhf-orb-eq} gives
\begin{equation}
    0=\hat{P}\left\{ \sum_qD_{pq}\left[ \hat{h}(t)-i\diffp{}{t} \right]|\phi_q\ket +\sum_{qrs}d_{pqrs}\hat{g}_{rs}|\phi_q\ket \right\},
\end{equation}
where the projection operator into the limited orbital space (this would be identity if doing TD-FCI) $\hat{P}$ is defined as
\begin{equation}
    \hat{P}=1-\sum_m|\phi_m\ket\bra\phi_m|.
\end{equation}
To obtain the equations of motion for the orbitals, we need to solve for the time-derivatives of the orbitals:
\begin{align}
    \hat{P}\sum_qD_{pq}i\diffp{}{t}|\phi_q\ket&=\hat{P}\left\{ \sum_qD_{pq} \hat{h}(t) |\phi_q\ket +\sum_{qrs}d_{pqrs}\hat{g}_{rs}|\phi_q\ket \right\}\\
    \hat{P}\sum_{pq}(\bvec{D}^{-1})_{np}D_{pq}i\diffp{}{t}|\phi_q\ket&=\hat{P}\left\{ \sum_{pq}(\bvec{D}^{-1})_{np}D_{pq} \hat{h}(t) |\phi_q\ket +\sum_{pqrs}(\bvec{D}^{-1})_{np}d_{pqrs}\hat{g}_{rs}|\phi_q\ket \right\}\\
    \hat{P}\sum_{q}\delta_{nq}i\diffp{}{t}|\phi_q\ket&=\hat{P}\left\{ \sum_{q}\delta_{nq} \hat{h}(t) |\phi_q\ket +\sum_{pqrs}(\bvec{D}^{-1})_{np}d_{pqrs}\hat{g}_{rs}|\phi_q\ket \right\}\\
    i\hat{P}\diffp{}{t}|\phi_n\ket&=\hat{P}\left\{ \hat{h}(t) |\phi_n\ket +\sum_{pqrs}(\bvec{D}^{-1})_{np}d_{pqrs}\hat{g}_{rs}|\phi_q\ket \right\}.
\end{align}
The resulting equation is an integro-differential equation (integrals due to the projection operator), and the solution of these tend to be cumbersome. To make it numerically more tractable, one can apply a unitary transformation (the MCTDHF equations are invariant under unitary transformations) on the orbitals such that
\begin{equation}
    \left\bra\phi_p\left|i\diffp{}{t}\right|\phi_q\right\ket\equiv\bra\phi'_p|\hat{Q}(t)|\phi'_q\ket.
\end{equation}
Using the new orbitals, and relabelling them without primes, we obtain a set of first-order differential equations \hl{how is this not a set of integro-differential equations?}:
\begin{equation}
    i\diffp{}{t}|\phi_n\ket=\hat{Q}(t)|\phi_n\ket + \hat{P}\left\{ \left[\hat{h}(t)-\hat{Q}(t)\right] |\phi_n\ket +\sum_{pqrs}(\bvec{D}^{-1})_{np}d_{pqrs}\hat{g}_{rs}|\phi_q\ket \right\}.
\end{equation}

Now, on to the coefficient equations. The big braket reads
\begin{equation}
    \left\bra\Psi\left|\hat{H}(t)-i\diffp{}{t}\right|\Psi\right\ket=\sum_{IJ}\left(C_I^*(t)C_J(t)-i\diffp{C_J}{t}\right)\left\bra I\left|\hat{H}(t)-i\diffp{}{t}\right|J\right\ket.
\end{equation}
The functional derivatives of the action functional is then
\begin{equation}
    0\equiv\frac{\delta}{\delta C_I^*}S=-i\diffp{}{t}C_I(t)+\sum_J C_J(t)\left\bra I\left|\hat{H}(t)-i\diffp{}{t}\right|J\right\ket.
\end{equation}
Carrying out the abovementioned change of basis, we get
\begin{equation}
    i\diffp{}{t}C_I(t)=\sum_J \bra I|\hat{H}(t)-\hat{Q}(t)|J\ket C_J(t),
\end{equation}
which resembles the TD-FCI equations, but is expressed in the reduced basis of the time-dependent Slater determinants.

\begin{definition}[Working spin-restricted MCTDHF equations]
In practical calculations, $\hat{Q}(t)=0$ is commonly used, which leads to the following working equations:
\begin{align}
    &i\diffp{}{t}C_I(t)=\sum_J \bra I|\hat{H}(t)|J\ket C_J(t),\\
    &i\hat{P}\diffp{}{t}|\phi_n\ket=\hat{P}\left\{ \hat{h}(t) |\phi_n\ket +\sum_{pqrs}(\bvec{D}^{-1})_{np}d_{pqrs}\hat{g}_{rs}|\phi_q\ket \right\}
\end{align}
    
\end{definition}

\subsection{OATDCC}
Orbital-adaptive

\section{Dynamical mean field theory}
\subsection{Green's function formalism}
Given a Hamiltonian $H$ and chemical potential $\mu$, the Green's function $\bvec{G}(\omega)$ at zero temperature is defined as
\begin{equation}
    G_{ij}(\omega)=\bra\Psi_0|a_i\frac{1}{\omega+\mu-(H-E_0)+i0}a_j\adj|\Psi_0\ket+\bra\Psi_0|a_j\adj\frac{1}{\omega+\mu+(H-E_0)-i0}a_i|\Psi_0\ket,
\end{equation}
where $\Psi_0$ and $E_0$ are the ground state and its energy. Knowing $\bvec{G}(\omega)$ gives us access to many properties of the system, like the one-particle density matrix $\bvec{P}$, electronic energy $E$, and the density of states $\bvec{A}(\omega)$:
\begin{align}
    &\bvec{P}=-i\int^{\infty}_{-\infty}e^{i\omega 0_+}\bvec{G}(\omega)\dl\omega,\\
    &E=-\frac{1}{2}i\int^{\infty}_{-\infty}e^{i\omega 0_+}\Tr[(\bvec{h}+\omega)\bvec{G}(\omega)]\dl\omega,\\
    &\bvec{A}(\omega)=-\frac{1}{\pi}\mathrm{Im}(\bvec{G}(\omega+i0_+)).
\end{align}

\section{DSRG}
\subsection{DSRG-MRPT3 implementation notes}
The energy expressions for DSRG-MRPT3 are \cite{liDrivenSimilarityRenormalization2017}:
\begin{align}
    &E^{(0)}(s)=E_0,\\
    &E^{(1)}(s)=0,\\
    &E^{(2)}(s)=\frac{1}{2}\bra\Psi_0|[\tilde{H}^{(1)}(s),\hat{A}^{(1)}(s)]|\Psi_0\ket ,\\
    &E^{(3)}(s)=\frac{1}{2}\bra\Psi_0|[\tilde{H}^{(1)}(s),\hat{A}^{(2)}(s)]|\Psi_0\ket +\frac{1}{2}\bra\Psi_0|[\tilde{H}^{(2)}(s),\hat{A}^{(1)}(s)]|\Psi_0\ket,
\end{align}
where 
\begin{align}
    \bar{H}^{(0)}(s)&=\hat{H}^{(0)},\\
    \bar{H}^{(1)}(s)&=[\hat{H}^{(0)},\hat{A}^{(1)}(s)]+\hat{H}^{(1)},\\
    \bar{H}^{(2)}(s)&=[\hat{H}^{(0)},\hat{A}^{(2)}(s)]+\frac{1}{2}[\tilde{H}^{(1)}(s),\hat{A}^{(1)}(s)],\\
    \bar{H}^{(3)}(s)&\approx [\hat{H}^{(0)},\hat{A}^{(3)}(s)]+\frac{1}{2}[\tilde{H}^{(1)}(s),\hat{A}^{(2)}(s)]+\frac{1}{2}[\tilde{H}^{(2)}_{1,2}(s),\hat{A}^{(1)}(s)],\\
    \tilde{H}^{(1)}(s)&=\hat{H}^{(1)}+\bar{H}^{(1)}(s),\\
    \tilde{H}^{(2)}(s)&=\bar{H}^{(2)}(s)-\frac{1}{6}[[\hat{H}^{(0)},\hat{A}^{(1)}(s) ], \hat{A}^{(1)}(s) ].
\end{align}

The one- and two-body elements of $\tilde{H}^{(1)}(s)$ are analytically known as:
\begin{align}
    [\tilde{H}^{(1)}(s)]^a_i&=f_a^{i,(1)}+\left[ f_a^{i,(1)}+\sum_{ux}\Delta_u^xt_{ax}^{iu,(1)}(s)\gamma_u^x \right]e^{-s(\Delta^i_a)^2}\label{eq:htilde1-1b}\\
    [\tilde{H}^{(1)}(s)]^{ab}_{ij}&=v^{ij,(1)}_{ab}+v^{ij,(1)}_{ab}e^{-s (\Delta_{ab}^{ij})^2 }.\label{eq:htilde1-2b}
\end{align}

The elements of $\tilde{H}^{(2)}(s)$ are not analytically known, and needs to be evaluated through:
\begin{align}
\tilde{H}^{(2)} &= \bar{H}^{(2)} - \frac{1}{6}[[[\hat{H}^{(0)},\hat{A}^{(1)}],\hat{A}^{(1)}],\hat{A}^{(1)}]\\
&=[\hat{H}^{(0)},\hat{A}^{(2)}]+\frac{1}{2}[\tilde{H}^{(1)},\hat{A}^{(1)}] - \frac{1}{6}[[\hat{H}^{(0)},\hat{A}^{(1)}],\hat{A}^{(1)}]
\end{align}

The third-order energy can therefore be split into three parts:
\begin{align}
    E^{(3)}(s)=&\frac{1}{2}\bra\Psi_0|[\tilde{H}^{(1)}(s),\hat{A}^{(2)}(s)]|\Psi_0\ket +\frac{1}{2}\bra\Psi_0|[\tilde{H}^{(2)}(s),\hat{A}^{(1)}(s)]|\Psi_0\ket\\
    =&-\frac{1}{12}\bra\Psi_0|[[[\hat{H}^{(0)},\hat{A}^{(1)}(s)],\hat{A}^{(1)}(s)],\hat{A}^{(1)}(s)]|\Psi_0\ket\label{eq:mrpt3-working}\\
    &+ \frac{1}{2}\bra\Psi_0|[\tilde{H}^{(1)}(s),\hat{A}^{(2)}(s)]|\Psi_0\ket + \frac{1}{2}\bra\Psi_0|[\bar{H}^{(2)},\hat{A}^{(1)}]|\Psi_0\ket \nonumber.
\end{align}

The amplitudes are given by
\begin{align}
t_a^{i,(n)}(s) &= \left[f_a^{i,(n)}+\sum_{ux}^{\mathbf{A}}\Delta_u^x t_{ax}^{iu,(n)}\gamma_u^x\right]\frac{1-e^{-s(\Delta_a^i)^2}}{\Delta_a^i}\\
t_{ab}^{ij,(n)}(s) &= v_{ab}^{ij,(n)}\frac{1-e^{-s(\Delta_{ab}^{ij})^2}}{\Delta_{ab}^{ij}},
\end{align}
where $n=1,2$, and $f_a^{i,(2)}$ and $v_{ab}^{ij,(2)}$ are one- and two-body non-diagonal elements of $\frac{1}{2}[\tilde{H}^{(1)}(s),\hat{A}^{(1)}(s)]$. In general, $f_a^{i,(n)}$ and $v_{ab}^{ij,(n)}$ are related to the off-diagonal elements of $\bar{H}^{(n)}$, \textit{via} the parameterization of the source operator:
\begin{equation}
r_{ab\dots}^{ij\dots,(n)}(s)=\left[ \bar{H}_{ab\dots}^{ij\dots,(n)}(s)+t_{ab\dots}^{ij\dots,(n)}(s)\Delta_{ab\dots}^{ij\dots} \right]    e^{-s(\Delta_{ab\dots}^{ij\dots})^2}
\end{equation}

Specifically for DSRG-MRPT3, evaluating $f_a^{i,(2)}$ and $v_{ab}^{ij,(2)}$ requires all elements of $\tilde{H}^{(1)}(s)$, but since\footnote{The following shows that this definition is consistent with \Cref{eq:htilde1-2b}:
\begin{align}
    \tilde{H}^{(1)}(s)&=[\hat{H}^{(0)}, \hat{A}^{(1)}]+2\hat{H}^{(1)}\\
    &=\frac{1}{4}\sum_{ijab}\left[-\Delta_{ab}^{ij}t^{ij,(1)}_{ab}(s)+2v^{ij,(1)}_{ab}\right]\{ \hat{a}^{ab}_{ij} \}+\mathrm{h.c.}\\
    &=\frac{1}{4}\sum_{ijab}\left[v^{ij,(1)}_{ab}[e^{-s(\Delta^{ij}_{ab})^2}-1]+2v^{ij,(1)}_{ab}\right]\{ \hat{a}^{ab}_{ij} \}+\mathrm{h.c.}\\
    &=\frac{1}{4}\sum_{ijab}\left[v^{ij,(1)}_{ab}[1+e^{-s(\Delta^{ij}_{ab})^2}]\right]\{ \hat{a}^{ab}_{ij} \}+\mathrm{h.c.},
\end{align}
where the result of the first commutator is taken from \cite[SI, eq. 16]{liMultireferenceDrivenSimilarity2015}. The same analysis can be done with the one-body terms.
}
\begin{equation}
    \tilde{H}^{(1)}(s)=[\hat{H}^{(0)}, \hat{A}^{(1)}]+2\hat{H}^{(1)},
\end{equation}
the first commutator is a purely non-diagonal (in the hole-particle sense), so the diagonal elements are all from $\hat{H}^{(1)}=\hat{F}^{(1)}+ \hat{V}^{(1)}$.

By observing the different quantities that enter into $E^{(3)}(s)$, we can adopt the following workflow:
\begin{enumerate}
    \item Energy part 1: $-\frac{1}{12}\bra\Psi_0|[[[\hat{H}^{(0)},\hat{A}^{(1)}(s)],\hat{A}^{(1)}(s)],\hat{A}^{(1)}(s)]|\Psi_0\ket$
    \begin{enumerate}
        \item Compute $-[[\hat{H}^{(0)},\hat{A}^{(1)}],\hat{A}^{(1)}]$, save it.
        \item Compute the contraction.
    \end{enumerate}
    \item Energy part 2: $\frac{1}{2}\bra\Psi_0|[\tilde{H}^{(1)}(s),\hat{A}^{(2)}(s)]|\Psi_0\ket$
    \begin{enumerate}
        \item Compute $\frac{1}{2}[\tilde{H}^{(1)},\hat{A}^{(1)}]=[\hat{H}^{(1)},\hat{A}^{(1)}]+\frac{1}{2}[[\hat{H}^{(0)},\hat{A}^{(1)}],\hat{A}^{(1)}]$, the latter from the previous step. Release $-[[\hat{H}^{(0)},\hat{A}^{(1)}],\hat{A}^{(1)}]$ from memory.
        \item Form $\hat{T}^{(2)}$ amplitudes.
        \item Compute the energy contraction.
    \end{enumerate}
    \item Energy part 3: $\frac{1}{2}\bra\Psi_0|[\bar{H}^{(2)},\hat{A}^{(1)}]|\Psi_0\ket$
    \begin{enumerate}
        \item Recall that 
        \begin{equation}
            \bar{H}^{(2)}=[\hat{H}^{(0)},\hat{A}^{(2)}(s)]+\frac{1}{2}[\tilde{H}^{(1)}(s),\hat{A}^{(1)}(s)],
        \end{equation}
        and that the one- and two-body elements of $[\hat{H}^{(0)},\hat{T}^{(n)}(s)]$ are
        \begin{align}
            &[\hat{H}^{(0)},\hat{T}^{(n)}(s)]_i^a=-\Delta_a^it_a^{i,(n)}+\sum_{ux}^{\bvec{A}}\Delta_u^xt_{ax}^{iu,(n)}\gamma_u^x\\
            &[\hat{H}^{(0)},\hat{T}^{(n)}(s)]_{ij}^{ab}=-\Delta_{ab}^{ij}t^{ij,(n)}_{ab}.
        \end{align}
        We are reminded that the one- and two-body elements of $\frac{1}{2}[\tilde{H}^{(1)},\hat{A}^{(1)}]$ are $f^{(2)}$ and $v^{(2)}$ respectively, so we can obtain the elements of $\bar{H}^{(2)}$ as
        \begin{align}
            &\bar{H}^{a,(2)}_i=[f_a^{i,(2)}+\sum_{ux}^{\bvec{A}}\Delta_u^xt_{ax}^{iu,(2)}\gamma_u^x]e^{-s(\Delta_a^i)^2}\\
            &\bar{H}^{ab,(2)}_{ij}=v_{ab}^{ij,(2)}e^{-s(\Delta_{ab}^{ij})^2},
        \end{align}
        which are reminiscent of the renormalized/dressed first-order integrals in \Cref{eq:htilde1-1b,eq:htilde1-2b}, but without their first terms.
    \end{enumerate}
\end{enumerate}

Since we always evaluate the elements of $[\hat{H},\hat{T}]_{1,2}$, to obtain the correct elements for $[\hat{H},\hat{A}]_{1,2}$, we need to use the following identity:
\begin{equation}
    [\hat{H},\hat{A}]=[\hat{H},\hat{T}-\hat{T}\adj]=[\hat{H},\hat{T}]+[\hat{H},\hat{T}]\adj
\end{equation}

\subsection{MR-LDSRG(2)}
The quadratic commutator approximation is given by:\cite{neuscammanQuadraticCanonicalTransformation2009}
\begin{equation}
    \bar{H}=\hat{H}+[\hat{H},\hat{A}]_{1,2}+\frac{1}{2!}[[\hat{H},\hat{A}],\hat{A}]_{1,2}+\frac{1}{3!}[[[\hat{H},\hat{A}],\hat{A}]_{1,2},\hat{A}]_{1,2}+\frac{1}{4!}[[[[\hat{H},\hat{A}],\hat{A}]_{1,2},\hat{A}],\hat{A}]_{1,2}+\dots,
\end{equation}
where it is important to note that the $[\hat{H},\hat{A}]$ in the second-order term is not truncated, and that the penultimate commutator in the fourth-order term is also not truncated, and so on.
\begin{equation}
\begin{split}
[[\hat{H},\hat{A}],\hat{A}] &= [[\hat{H},\hat{A}],\hat{T}] + [[\hat{H},\hat{A}],\hat{T}]\adj\\
&= [[\hat{H},\hat{T}]+[\hat{H},\hat{T}]\adj,\hat{T}] + [[\hat{H},\hat{T}]+[\hat{H},\hat{T}]\adj,\hat{T}]\adj\\
&= [[\hat{H},\hat{T}],\hat{T}] - [[\hat{H},\hat{T}],\hat{T}\adj] + \mathrm{h.c.}
\end{split}
\end{equation}

\subsection{Spin-free DSRG}
The spin-free DSRG formalism uses an equally weighted ensemble of the entire multiplet \cite{liSpinfreeFormulationMultireference2021}:
\begin{equation}
    \hat{\rho}_S=\frac{1}{2S+1}\sum_{M_S=-S}^S|\Psi(S,M_S)\ket\bra\Psi(S,M_S)|.
\end{equation}
This is a singlet operator and hence invariant under spin rotations.

\section{Review of MRPTs}
\subsubsection{NEVPT2}

\section{The flow equation approach}

\subsection{The flow equation approach}
A direct perturbative expansion of the Heisenberg equation of motion is not controlled in the limit of long times. However, if a change of basis could be performed that transforms $H$ into a non-interacting form, then long time evolution can be performed.

The main concept of the flow equation approach is to implement the transformation as a sequence of infinitesimal unitary transformations that allows the separation of energy scales \cite{ecksteinNewTheoreticalApproaches2009}. Analogous to the Heisenberg equation, we introduce the flow equation
\begin{equation}
\label{eq:floweq}
\diff{H}{B}=[\eta(B), H(B)],
\end{equation}
with the initial condition $H(0)=H$, and $\eta$ anti-Hermitian (note the lack of $i$ compared to the Heisenberg equation). The analogy to the Heisenberg equation is that all of the transformed Hamiltonians $H(B)$ will be unitarily equivalent to the original Hamiltonian $H(0)$. The choice of the canonical generator is the crux of the method:
\begin{equation}
\eta(B)=[H_0(B),H_{\mathrm{int}}(B)],
\end{equation}
where $H_0$ and $H_{\mathrm{int}}$ are the diagonal and non-diagonal part of the Hamiltonian. This choice ensures that $H(B)$ becomes more diagonal with an effective band width of $\Lambda\propto B^{-1/2}$.

\subsection{A motivating example}
(From \cite[Ch. 2.2.2]{kehreinFlowEquationApproach2006}) We do not want to eliminate part of the Hilbert space, so we need to consider infinitesimal unitary transformations that eliminate interaction matrix elements that couple states with a certain energy difference. We consider a model Hamiltonian
\begin{equation}
H=H_0+H_{\mathrm{int}}=\sum_n\epsilon_nc_n^{\dagger}c_n+H_{\mathrm{int}}.
\end{equation}
Let's say there's a specific Hermitian element (\textit{i.e.}, just one element, no summation here) $M$ we want to eliminate from $H_{\mathrm{int}}$, given by
\begin{equation}
M=g(c^{\dagger}_{m_1'}c^{\dagger}_{m_2'}c_{m_1}c_{m_2}+\mathrm{h.c.}).
\end{equation}

We can construct a new Hamiltonian $\tilde{H}=UHU^{\dagger}$ via a unitary transformation $U=e^{\eta}$ with $\eta$ anti-Hermitian by definition. If we choose, seemingly arbitrarily for now, that
\begin{equation}
\eta=\frac{g}{\epsilon_{m_1'}+\epsilon_{m_2'}-\epsilon_{m_1}-\epsilon_{m_2}}(c^{\dagger}_{m_1'}c^{\dagger}_{m_2'}c_{m_1}c_{m_2}-\mathrm{h.c.}),
\end{equation}
we can write the transformed Hamiltonian as
\begin{align}
\tilde{H}&=e^{\eta}(H_0+H_{\mathrm{int}})e^{-\eta}\\
&=H_0+H_{\mathrm{int}}+[\eta,H_0+H_{\mathrm{int}}]+\frac{1}{2}[\eta,[\eta,H_0+H_{\mathrm{int}}]]+\dots,
\end{align}
which is the standard BCH expansion. However, the choice of $\eta$ enables us to evaluate the commutator $[\eta,H_0]$ using elementary commutator relations to obtain $[\eta,H_0]=M$. We then conclude that
\begin{equation}
\tilde{H}=H_0+(H_{\mathrm{int}}-M)+\mathcal{O}(g^2).
\end{equation}
This is to say, we have eliminated the interaction term $M$ to first order in the coupling constant from the total interaction. We can then iteratively eliminate smaller and smaller energy differences $\Delta E=\epsilon_{m_1'}+\epsilon_{m_2'}-\epsilon_{m_1}-\epsilon_{m_2}$.

\subsection{Ansatz for the unitary transformation}
However, we don't yet have a general way to eliminate elements for general interaction terms. This is where the flow equation approach in \Cref{eq:floweq} comes in. First of all, we need to verify the claim that it generates a family of Hamiltonians $H(B)$ that are unitarily equivalent to the original Hamiltonian $H(0)$. Let us use the following Ansatz that we claim solves the flow equation \Cref{eq:floweq}:
\begin{equation}
H(B)=U(B)H(0)U^{\dagger}(B),
\end{equation}
where $U(B)$ is defined as
\begin{align}
U(B)&=T_B\exp\left(\int_0^B\eta(B')\dl B'\right)\label{eq:ubdef}\\
&=1+\sum_{n=1}^{\infty}\frac{1}{n!}\int_0^BT_B\{ \eta(B_1)\dots\eta(B_n) \}\dl B_1\dots\dl B_n,
\end{align}
which is analogous to the \href{https://en.wikipedia.org/wiki/Dyson_series}{Dyson series} expansion of the time evolution operator in the interaction picture. Here $T_B\{\dots\}$ denotes the $B$-ordering operator, analogous to the time-ordering operator in the Dyson series. It is defined as
\begin{equation}
T_B\{ \eta(B_1)\dots\eta(B_n) \}=\eta(B_{\pi(1)})\dots\eta(B_{\pi(n)}),
\end{equation}
where the permutation $\pi$ is the one that sorts the $B_{\pi(n)}$'s from largest to smallest: $B_{\pi(1)}\geq\dots\geq B_{\pi(n)}$.

One can check that with this definition, $U(B)$ is a unitary operator, since $U(B)U^{\dagger}(B)=1$ (remember $\eta$ is anti-Hermitian). We can also perform the derivative in the LHS of \Cref{eq:floweq}:
\begin{equation}
\diff{}{B}(U(B)H(0)U^{\dagger}(B))=\diff{U(B)}{B}U^{\dagger}(B)H(B)+H(B)U(B)\diff{U^{\dagger}(B)}{B},
\end{equation}
where we have inserted identity where appropriate. From \Cref{eq:ubdef} we can write
\begin{equation}
\diff{U(B)}{B}U^{\dagger}(B)=\eta(B)U(B)U^{\dagger}(B)=\eta(B),
\end{equation}
so we finally have
\begin{equation}
\diff{}{B}(U(B)H(0)U^{\dagger}(B))=\eta(B)H(B)+H(B)\eta^{\dagger}(B)=[\eta(B),H(B)].
\end{equation}
This shows that our Ansatz of $H(B)=U(B)H(0)U^{\dagger}(B)$ obeys the flow equation \Cref{eq:floweq}. The initial condition that $U(0)H(0)U^{\dagger}(0)=H(0)$ is also fulfilled since $U(0)=1$. We have completed the proof that the Ansatz together with the definition of $U(B)$ is indeed a solution to the flow equation.

\subsection{Choice of generator}
For a Hamiltonian that can be split up into a diagonal and an interaction part
\begin{equation}
H(B)=H_0(B)+H_{\mathrm{int}}(B),
\end{equation}
Wegner's canonical generator is given by
\begin{equation}
\eta(B)=[H_0(B),H_{\mathrm{int}}(B)].
\end{equation}
$\eta(B)$ is anti-Hermitian as required, as it is a commutator between two Hermitian operators. $\eta(B)$ has the units of $(\mathrm{energy})^{2}$, and together with the flow equation \Cref{eq:floweq}, we can conclude that $B$ has the units of $(\mathrm{energy})^{-2}$. We now find out the consequences of this choice of generator for a typical model Hamiltonian for an interacting fermion:
\begin{equation}
H(B)=\underbrace{ \sum_n\epsilon_n(B)c_n^{\dagger}c_n }_{H_0(B)}+\underbrace{ \sum_{m_1',m_2',m_1,m_2} g_{m_1',m_2',m_1,m_2}(B)c_{m_1'}^{\dagger}c_{m_2'}^{\dagger}c_{m_1}c_{m_2} }_{H_{\mathrm{int}}(B)}.
\end{equation}

The canonical generator yields, via commutator relations
\begin{equation}
\eta(B)=\sum_{m_1',m_2',m_1,m_2} g_{m_1',m_2',m_1,m_2}(B) \Delta E^{m_1',m_2'}_{m_1,m_2}(B) c_{m_1'}^{\dagger}c_{m_2'}^{\dagger}c_{m_1}c_{m_2},
\end{equation}
where $\Delta E^{ij}_{ab}=\epsilon_i+\epsilon_j-\epsilon_a-\epsilon_b$. The flow equation then reads
\begin{align}
\diff{H(B)}{B}&=[\eta(B),H_0(B)]+[\eta(B),H_{\mathrm{int}}(B)]\\
&=-\sum_{m_1',m_2',m_1,m_2} g_{m_1',m_2',m_1,m_2}(B) [\Delta E^{m_1',m_2'}_{m_1,m_2}(B)]^2 c_{m_1'}^{\dagger}c_{m_2'}^{\dagger}c_{m_1}c_{m_2}+\mathcal{O}(g^2),
\end{align}
where only the first commutator contributed to the first-order terms. Matching the rank of the operators, we obtain the differential equation
\begin{equation}
\diff{g_{m_1',m_2',m_1,m_2}}{B}=-[\Delta E^{m_1',m_2'}_{m_1,m_2}(B)]^2 g_{m_1',m_2',m_1,m_2}(B)+\mathcal{O}(g^2).
\end{equation}
Assuming $\Delta E(B)$ is approximately constant \hl{justify?}, we obtain the \textit{approximate} linearlized solution:
\begin{equation}
g_{m_1',m_2',m_1,m_2}(B)=g_{m_1',m_2',m_1,m_2}(0)e^{-B[\Delta E^{m_1',m_2'}_{m_1,m_2}(B)]^2}.
\end{equation}
This shows that the canonical generator gradually separates the energy scales from large to small for increasing $B$.

At this point, we will prove that the canonical generator will make the Hamiltonian increasingly energy diagonal. We first write that 
\begin{align}
&\mathrm{Tr}(H_0(B)H_{\mathrm{int}}(B))=0\label{eq:trace1}\\
&\mathrm{Tr}\left(\diff{H_0(B)}{B}H_{\mathrm{int}}(B)\right)=0,\label{eq:trace2}
\end{align}
which is due to the fact that the product must change the quantum state it acts on by at least $1$. This then implies 
\begin{equation}
\diff{}{B}\mathrm{Tr}(H_{\mathrm{int}}^2(B))\leq 0.
\end{equation}
\begin{proof}
    \begin{align}
    \diff{}{B}\mathrm{Tr}(H_{\mathrm{int}}^2(B))&=2\mathrm{Tr}\left(H_{\mathrm{int}}(B)\diff{H_{\mathrm{int}}(B)}{B}\right)\\
    &=2\mathrm{Tr}\left(H_{\mathrm{int}}(B)\diff{H(B)}{B}\right)\\
    &=2\mathrm{Tr}\left(H_{\mathrm{int}}(B)[\eta(B),H(B)]\right)\\
    &=2\mathrm{Tr}(\eta(B)(H(B)H_{\mathrm{int}}(B)-H_{\mathrm{int}}(B)H(B))),
    \end{align}
    where the second equality is due to \Cref{eq:trace1}, the third due to the flow equation, and the fourth due to cyclic permutation under trace.

    The canonical generator can also be written as
    \begin{equation}
    \eta(B)=[H(B),H_{\mathrm{int}}(B)],
    \end{equation}
    as the commutator of an operator with itself vanishes. So we finally obtain
    \begin{align}
    \diff{}{B}\mathrm{Tr}(H^2_{\mathrm{int}}(B))&=2\mathrm{Tr}\left(\eta^2(B)\right)\\
    &=-2\mathrm{Tr}\left(\eta^{\dagger}(B)\eta(B)\right)\\
    &\leq 0,
    \end{align}
    since $\eta^{\dagger}(B)\eta(B)$ is a positive semi-definite operator ($\bra\psi|\eta^{\dagger}(B)\eta(B)|\psi\ket\geq0$), this proves that the flow makes the Hamiltonian more diagonal, provided that $\eta(B)\neq0$, $\forall B\geq0$. This additional constraint is due to the fact that if $\eta(B)=0$, then $H_0(B)$ and $H_{\mathrm{int}}(B)$ commute, and we can simply diagonalize both in a simultaneous eigenbasis.
\end{proof}


\section{Hartree--Fock theory}
\subsection{CSFs}
In restricted Hartree--Fock theory, the electronic state is represented by a single CSF
\begin{equation}
|\mathrm{CSF}\ket=\sum_iC_i|i\ket,
\end{equation}
where $|i\ket$ are Slater determinants and the coefficients $C_i$ are fixed by spin symmetry. We use the orbital partitioning of inactive/core: $i,j,k,l$; active: $v,w,x,y,z$; virtual/secondary: $a,b,c,d,e$; and general: $m,n,o,p,\dots$.

There are three important examples of a RHF CSF:
\begin{enumerate}
    \item Closed-shell: the CSF contains a single Slater determinant:
    \begin{equation}
        |\mathrm{CSF}\ket=\underbrace{\left( \prod_ia_{i\alpha}^{\dagger}a_{i\beta}^{\dagger} \right)}_{\equiv \hat{A}_c^{\dagger}}|\mathrm{vac}\ket,
    \end{equation}
    where $\hat{A}_c^{\dagger}$ is the core creation operator.
    \item The triplet CSF has single occupancy in two orbitals has three components:
    \begin{align}
    |\mathrm{CSF}\ket^{1,1}&=\hat{Q}_{vw}^{1,1}\hat{A}_c^{\dagger}|\mathrm{vac}\ket=a_{v\alpha}^{\dagger}a_{w\alpha}^{\dagger}\left( \prod_ia_{i\alpha}^{\dagger}a_{i\beta}^{\dagger} \right)|\mathrm{vac}\ket\\
    |\mathrm{CSF}\ket^{1,0}&=\hat{Q}_{vw}^{1,0}\hat{A}_c^{\dagger}|\mathrm{vac}\ket=\frac{1}{\sqrt{2}}(a_{v\alpha}^{\dagger}a_{w\beta}^{\dagger}+a_{v\beta}^{\dagger}a_{w\alpha}^{\dagger})\left( \prod_ia_{i\alpha}^{\dagger}a_{i\beta}^{\dagger} \right)|\mathrm{vac}\ket\\
    |\mathrm{CSF}\ket^{1,-1}&=\hat{Q}_{vw}^{1,-1}\hat{A}_c^{\dagger}|\mathrm{vac}\ket=a_{v\beta}^{\dagger}a_{w\beta}^{\dagger}\left( \prod_ia_{i\alpha}^{\dagger}a_{i\beta}^{\dagger} \right)|\mathrm{vac}\ket,
    \end{align}
    where the $\hat{Q}$'s are triplet two-body creation operators (see \cite[eq. 2.3.17-2.3.19]{helgakerMolecularElectronicStructure2000}.
    \item The singlet CSF can be formed by the similarly defined singlet two-body creation operator:
    \begin{equation}
    |\mathrm{CSF}\ket^{0,0}=\hat{Q}_{vw}^{0,0}\hat{A}_c^{\dagger}|\mathrm{vac}\ket=\frac{1}{\sqrt{2}}(a_{v\alpha}^{\dagger}a_{w\beta}^{\dagger}-a_{v\beta}^{\dagger}a_{w\alpha}^{\dagger})\left( \prod_ia_{i\alpha}^{\dagger}a_{i\beta}^{\dagger} \right)|\mathrm{vac}\ket.
    \end{equation}
\end{enumerate}

\subsection{Orbital rotations}
We can rotation a given CSF into another via a unitary rotation:
\begin{equation}
|\mathrm{CSF}(\bvec{\kappa})\ket=e^{-\hat{\kappa}}|\mathrm{CSF}\ket.
\end{equation}
If we restrict valid rotations to those that preserve the spin of the CSF, then we have
\begin{equation}
    \hat{\kappa}=\sum_{p>q}\kappa_{pq}(E_{pq}-E_{qp})=\sum_{p>q}\kappa_{pq}E^-_{pq}.
\end{equation}
If we also wish to conserve the spatial symmetry of the CSF, we must retain in the orbital-rotation operator only those excitation operators that transform as the totally symmetric irreducible representation of the molecular point group. For Abelian point groups, this is accomplished by summing over only those pairs $pq$ where $p$ and $q$ transform as the same irreducible representation. 

We further define redundant rotation parameters as those $\kappa_{pq}$ that are not necessary for a first-order transformation of the wavefunction ($-\hat{\kappa}|\mathrm{CSF}\ket$), which is satisfied by the condition
\begin{equation}
E_{pq}^-|\mathrm{CSF}\ket\equiv 0.
\end{equation}
We now identify the redundant parameters in the three CSF examples given above:
\begin{enumerate}
    \item Closed-shell. The core-core and virtual-virual rotations are clearly redundant, whereas the core-virtual rotations aren't:
    \begin{equation}
    E_{aj}^-\left(\prod_ia_{i\alpha}^{\dagger}a_{i\beta}^{\dagger} \right)|\mathrm{vac}\ket\neq 0.
    \end{equation}
    \item The two-electron open-shell CSFs can be written in a general form as
    \begin{equation}
    |\mathrm{CSF}\ket^{S,M}=\hat{Q}_{vw}^{S,M}\hat{A}_c^{\dagger}|\mathrm{vac}\ket.
    \end{equation}
    Again the core-core and virtual-virtual rotations are redundant. Here all the inter-space rotations are nonredundant. The only class left is the active-active, which is nonredundant in the singlet case, and redundant in the triplet case. Refer to \cite[Ch. 10.1.2]{helgakerMolecularElectronicStructure2000} for more details.
\end{enumerate}

\subsection{Expansion of the energy}
We expand the energy in a Taylor series about $\bvec{\kappa}=\bvec{0}$:
\begin{equation}
E(\bvec{\kappa})=\bra\mathrm{CSF}(\bvec{\kappa})|\hat{H}|\mathrm{CSF}(\bvec{\kappa})\ket=E^{(0)}+\bvec{\kappa}^{\mathrm{T}}\bvec{E}^{(1)}+\frac{1}{2}\bvec{\kappa}^{\mathrm{T}}\bvec{E}^{(2)}\bvec{\kappa}+\dots.
\end{equation}

We can expand $E(\bvec{\kappa})$ in a BCH series:
\begin{equation}
E(\bvec{\kappa})=\bra\mathrm{CSF} | e^{\hat{\kappa}}\hat{H}e^{-\hat{\kappa}} |\mathrm{CSF} \ket =\bra\mathrm{CSF}|\hat{H}|\mathrm{CSF}\ket+\bra\mathrm{CSF}|[\hat{\kappa},\hat{H}]|\mathrm{CSF}\ket+\frac{1}{2}\bra\mathrm{CSF}|[\hat{\kappa},[\hat{\kappa},\hat{H}]]|\mathrm{CSF}\ket+\dots.
\end{equation}
Identifying terms occuring at the same orders in the Taylor and BCH expansions, we can obtain expressions for the electronic energy, gradient and Hessian at $\bvec{\kappa}=\bvec{0}$:
\begin{align}
E^{(0)}&=E(\bvec{0})=\bra\mathrm{CSF}|\hat{H}|\mathrm{CSF}\ket\\
E^{(1)}_{pq}&=\bra\mathrm{CSF}|[E_{pq}^-,\hat{H}]|\mathrm{CSF}\ket\\
E^{(2)}_{pq,rs}&=\frac{1}{2}(1+P_{pq,rs})\bra\mathrm{CSF}|[E_{pq}^-,[E_{rs}^-,\hat{H}]]|\mathrm{CSF}\ket,
\end{align}
where $+P_{pq,rs}$ permutes $pq$ with $rs$, making the expression symmetric.

For real wavefunctions, we can simplify the gradient and Hessian to
\begin{align}
E^{(1)}_{pq}&=2\bra\mathrm{CSF}|[E_{pq},\hat{H}]|\mathrm{CSF}\ket\\
E^{(2)}_{pq,rs}&=(1+P_{pq,rs})\bra\mathrm{CSF}|[E_{pq},[E_{rs}^-,\hat{H}]]|\mathrm{CSF}\ket\\
&=(1+P_{pq,rs})\bra\mathrm{CSF}|[E_{pq}^-,[E_{rs},\hat{H}]]|\mathrm{CSF}\ket.
\end{align}

\subsection{The Hartree--Fock state}
The Hartree--Fock state is obtained when the energy is stationary w.r.t. unitary variations in the MOs:
\begin{equation}
    \delta E(\bvec{\kappa})=\delta\bra \mathrm{CSF}(\bvec{\kappa})|\hat{H}|\mathrm{CSF}(\bvec{\kappa})\ket=0,
\end{equation}
and the state is correspondingly
\begin{equation}
|\mathrm{HF}\ket=e^{-\hat{\kappa}^{\mathrm{HF}}}|\mathrm{CSF}\ket.
\end{equation}
The original orbitals are almost always transformed to the Hartree--Fock basis by
\begin{equation}
^{\mathrm{HF}}a_P^{\dagger}=e^{-\hat{\kappa}^{\mathrm{HF}}}a_p^{\dagger}e^{\hat{\kappa}^{\mathrm{HF}}}.
\end{equation}

At a stationary point, we have the orbital gradients vanish \textit{only in the Hartree--Fock orbital basis}, where $\bvec{\kappa}^{\mathrm{HF}}=\bvec{0}$:
\begin{equation}
    E_{pq}^{(1)}=2\bra\mathrm{HF}|[E_{pq},\hat{H}]|\mathrm{HF}\ket=0.
\end{equation}
To characterise the stationary point as either local minima or saddle points, we need to consider the electronic Hessian: it is positive definite at local minima (all positive eigenvalues). First, we use the Jacobi identity, $[A,[B,C]]+[C,[A,B]]+[B,[C,A]]=0$, to write
\begin{equation}
\bra\mathrm{HF}|[E_{rs}^-,[E_{pq}^-,\hat{H}]]|\mathrm{HF}\ket=\bra\mathrm{HF}|[E_{pq}^-,[E_{rs}^-,\hat{H}]]|\mathrm{HF}\ket+\bra\mathrm{HF}|[[E_{rs}^-,E_{pq}^-],\hat{H}]|\mathrm{HF}\ket.
\end{equation}
The last term vanishes due to the stationary conditions and \cite[eq. 10.2.6]{helgakerMolecularElectronicStructure2000}. We can then write the Hessian as
\begin{equation}
E_{pq,rs}^{(2)}=2\bra\mathrm{HF}|[E_{pq},[E_{rs}^-,\hat{H}]]|\mathrm{HF}\ket=2\bra\mathrm{HF}|[E_{pq}^-,[E_{rs},\hat{H}]]|\mathrm{HF}\ket
\end{equation}

\subsection{The Fock matrix}
The Fock matrix for a closed-shelled state can be identified as \cite[Ch. 10.3.2]{helgakerMolecularElectronicStructure2000}
\begin{equation}
    \hat{f}=\frac{1}{2}\sum_{pq}\sum_{\sigma}\bra\mathrm{cs}|[a_{q\sigma}^{\dagger},[a_{p\sigma},\hat{H}]]_+|\mathrm{cs}\ket E_{pq}.
\end{equation}
The Fock matrix is nondiagonal when constructed from an arbitrary set of orbitals, but in the canonical representation, the orbitals satisfy the \textit{canonical conditions}:
\begin{equation}
f_{pq}=\epsilon_p\delta_{pq}.
\end{equation}
We can derive the expressions of the Fock matrix in terms of MO integrals by substituing in the full electronic Hamiltonian. The one-electron part $\sum_{pq}h_{pq}E_{pq}$ gives
\begin{align}
\hat{f}_1&=\frac{1}{2}\sum_{pq}\sum_{rs}h_{rs}\sum_{\sigma}\bra\mathrm{cs}|[\hat{a}^{\dagger}_{q\sigma},[a_{p\sigma},E_{rs}]]_+|\mathrm{cs}\ket E_{pq}\\
&=\frac{1}{2}\sum_{pqrs}h_{rs}\sum_{\sigma}\bra\mathrm{cs}|[\hat{a}^{\dagger}_{q\sigma},\delta_{pr}a_{s\sigma}]_+|\mathrm{cs}\ket E_{pq}\\
&=\frac{1}{2}\sum_{pqrs}h_{rs}\sum_{\sigma}\delta_{pr}\delta_{qs}E_{pq}\\
&=\sum_{pq}h_{pq}E_{pq}.
\end{align}
We observe that the Fock operator reproduces the true Hamiltonian in the absence of two-electron interactions. The two electron part is 
\begin{equation}
\hat{g}=\frac{1}{2}\sum_{pqrs}\sum_{\sigma\tau}g_{pqrs}a_{p\sigma}^{\dagger}a_{r\tau}^{\dagger}a_{s\tau}a_{q\sigma}.
\end{equation}
The inner commutator can be evaluated using a commutator relation \cite[eq. 10.3.20]{helgakerMolecularElectronicStructure2000}:
\begin{align}
[a_{m\sigma},\hat{g}]&=\frac{1}{2}\sum_{pqrs}\sum_{\tau\nu}g_{pqrs}[a_{m\sigma},a^{\dagger}_{p\nu}a^{\dagger}_{r\tau}a_{s\tau}a_{q\nu}]\\
&=\frac{1}{2}\sum_{pqrs}\sum_{\tau\nu}g_{pqrs} (\delta_{mp}\delta_{\sigma\nu}a_{r\tau}^{\dagger}-\delta_{mr}\delta_{\sigma\tau}a^{\dagger}_{p\nu}) a_{s\tau}a_{q\nu}\\
&=\sum_{prs}\sum_{\tau}g_{mqrs}a_{r\tau}^{\dagger}a_{s\tau}a_{q\sigma}.
\end{align}
The outer anticommutator and be likewise evaluated
\begin{align}
\sum_\sigma[a_{n\sigma}^{\dagger},[a_{m\sigma},\hat{g}]]_+&=\sum_{qrs} g_{mqrs}\sum_{\sigma\tau}[a_{n\sigma}^{\dagger},a_{r\tau}^{\dagger}a_{s\tau}a_{q\sigma}]_+\\
&=\sum_{qrs}g_{mqrs}\sum_{\sigma\tau}(\delta_{nq}a_{r\tau}^{\dagger}a_{s\tau}-\delta_{ns}\delta_{\sigma\tau}a_{r\tau}^{\dagger}a_{q\sigma})\\
&=\sum_{rs}(2g_{mnrs}-g_{msrn})E_{rs}.
\end{align}
So we now have
\begin{equation}
\frac{1}{2}\sum_{\sigma}[a_{q\sigma}^{\dagger},[a_{p\sigma},\hat{H}]]_+=h_{pq}+\frac{1}{2}\sum_{rs}(2g_{pqrs}-g_{psrq})E_{rs}.
\end{equation}
The $1$-RDM $D_{pq}=\bra\mathrm{cs}|E_{pq}|\mathrm{cs}\ket$ of a closed-shelled state is simply $2\delta_{pq}$ for occupied $p$ or $0$ for virtual $p$. We then have the final expression of the familiar Fock matrix element in the MO basis:
\begin{equation}
f_{pq}=h_{pq}+\sum_i(2g_{pqii}-g_{piiq}).
\end{equation}

\subsection{The RHF total energy and orbital energies}
The Fock operator may be written in the form $\hat{f}=\hat{h}+\hat{V}$, where the $\hat{V}$ is the Fock potential, given by
\begin{equation}
\hat{V}=\sum_{pq}V_{pq}E_{pq}=\sum_{pq}\sum_{i}(2g_{pqii}-g_{piiq})E_{pq},
\end{equation}
which replaces the genuine two-electron interaction $\hat{g}$ in the full electronic Hamiltonian with an effective potential.

In the canonical form, we have the orbital energies as the diagonal elements of the Fock matrix, which are given by
\begin{equation}
\epsilon_p=f_{pp}=h_{pp}+\sum_i(2g_{ppii}-g_{piip}).
\end{equation}
The interpretation of the contributions become clear in the full integral form:
\begin{align}
\epsilon_p=&\int\phi_p(\bvec{r}_1)\left( -\frac{1}{2}\nabla^2-\sum_K\frac{Z_K}{r_{1K}} \right)\phi_p(\bvec{r}_1)\dl \bvec{r}_1\\
&+2\sum_i\iint\frac{\phi_p(\bvec{r}_1)\phi_p(\bvec{r}_1)\phi_i(\bvec{r}_2)\phi_i(\bvec{r}_2)}{r_{12}}\dl\bvec{r}_1\dl\bvec{r}_2-\sum_i\iint\frac{\phi_p(\bvec{r}_1)\phi_i(\bvec{r}_1)\phi_i(\bvec{r}_2)\phi_p(\bvec{r}_2)}{r_{12}}\dl\bvec{r}_1\dl\bvec{r}_2,
\end{align}
which shows that the first term is the kinetic energy of the electron and its attractive interactions with the stationary nuclei; and the second term is a classical Coulomb potential for electrostatic repulsion between the charge distribution of this electron and the rest of the electrons; and the last term is the nonclassical exchange potential accounting for Fermi correlation of same-spin electrons.

The total RHF energy can be written in terms of one- and two-electron density matrix elements:
\begin{equation}
E^{(0)}=\sum_{pq}D_{pq}h_{pq}+\frac{1}{2}\sum_{pqrs}d_{pqrs}g_{pqrs}+E_{\mathrm{nuc}}.
\end{equation}
The only nonzero elements of the density matrices are those with all-inactive indices:
\begin{align}
D_{ij}&=2\delta_{ij}\\
d_{ijkl}&=\bra\mathrm{HF}|e_{ijkl}|\mathrm{HF}\ket=D_{ij}D_{kl}-\frac{1}{2}D_{il}D_{kj}=4\delta_{ij}\delta_{kl}-2\delta_{il}\delta_{kj}.
\end{align}
We can then obtain the total RHF energy as
\begin{equation}
E^{(0)}=2\sum_ih_{ii}+\sum_{ij}(2g_{iijj}-g_{ijji})+E_{\mathrm{nuc}}.
\end{equation}

\subsection{Second-order optimization}
In the $n$-th iteration of Newton's method, a second-order Taylor expansion of the energy function around the current expansion point is formed:
\begin{equation}
Q_n(\bvec{z})=E_n^{(0)}+\Delta\bvec{z}\tp\bvec{E}_n^{(1)}+\frac{1}{2}\Delta\bvec{z}\tp\bvec{E}_n^{(2)}\Delta\bvec{z}.
\end{equation}
The Newton step $\Delta\bvec{z}_n$ is generated by minimizing $Q_n(\bvec{z})$, which is achieved by setting its gradient to zero, giving
\begin{equation}
\bvec{E}_n^{(2)}\Delta\bvec{z}_n=-\bvec{E}_n^{(1)}.
\end{equation}
To ensure that we work in a local region where convergence is guaranteed, we use the trust-region method, where a Newton step is only taken if
\begin{equation}
||\Delta\bvec{z}_n||\leq h_n.
\end{equation}
This can be summarized by minimizing instead a Lagrangian
\begin{equation}
L_n(\bvec{z})=E_n^{(0)}+\Delta\bvec{z}\tp\bvec{E}_n^{(1)}+\frac{1}{2}\Delta\bvec{z}\tp\bvec{E}_n^{(2)}\Delta\bvec{z}-\frac{\mu}{2}(\Delta\bvec{z}\tp\Delta\bvec{z}-h_n^2),
\end{equation}
giving the modified Newton equations
\begin{equation}
(\bvec{E}_n^{(2)}-\mu\bvec{1})\Delta\bvec{z}_n=-\bvec{E}_n^{(1)},\ \ ||\Delta\bvec{z}_n(\mu)||= h_n,
\end{equation}
where the multiplier $\mu$ is also known as the level-shift parameter. We note that the linear transformations of the form $\bvec{\sigma}=\bvec{E}^{(2)}\bvec{z}$ can require pre-evaluation of the entire Hessian matrix, which is unfeasible. We will now outline how these linear transformations can be achieved without explicitly calculating the Hessian.

The electronic gradient for a real CSF is given by 
\begin{align}
E_{mn}^{(1)}&=2\bra\mathrm{CSF}|[E_{mn},\hat{H}]|\mathrm{CSF}\ket\\
&=2\sum_{\sigma}[\bra\mathrm{CSF}|a_{m\sigma}^{\dagger}[a_{n\sigma},\hat{H}]|\mathrm{CSF}\ket+\bra\mathrm{CSF}|[a_{m\sigma}^{\dagger},\hat{H}]a_{n\sigma}|\mathrm{CSF}\ket]\\
&=2\sum_{\sigma}[\bra\mathrm{CSF}|a_{m\sigma}^{\dagger}[a_{n\sigma},\hat{H}]|\mathrm{CSF}\ket-\bra\mathrm{CSF}|a_{n\sigma}^{\dagger}[a_{m\sigma},\hat{H}]|\mathrm{CSF}\ket].
\end{align}
Introducing the generalized Fock matrix (where instead of a closed-shell determinant, we now have a general CSF):
\begin{equation}
F_{mn}=\sum_{\sigma}\bra\mathrm{CSF}|a_{m\sigma}^{\dagger}[a_{n\sigma},\hat{H}]|\mathrm{CSF}\ket,
\end{equation}
we can write the electronic gradient as 
\begin{equation}
E_{mn}^{(1)}=2(F_{mn}-F_{nm}).
\end{equation}

\section{The CI model}
We summarize the main points of \cite[Ch. 11]{helgakerMolecularElectronicStructure2000} as there 

\section{The MCSCF model}
The MCSCF \textit{ansatz} is given as
\begin{equation}
    |\Psi_{\mathrm{CASSCF}}\rangle= e^{-\hat{\kappa}} \sum_I c_I|\Phi_I\rangle= e^{-\hat{\kappa}}|\Psi_{\mathrm{CI}}\rangle.
\end{equation}
where $e^{-\hat{\kappa}}$ is the orbital-rotation operator that allows us to tailor the orbitals to the CI expansion, and $\hat{\kappa}$ is given by (\cite[eq. 3.3.22]{helgakerMolecularElectronicStructure2000})
\begin{equation}
    \hat{\kappa}=\sum_{p<q}\kappa_{pq}(\hat{E}_{pq}-\hat{E}_{qp}),
\end{equation}
where the restricted sum represents only the nonredundant orbital rotations in the HF sense \cite[Ch. 10.1.2]{helgakerMolecularElectronicStructure2000}. The MCSCF theory introduces further redundancies that will be discussed further.

The energy of the \textit{ansatz} can be variationally optimized:
\begin{equation}
E=\min_{\bvec{\kappa},\bvec{C}}\frac{\langle \Psi_{\mathrm{CASSCF}}|\hat{H}|\Psi_{\mathrm{CASSCF}}\rangle}{\langle \Psi_{\mathrm{CASSCF}}|\Psi_{\mathrm{CASSCF}}\rangle}.
\end{equation}

We now refine the parameterization of the \textit{ansatz} so that redundancies can be easily controlled \cite[Ch. 11.4.1]{helgakerMolecularElectronicStructure2000}:
\begin{equation}
|\bvec{C}\rangle=e^{-\hat{\kappa}}\frac{|0\rangle + \hat{P}|\bvec{c}\rangle}{\sqrt{1+\langle\bvec{c}|\hat{P}|\bvec{c}\rangle}},
\end{equation}
where the reference state $|0\rangle$ is our current approximation to the electronic state, given by
\begin{equation}
    |0\rangle=\sum_IC_I^{(0)}|\Phi_I\rangle,
\end{equation}
which is normalized to unity, \textit{i.e.}, $\sum_I(C_I^{(0)})^2=1$, whereas the variation state $|\bvec{c}\rangle$ contains the variational parameters:
\begin{equation}
|\bvec{c}\rangle = \sum_Ic_I|\Phi_I\rangle,
\end{equation}
and $\hat{P}$ projects any reference state content from $|\bvec{c}\rangle$:
\begin{equation}
    \hat{P}=1-|0\rangle\langle 0 |.
\end{equation}

\subsection{Taylor expansion of MCSCF energy}
Writing the MCSCF energy as
\begin{equation}
E(\bvec{c},\bvec{\kappa})=\frac{W(\bvec{c},\bvec{\kappa})}{S(\bvec{c},\bvec{\kappa})}, 
\end{equation}
with
\begin{align}
W(\bvec{c},\bvec{\kappa})&=\left[\langle\bvec{c}|\hat{P}+\langle 0| \right]e^{\hat{\kappa}}\hat{H}e^{-\hat{\kappa}}\left[|0\rangle+\hat{P}|\bvec{c}\rangle \right]\\
S(\bvec{c},\bvec{\kappa})&=\left[\langle\bvec{c}|\hat{P}+\langle 0| \right]e^{\hat{\kappa}}e^{-\hat{\kappa}}\left[|0\rangle+\hat{P}|\bvec{c}\rangle \right]=\langle 0|0\rangle+\langle\bvec{c}|\hat{P}|\bvec{c}\rangle=S(\bvec{c}).
\end{align}
We can collect the variational parameters of a general MCSCF wavefunction into a vector
\begin{equation}
\bvec{\lambda}=\binom{\bvec{c}}{\bvec{\kappa}},
\end{equation}
with the reference state in each iteration being the zero vector (using the optimal orbitals from the previous iteration). Clearly this requires and AO to MO integral transformation in each macro-iteration of the MCSCF algorithm.

We aim to develop second-order optimization methods, so we expand the MCSCF energy to second order w.r.t. variational parameters $\bvec{\lambda}$, giving
\begin{equation}
E(\bvec{\lambda})\approx E^{(0)}+\bvec{E}^{(1),\mathrm{T}}\bvec{\lambda}+\frac{1}{2}\bvec{\lambda}^{\mathrm{T}}\bvec{E}^{(2)}\bvec{\lambda}.
\end{equation}
Here $E^{(0)}=\langle 0|\hat{H}|0\rangle$ is the MCSCF energy at the end of the previous iteration, and $\bvec{E}^{(1)}$ and $\bvec{E}^{(2)}$ are the electronic gradient and electronic Hessian respectively, and their block structures are given by
\begin{align}
\bvec{E}^{(1)}&=\tvec{^{\mathrm{c}}\bvec{E}^{(1)}}{^{\mathrm{0}}\bvec{E}^{(1)}} \\
\bvec{E}^{(2)}&=\ttmat{^{\mathrm{cc}}\bvec{E}^{(2)}}{^{\mathrm{co}}\bvec{E}^{(2)}}{^{\mathrm{oc}}\bvec{E}^{(2)}}{^{\mathrm{oo}}\bvec{E}^{(2)}},
\end{align}
where c and o indicate partial differentiation w.r.t. configuration and orbital parameters:
\begin{align}
^{\mathrm{c}}E^{(1)}_i&=\diffp{E}{c_i}[\bvec{\lambda}=\bvec{0}]\\
^{\mathrm{o}}E^{(1)}_{pq}&=\diffp{E}{\kappa_{pq}}[\bvec{\lambda}=\bvec{0}]\\
^{\mathrm{cc}}E^{(2)}_{i,j}&=\diffp{E}{c_i,c_j}[\bvec{\lambda}=\bvec{0}]\\
^{\mathrm{oc}}E^{(2)}_{pq,i}&=^{\mathrm{co}}E^{(2)}_{i,pq}=\diffp{E}{\kappa_{pq},c_i}[\bvec{\lambda}=\bvec{0}]\\
^{\mathrm{oo}}E^{(2)}_{pq,rs}&=\diffp{E}{\kappa_{pq},\kappa_{rs}}[\bvec{\lambda}=\bvec{0}].
\end{align}
The expressions of the gradient and Hessian can be derived by differentiating w.r.t. $\bvec{\lambda}$:
\begin{equation}
S(\bvec{c})E(\bvec{c},\bvec{\kappa})=W(\bvec{c},\bvec{\kappa}),
\end{equation}
which gives (remember $S^{(0)}=1$)
\begin{align}
\bvec{E}^{(1)}&=\bvec{W}^{(1)}-E^{(0)}\bvec{S}^{(1)}\\
\bvec{E}^{(2)}&=\bvec{W}^{(2)}-\bvec{E}^{(1)}\bvec{S}^{(1),\mathrm{T}}-\bvec{S}^{(1)}\bvec{E}^{(1),\mathrm{T}}-E^{(0)}\bvec{S}^{(2)},
\end{align}
note that terms like $\bvec{E}^{(1)}\bvec{S}^{(1),\mathrm{T}}$ represent an outer product, not a dot product. \hl{why has this assumed real orbitals?}. Further simplifications arise since the gradient $\bvec{S}^{(1)}$ vanishes (allowed changes in $\bvec{c}$ must be orthogonal to itself), and only $^{\mathrm{cc}}\bvec{S}^{(2)}$ is nonzero. We can then arrive at the compact expressions for the MCSCF electronic gradient (remembering that we are expanding around $\bvec{\lambda}=\bvec{0}$):
\begin{align}
^{\mathrm{c}}E_i^{(1)}&=2\langle i| \hat{P}\hat{H}|0\rangle=2\langle i|\hat{H}|0\rangle-2C_i^{(0)}E^{(0)}\\
^{\mathrm{o}}E_{pq}^{(1)}&=\langle 0 |[E_{pq}^-,\hat{H}]|0\rangle.
\end{align}
For the derivation of the orbital gradient, see \cite[eq. 10.1.25 and eq. 10S.1.6]{helgakerMolecularElectronicStructure2000}. The Hessian elements are given by
\begin{align}
^{\mathrm{cc}}E_{i,j}^{(2)}&=2\langle i|\hat{P}(\hat{H}-E^{(0)})\hat{P}|j\rangle=2\langle i |\hat{H}-E^{(0)}|j\rangle-C_i^{(0)}E_j^{(1)}-C_j^{(0)}E_i^{(1)}\\
^{\mathrm{oc}}E_{pq,i}^{(2)}&=2\langle i |\hat{P}[E^-_{pq},\hat{H}]|0\rangle=2\langle i |[E^-_{pq},\hat{H}]|0\rangle-2C_i^{(0)}E_{pq}^{(1)}\\
^{\mathrm{oo}}E_{pq,rs}^{(2)}&=\frac{1}{2}(1+\hat{P}_{pq,rs})\langle 0 |[E_{pq}^-,[E_{rs}^-,\hat{H}]]|0\rangle,
\end{align}
where $\hat{P}_{pq,rs}$ permutes $pq$ with $rs$, making the expression symmetric.


\section{Exact two-component theories}
\subsection{The four-component Hamiltonian}
The many electron Dirac--Coulomb(--Breit) Hamiltonian \cite[eq. 9.1, p. 335]{reiherRelativisticQuantumChemistry2014} is given by
\begin{equation}
    \hat{H} = \sum_{i}^N\hat{h}^{\mathrm{D}}(i) + \sum_{i<j}^Ng(i,j).
\end{equation}
This is also known as the four-component Hamiltonian. The one-electron part $h^{\mathrm{D}}$ is given by

\begin{equation}
    \hat{h}^{\mathrm{D}}=
    \begin{bmatrix}
        V_{\mathrm{nuc}} & c(\bvec{\sigma}\cdot\bvec{p})\\
        c(\bvec{\sigma}\cdot\bvec{p}) & V_{\mathrm{nuc}}-2mc^2
    \end{bmatrix},
\end{equation}
or equivalently,
\begin{align}
\label{eq:dirac_algebraic}
    (V_{\mathrm{nuc}}-\epsilon)\psi\lgc+c(\bvec{\sigma}\cdot\bvec{p})\psi\smc&=0\\
    c(\bvec{\sigma}\cdot\bvec{p})\psi\lgc+(V_{\mathrm{nuc}}-\epsilon-2mc^2)\psi\smc&=0.
\end{align}
The two-electron part is one of the following \cite[eqs. 3-7]{liuIdeasRelativisticQuantum2010}, \cite[eq. 6]{kelleyLargescaleDiracFock2013a}:
\begin{align}
    g^{\mathrm{C}}&=\frac{1}{r_{12}}\\
    g^{\mathrm{G}}&=\frac{1}{r_{12}}-\frac{\bvec{\alpha}_1\cdot\bvec{\alpha}_2}{r_{12}}\\
    g^{\mathrm{B}}&=\frac{1}{r_{12}}-\frac{\bvec{\alpha}_1\cdot\bvec{\alpha}_2}{r_{12}}-\frac{[(\bvec{\alpha}_1\cdot\bvec{\nabla}_1)(\bvec{\alpha}_2\cdot\bvec{\nabla}_2)r_{12} ]}{2}\\
    &=\frac{1}{r_{12}}-\frac{\bvec{\alpha}_1\cdot\bvec{\alpha}_2}{2r_{12}}-\frac{(\bvec{\alpha}_1\cdot\bvec{\nabla}_1)(\bvec{\alpha}_2\cdot\bvec{\nabla}_2)}{2r_{12}^3}.
\end{align}

In a mean-field theory, as we will mostly be dealing with, we introduce the Fock operator $\hat{f}$:
\begin{equation}
    \hat{f}=\hat{h}^{\mathrm{D},0}+V_{\mathrm{nuc}}+V_{\mathrm{ee}}\equiv\hat{h}^{\mathrm{D},0}+V,
\end{equation}
where $\hat{h}^{\mathrm{D},0}$ is the field-free Dirac operator (this is done for notational ease), and $V_{\mathrm{ee}}$ is the mean-field electron-electron interaction potential. 

It is very important to realize that in deriving the X2C theory, we assume $V$ is \textit{diagonal}, and that $V^{\mathrm{LL}}=V^{\mathrm{SS}}$. This is importantly \textit{not} the case for Dirac--Hartree--Fock, where exchange integrals contain off-diagonal elements; also, the Breit interaction is \textit{exclusively} off-diagonal. We need to keep these in mind in going through the following derivations (see discussions in \cite[Ch. 11.1.1]{reiherRelativisticQuantumChemistry2014}).


\subsection{Exact coupling operator}
The Foldy-Wouthuysen \cite{foldyDiracTheorySpin1950} transformation looks for a unitary transformation $\hat{\Omega}$ that will zero the small component:
\begin{equation}
    \hat{\Omega}\begin{bmatrix}\psi\lgc\\ \psi\smc\end{bmatrix}=\begin{bmatrix}\psi^{\mathrm{FW}}\\ 0\end{bmatrix},
\end{equation}
which implies
\begin{equation}
    {\Omega}_{21}\psi\lgc+{\Omega}_{22}\psi\smc=0.
\end{equation}
Let's introduce the coupling operator $\hat{ X }$ such that $\psi\smc=\hat{ X }\psi\lgc$, so we may expand $\hat{\Omega}$ as
\begin{equation}
    \hat{\Omega}=\begin{bmatrix}\Omega_{11}&\Omega_{12}\\ -\hat{ X } & I_2\end{bmatrix}\mathrel{\substack{\mathrm{(orth.)}\\\rightarrow}}
    \begin{bmatrix}I_2&\hat{ X }^{\dagger} \\ -\hat{ X } & I_2\end{bmatrix},
\end{equation}
where the last equality uses the fact that we want $\hat{\Omega}$ to be a unitary transformation, but in this step we only enforce the orthogonality. We then normalize the transformation to obtain a unitary transformation. The norm is determined by
\begin{equation}
    \hat{\Omega}^{\dagger}\hat{\Omega}=\begin{bmatrix}I_2&-\hat{ X }^{\dagger} \\ \hat{ X } & I_2\end{bmatrix}\begin{bmatrix}I_2&\hat{ X }^{\dagger} \\ -\hat{ X } & I_2\end{bmatrix}=
    \begin{bmatrix}
        I_2+\hat{ X }^{\dagger}\hat{ X } & 0_2 \\ 0_2 & I_2+\hat{ X }\hat{ X }^{\dagger}.
    \end{bmatrix}
\end{equation}
The overall unitary transformation $\hat{U}$ (which is just $\hat{\Omega}$ normalized) can then be written as
\begin{equation}
\label{eq:fwtrans}
    \hat{U}=\begin{bmatrix}
        \frac{1}{\sqrt{1+\hat{ X }^{\dagger}\hat{ X }}} & \frac{1}{\sqrt{1+\hat{ X }^{\dagger}\hat{ X }}}\hat{ X }^{\dagger} \\
        -\frac{1}{\sqrt{1+\hat{ X }\hat{ X }^{\dagger}}}\hat{ X } & \frac{1}{\sqrt{1+\hat{ X }\hat{ X }^{\dagger}}},
    \end{bmatrix}
\end{equation}
The Dirac equation now becomes
\begin{align}
    \hat{U}\hat{f}\hat{U}^{\dagger}\hat{U}\psi&=\epsilon\hat{U}\psi\\
    \begin{bmatrix}
        \hat{f}^+\ & 0_2\\ 0_2 & \hat{f}^-
    \end{bmatrix}
    \begin{bmatrix}
        \psi^{\mathrm{FW}} \\ 0
    \end{bmatrix}
    &=\epsilon\begin{bmatrix}
        \psi^{\mathrm{FW}} \\ 0
    \end{bmatrix}.
\end{align}
The positive block of the Fock operator, $\hat{f}^+$, is
\begin{equation}
    \hat{f}^+=\frac{1}{\sqrt{1+\hat{ X }^{\dagger}\hat{ X } }}[V+c(\bvec{\sigma}\cdot\bvec{p})\hat{ X }+c\hat{ X }^{\dagger}(\bvec{\sigma}\cdot\bvec{p})+\hat{ X }^{\dagger}(V-2mc^2)\hat{ X } ]\frac{1}{\sqrt{1+\hat{ X }^{\dagger}\hat{ X } }}.
\end{equation}
Similarly, from the definition of $\hat{U}$ in \Cref{eq:fwtrans}, we can also find $\psi^{\mathrm{FW}}$:
\begin{equation}
    \psi^{\mathrm{FW}}=\frac{1}{\sqrt{1+\hat{ X }^{\dagger}\hat{ X }}}(\psi\lgc+\hat{ X }^{\dagger}\psi\smc)=\sqrt{1+\hat{ X }^{\dagger}\hat{ X }}\psi\lgc,
\end{equation}
since $\psi\smc=\hat{ X }\psi\lgc$. From the coupled equation form of the Dirac equation, we can easily see that the unnormalized coupling operator $\hat{ X }$ is
\begin{align}
    \hat{ X }&=(2mc^2-V+\epsilon)^{-1}c(\bvec{\sigma}\cdot\bvec{p})\\
    \hat{ X }^{\dagger}&=c(\bvec{\sigma}\cdot\bvec{p})(2mc^2-V+\epsilon)^{-1}.
\end{align}
However, the exact coupling operator $\hat{ X }$ is dependent on the eigenspectrum of the full Dirac Hamiltonian, including the negative energy states. This means we need to solve the full Dirac equation, making this a rather futile exercise. However, this is the starting point for \textit{elimination techniques}.

Finally, if we wanted to obtain an energy-independent expression for $\hat{ X }$, we can manipulate the Dirac equation to obtain
\begin{align}
\hat{ X }V\psi\lgc+\hat{ X }c(\bvec{\sigma}\cdot\bvec{p})\hat{ X }\psi\lgc&=c(\bvec{\sigma}\cdot\bvec{p})\psi\lgc+V\hat{ X }\psi\lgc-2mc^2\hat{ X }\psi\lgc\\
    &\downarrow\nonumber\\
    \hat{ X }&=\frac{1}{2mc^2}\left\{ c(\bvec{\sigma}\cdot\bvec{p}) -[\hat{ X },V]-\hat{ X }c(\bvec{\sigma}\cdot\bvec{p})\hat{ X } \right\}.
\end{align}
This quadratic operator equation is still just as hard as solving the Dirac equation.

\subsection{Modified Dirac equation}
From the exact coupling condition 
\begin{equation}
    \psi\smc=(2mc^2-V+\epsilon)^{-1}c(\bvec{\sigma}\cdot\bvec{p})\psi\lgc
\end{equation}
we can omit the totally symmetric prefactor, and define the pseudo-large component $\phi\plc$ as
\begin{equation}
    2mc\psi\smc=(\bvec{\sigma}\cdot\bvec{p})\phi\plc.
\end{equation}
This is equivalent to applying a (non-unitary) transformation to the original four-component wave function.
\begin{equation}
    \begin{bmatrix}
        \psi\lgc \\ \psi\smc
    \end{bmatrix}
    =
    \begin{bmatrix}
        I_2 & 0_2 \\
        0_2 & \bvec{\sigma}\cdot\bvec{p}/2mc
    \end{bmatrix}
    \begin{bmatrix}
        \psi\lgc \\
        \phi\plc
    \end{bmatrix},
\end{equation}
where the transformation operator/matrix is $\hat{L}$. Substituting in and applying $\hat{L}^{\dagger}$ to the original Dirac equation from the left gives
\begin{align}
    &\begin{bmatrix}
        V & T \\
        T & W_0-T
    \end{bmatrix}
    \begin{bmatrix}
        \psi\lgc \\
        \phi\plc
    \end{bmatrix}=
    \begin{bmatrix}
        I_2 & 0_2 \\
        0_2 & T/2mc^2
    \end{bmatrix}
    \begin{bmatrix}
        \psi\lgc \\
        \phi\plc
    \end{bmatrix}\epsilon,\\
    &T=\frac{\bvec{p}^2}{2m},\ W_0=\frac{1}{4m^2c^2}(\bvec{\sigma}\cdot\bvec{p})V(\bvec{\sigma}\cdot\bvec{p})=[\bvec{p}V\cdot\bvec{p} +i\bvec{\sigma}\cdot(\bvec{p}V\times\bvec{p})]/4m^2c^2 .
\end{align}
This is the modified Dirac equation in its matrix form. Notice the appearance of the metric $\hat{G}$ on the right. In the coupled equation form, we have
\begin{align}
    (V-\epsilon)\psi\lgc+T\phi\plc&=0\\
    T\psi\lgc+\left[ \frac{1}{4m^2c^2}(\bvec{\sigma}\cdot\bvec{p})(V-\epsilon)(\bvec{\sigma}\cdot\bvec{p})-T \right]\phi\plc&=0
\end{align}
The large component $\psi\lgc$ and the pseudo-large $\phi\plc$ component now have the same symmetry properties, and can be expanded in the same basis set. This is why the modified Dirac equation, although completely equivalent to the original equation, is more advantageous.

\subsection{Basis set expansion}
Introducing a two-component basis set $\{ \chi _{\mu}\}$, in which both the large and pseudo-large components can be expanded:
\begin{align}
    &\psi\lgc_i(\bm{r})=\sum_{\mu}c_{i\mu}\lgc \chi _{\mu}(\bm{r})\\
    &\phi\plc_i(\bm{r})=\sum_{\mu}c_{i\mu}\plc \chi _{\mu}(\bm{r}),
\end{align}
where $c\plc$ indicates the pseudo-large component coefficients. The modified Dirac equation can then be cast into a Roothaan-type matrix form:
\begin{equation}
\ttmat{\bm{V}}{\bm{T}}{\bm{T}}{\bm{W}_0-\bm{T}}\ttmat{\bm{c}_+\lgc}{\bm{c}_-\lgc}{\bm{c}_+\plc}{\bm{c}_-\plc}
=\ttmat{\bm{S}}{\bm{0}}{\bm{0}}{\bm{T}/2mc^2}\ttmat{\bm{c}_+\lgc}{\bm{c}_-\lgc}{\bm{c}_+\plc}{\bm{c}_-\plc}\ttmat{\bm{\epsilon}_+}{\bm{0}}{\bm{0}}{\bm{\epsilon}_-}.
\end{equation}

\subsection{One step X2C}
We are now ready to discuss the algorithm behind one-step X2C \cite{iliasInfiniteorderTwocomponentRelativistic2007a}. 

\begin{enumerate}
\item Set up the one-particle Dirac equation \textit{without} the electron-electron interaction potential (this is an approximation, as self consistency is required otherwise, see discussion in II.D of \cite{iliasInfiniteorderTwocomponentRelativistic2007a})
\begin{equation}
    h^{\mathrm{AO}}c_i^{\mathrm{AO}}=S^{\mathrm{AO}}c_i^{\mathrm{AO}}\epsilon_i.
\end{equation}
This step uses uncontracted Gaussian primitives and the small component basis set is generated by UKB.
\item Enforce RKB while maintaining orthogonality, carried out in three sub-steps
\begin{enumerate}
\item Canonical orthonormalization:
\begin{equation}
    h^{\mathrm{MO}}c_i^{\mathrm{MO}}=c_i^{\mathrm{MO}}\epsilon_i,
\end{equation}
where $h^{\mathrm{MO}}=V_1^{\dagger}h^{\mathrm{AO}}V_1$ and $I=V_1^{\dagger}S^{\mathrm{AO}}V_1$.
\item Transfer to the modified form of the equation using the transformation matrix $L$:
\begin{equation}
    {h}^{\mathrm{MO,mod}}{c}_i^{\mathrm{MO,mod}}={G}{c}_i^{\mathrm{MO,mod}}\epsilon_i,
\end{equation}
\item Orthonormalize again:
\begin{equation}
\label{eq:x2c}
    h^{\mathrm{MO}}c_i^{\mathrm{MO}}=c_i^{\mathrm{MO}}\epsilon_i,
\end{equation}
where $h^{\mathrm{MO}}=V_2^{\dagger}{h}^{\mathrm{MO,mod}}V_2$ and $I=V_2^{\dagger}GV_2$.
\end{enumerate}
Overall, we have essentailly performed an orthonormalization step with the RKB transformation embedded, \textit{i.e.},
\begin{equation}
    V=V_1V_2=\ttmat{V^{\mathrm{LL}}}{0}{0}{V^{\mathrm{SS}}}.
\end{equation}
Note that the $V$ here is not the potential, but the orthonormalization matrix.
\item We solve the eigenvalue problem in \Cref{eq:x2c}, obtaining the coefficient matrix $C$. We can write the coupling equations as two systems of linear equations:
\begin{align}
    C\plc_+&=XC\lgc_+\\
    C\lgc_-&=-X^{\dagger}C\plc_-.
\end{align}
The upper system can be directly inverted, but in practice it is better to use Cholesky decomposition with a positive definite Hermitian matrix, therefore we can take the Hermitian conjugate of the second system and premultiply with $C\plc_-$ to give
\begin{equation}
\label{eq:x_expr}
    \underbrace{[C\plc_- C_-^{\mathrm{P}\dagger}]}_{\equiv A}X = \underbrace{-[C_-\plc C_-^{\mathrm{L}\dagger}]}_{\equiv B} \ \ \rightarrow\ AX=B.
\end{equation}
We can then construct the decoupling transformation matrix $U$ from $X$.
\item We construct the transformation matrix $U$:
\begin{equation}
    U=W_1W_2=\ttmat{1}{-X^{\dagger}}{X}{1}\ttmat{ (1+X^{\dagger}X)^{-1/2} }{0}{0}{ (1+XX^{\dagger})^{-1/2} }.
\end{equation}
We also extract the positive-energy two-component Hamiltonian in the MO basis, $h^{\mathrm{MO}}_+$, which is the upper-left block.
\item The Hamiltonian is projected back onto the large component AO basis:
\begin{equation}
    h_+^{\mathrm{AO}}=[V^{\mathrm{LL},\dagger}]^{-1}h_+^{\mathrm{MO}}[V^{\mathrm{LL}}]^{-1},\ [V^{\mathrm{LL}}]^{-1}=V^{\mathrm{LL},\dagger}S^{\mathrm{LL, AO}}.
\end{equation}
This is the starting point for calculations in two-component mode.
\end{enumerate}

\subsection{The two-electron interaction}
So far we have made no approximation, and the X2C Hamiltonian exactly reproduces the eigenvalues of the four-component \textit{one-particle} Dirac equation. However, compared to four-component mean-field theories, where $V=V_{\mathrm{nuc}}+V_{\mathrm{ee}}$, we have essentially made the approximation that $V\rightarrow V_{\mathrm{nuc}}$.

The most consistent choice at this point (which would be an approximation nonetheless), would be to transform the four-component two-electron operator using the same decoupling transformation as follows \cite[eq. 14.57]{reiherRelativisticQuantumChemistry2014}:
\begin{equation}
    f_+\approx \left( U[V_{\mathrm{nuc}}](h^{\mathrm{D}}+V_{\mathrm{nuc}}+V_{\mathrm{ee}}[\psi_i^{(+)}])U[V_{\mathrm{nuc}}]^{\dagger} \right)^{++}.
\end{equation}
Alternatively, this is to say, the four-component two-electron integrals should be transformed as \cite{sikkemaMolecularMeanfieldApproach2009,saueRelativisticHamiltoniansChemistry2011}
\begin{align}
    &[U(1)\otimes U(2)]^{\dagger}G(1,2)[U(1)\otimes U(2)]\\
    =&\begin{bmatrix}
        \tilde{G}^{++}_{++} & \tilde{G}^{+-}_{++} & \tilde{G}^{-+}_{++} & \tilde{G}^{--}_{++}\\
        \tilde{G}^{++}_{+-} & \tilde{G}^{+-}_{+-} & \tilde{G}^{-+}_{+-} & \tilde{G}^{--}_{+-}\\
        \tilde{G}^{++}_{-+} & \tilde{G}^{+-}_{-+} & \tilde{G}^{-+}_{-+} & \tilde{G}^{--}_{-+}\\
        \tilde{G}^{++}_{--} & \tilde{G}^{+-}_{--} & \tilde{G}^{-+}_{--} & \tilde{G}^{--}_{--}
    \end{bmatrix},
\end{align}
where $G(1,2)$ is the supermatrix of two-electron integrals at the four-component level containing integrals $G^{QS}_{PR}$. We would retain the $\tilde{G}^{++}_{++}$ block as the two-component two-electron integrals. This transformation unfortunately is more expensive than the four-component calculation. 

Oftentimes, the untransformed two-electron operator is used, which is equivalent to the approximation of
\begin{equation}
    f_+\approx \left( U[V_{\mathrm{nuc}}](h^{\mathrm{D}}+V_{\mathrm{nuc}})U[V_{\mathrm{nuc}}]^{\dagger} \right)^{++}
    +V_{\mathrm{ee}}[\tilde{\psi}_i^{(L)}],
\end{equation}
which is in turn equivalent to using only the $\langle \mathrm{LL}|\mathrm{LL}\rangle$ class of four-component integrals \cite[eq. 28]{sikkemaMolecularMeanfieldApproach2009}, which is a complete neglect of two-electron picture change \cite{sikkemaMolecularMeanfieldApproach2009}. This approximation is commonly carried out, and works reasonably well for valence electron properties, but all two-electron spin-orbit coupling effect is lost, therefore, for example, this cannot resolve spin-orbit splittings in the fine structure of atomic spectra.

\subsection{Molecular mean field by Sikkema et al. (2009)}
(The description in Knecht et al. \cite{knechtExactTwocomponentHamiltonians2022} is clearer than the original paper \cite{sikkemaMolecularMeanfieldApproach2009} or the review paper on the same topic \cite{saueRelativisticHamiltoniansChemistry2011}). In view of the affordability of even molecular DHF calculations compared to correlated calculations, the mmfX2C approach uses the converged DHF Fock matrix to define the decoupling transformation $U$, which is then combined with the untransformed two-electron operator in correlated calculations. The converged DHF Fock matrix carries infinite-order molecular mean-field spin-orbit corrections \cite{liuInfiniteorderQuasirelativisticDensity2006,pengMakingFourTwocomponent2007}.

To be precise, we find the decoupling transformation $U$ that block-diagonalizes the Fock matrix in the orthonormalized atomic basis, and then transform the MO coefficients with it:
\begin{equation}
    \tilde{F}_{\mu\nu}^{\mathrm{2c}}=\left[U^{\dagger}F^{\mathrm{4c}}U  \right]_{\mu\nu}^{\mathrm{LL}},\ \ \tilde{c}_{\mu i}^{\mathrm{2c}}=\left[ U^{\dagger}c^{\mathrm{4c}} \right]_{\mu i}^{\mathrm{L}+},
\end{equation}
where $U$ is built from the matrix $X$, which is can be computed from the converged MO coefficients by \Cref{eq:x_expr}. The picture-changed two-component coefficients are then used to transform the $\langle \mathrm{LL}|\mathrm{LL}\rangle$ integrals. Note that this \textit{does not} amount to the complete neglect of two-electron picture-change, as the picture-changed MO coefficients carry mean-field spin-orbit corrections to infinite order. The time savings in the four-index transformation step is the main attraction of the mmfX2C method.

\subsection{X2CAMF by Liu and Cheng (2018)}
(The first paper \cite{liuAtomicMeanfieldSpinorbit2018} has good introduction and thorough derivations, although towards the end it got a little confusing, the second paper \cite{zhangAtomicMeanFieldApproach2022} has better organization, and even more thorough computational details.)
\subsubsection{Definition of integrals}
The fully transformed four-component Coulomb integrals are given by (Einstein summation is assumed unless explicit summation is shown):
\begin{align}
    \langle pq|\frac{1}{r_{12}}|rs\rangle&=c^{\mathrm{L}*}_{\mu p}c^{\mathrm{L}*}_{\nu q}c^{\mathrm{L}}_{\rho r}c^{\mathrm{L}}_{\lambda s}g^{\mathrm{C, LLLL}}_{\mu\rho\nu\lambda} + c^{\mathrm{L}*}_{\mu p}c^{\mathrm{S}*}_{\nu q}c^{\mathrm{L}}_{\rho r}c^{\mathrm{S}}_{\lambda s}g^{\mathrm{C, LLSS}}_{\mu\rho\nu\lambda}\nonumber\\
    &+c^{\mathrm{S}*}_{\mu p}c^{\mathrm{L}*}_{\nu q}c^{\mathrm{S}}_{\rho r}c^{\mathrm{L}}_{\lambda s}g^{\mathrm{C, SSLL}}_{\mu\rho\nu\lambda}+c^{\mathrm{S}*}_{\mu p}c^{\mathrm{S}*}_{\nu q}c^{\mathrm{S}}_{\rho r}c^{\mathrm{S}}_{\lambda s}g^{\mathrm{C, SSSS}}_{\mu\rho\nu\lambda},
\end{align}
note that the MO integrals are defined in the physicists' notation, where the AO integrals are in Mulliken notation, for example:
\begin{equation}
    g^{\mathrm{C, LLSS}}_{\mu\rho\nu\lambda}=\frac{1}{4c^2}(\mu\rho|(\bvec{\sigma}\cdot\bvec{p})\nu(\bvec{\sigma}\cdot\bvec{p})\lambda).
\end{equation}
Applying the Dirac relation, we can partition the integrals into spin-free (SF) and spin-dependent (SD) parts:
\begin{align}
    [(\bvec{\sigma}\cdot\bvec{p})A][(\bvec{\sigma}\cdot\bvec{p})B]&=(\bvec{p}A)\cdot(\bvec{p}B)+i\bvec{\sigma}\cdot[(\bvec{p}A)\times(\bvec{p}B)]\\
    &=\underbrace{(\bvec{p}A)\cdot(\bvec{p}B)}_{\mathrm{spin-free}}+\underbrace{i\epsilon_{uvw}(\bvec{p}A)_{v}\sigma_u(\bvec{p}B)_w}_{\mathrm{spin-dependent}},
\end{align}
where $u,v,w\in\{x,y,z\}$. We can define the SF integrals as
\begin{align}
    g^{\mathrm{C, LLLL, SF}}_{\mu\rho\nu\lambda} &= (\mu\rho|\nu\lambda),\\
    g^{\mathrm{C, LLSS, SF}}_{\mu\rho\nu\lambda} &= \frac{1}{4c^2}(\mu\rho|(\bvec{p}\nu)\cdot(\bvec{p}\lambda)),\\
    g^{\mathrm{C, SSLL, SF}}_{\mu\rho\nu\lambda} &= \frac{1}{4c^2}((\bvec{p}\mu)\cdot(\bvec{p}\rho)|\nu\lambda),\\
    g^{\mathrm{C, SSSS, SF}}_{\mu\rho\nu\lambda} &= \frac{1}{16c^4}((\bvec{p}\mu)\cdot(\bvec{p}\rho)|(\bvec{p}\nu)\cdot(\bvec{p}\lambda)).
\end{align}
The SD integrals are more complicated as the dot and cross products can produce cross terms in the $(\mathrm{SS}|\mathrm{SS})$ case:
\begin{align}
g^{\mathrm{C, LLSS, SD}}_{\mu\rho\nu\lambda}=&\frac{1}{4c^2}( \mu\rho|i\epsilon_{uvw}p_v\nu\sigma_up_w\lambda ),\label{eq:sd_ao_llss}\\
g^{\mathrm{C, SSLL, SD}}_{\mu\rho\nu\lambda}=&\frac{1}{4c^2}(i\epsilon_{uvw}p_v\mu\sigma_up_w\rho|\nu\lambda ),\label{eq:sd_ao_ssll}\\
g^{\mathrm{C, SSSS, SD}}_{\mu\rho\nu\lambda}=&\frac{1}{16c^4}(i\epsilon_{uvw}p_v\mu\sigma_up_w\rho|(\bvec{p}\nu)\cdot(\bvec{p}\lambda))\nonumber\\
&+\frac{1}{16c^4}((\bvec{p}\mu)\cdot(\bvec{p}\rho)|i\epsilon_{uvw}p_v\nu\sigma_up_w\lambda)\nonumber\\
&+\frac{1}{16c^4}(i\epsilon_{uvw}p_v\mu\sigma_up_w\rho|i\epsilon_{u'v'w'}p_{v'}\nu\sigma_{u'}p_{w'}\lambda).\label{eq:sd_ao_ssss}
\end{align}
Note that there are no $(\mathrm{LL}|\mathrm{LL})$ class for the SD integrals.

\subsubsection{The approximations}
The central approximation is to approximate the SD integrals in a mean-field manner, reducing them into two-index integrals. This is equivalent to writing (we focus on the simple case of the DC Hamiltonian) \hl{the $g$ here, from the 2022 paper, are antisymmetrized, which is inconsistent with the 2018 paper}
\begin{align}
H^{\mathrm{DC}}&=\sum_{pq}h_{pq}^{\mathrm{4c}}\ahat_p^{\dagger}\ahat_q+\frac{1}{4}\sum_{pqrs}(g_{pq,rs}^{\mathrm{C,SD}}+g_{pq,rs}^{\mathrm{C,SF}})\ahat_p^{\dagger}\ahat_q^{\dagger}\ahat_s\ahat_r\\
&\approx \sum_{pq}(h_{pq}^{\mathrm{4c}} +\sum_in_ig_{pi,qi}^{\mathrm{C,SD}})\ahat_p^{\dagger}\ahat_q+\frac{1}{4}\sum_{pqrs}g_{pq,rs}^{\mathrm{C,SF}}\ahat_p^{\dagger}\ahat_q^{\dagger}\ahat_s\ahat_r.
\end{align}
Here $\phi_i$ are a general set of occupied spinors. This is a generic expression of the mean-field approximation, as it can be seen to be analogous to that in the Hartree--Fock approximation, where the Fock operator contains the $\langle pi||qi\rangle$ class of two-electron integrals.

Note that while we are writing the Hamiltonian in the MO basis for illustration purposes, we need the matrix elements and integrals in the AO basis to carry out the X2CAMF SCF iterations themselves. Again this is akin to formulating the HF approximation in the putative MO basis first.

To further exploit the locality of spin-orbit interaction, and avoid evaluating the molecular relativistic two-electron integrals, the atomic mean-field (AMF) approximation is introduced:
\begin{align}
H^{\mathrm{DC,AMF}}&\equiv\sum_{pq}(h_{pq}^{\mathrm{4c}}+h_{pq}^{\mathrm{AMF}})\ahat_p^{\dagger}\ahat_q+\frac{1}{4}\sum_{pqrs}g_{pq,rs}^{\mathrm{C,SF}}\ahat_p^{\dagger}\ahat_q^{\dagger}\ahat_s\ahat_r\\
&=\sum_{pq}(h_{pq}^{\mathrm{4c}} +\sum_A\sum_{i_A}n_{i_A}g_{p{i_A},q{i_A}}^{\mathrm{C,SD,A}})\ahat_p^{\dagger}\ahat_q+\frac{1}{4}\sum_{pqrs}g_{pq,rs}^{\mathrm{C,SF}}\ahat_p^{\dagger}\ahat_q^{\dagger}\ahat_s\ahat_r,
\end{align}
where $A$ runs over atoms in the molecule, and $i_A$ are occupied (`molecular'/LCAO/DHF) spinors in atom $A$. It is more practical to write the AMF integrals in the AO basis:
\begin{equation}
    h^{\mathrm{AMF}}_{pq}=\sum_A\sum_{\mu_A\nu_A}c^{\mathrm{4c},*}_{\mu_AP}c^{\mathrm{4c}}_{\nu_Aq}h_{\mu_A\nu_A}^{\mathrm{AMF}},
\end{equation}
the exact expressions for the AO basis AMF integrals are given in the next subsection.

Note that so far we have been working in the 4c picture. We extract the transformation matrix $U$ from the bare nuclear potential and picture change $h^{\mathrm{4c}}$ into $h^{\mathrm{X2C-1e}}$. And $h^{\mathrm{AMF}}$ is picture changed by using the $U$ matrices derived from atomic converged DHF Fock matrices, and then cast back to the AO basis (as usual), and the entire 1e operator $h^{\mathrm{X2CAMF}}=h^{\mathrm{X2C-1e}}+h^{\mathrm{2c,AMF}}$ in the AO basis is used in the SCF calculation.

We introduce the further approximation of ignoring scalar two-electron picture-change effects. This is practically done by only using the LLLL class of AO integrals (cf. \cite[eq. 28]{sikkemaMolecularMeanfieldApproach2009}).

\subsubsection{AO basis AMF integral expressions}
In this subsection we give the expressions of the matrix elements of $h_{\mu_A\nu_A}^{\mathrm{AMF}}$.
The AO two-center integrals are contractions of density matrices and atomic two-electron integrals:
\begin{align}
h_{\mu\rho}^{\mathrm{AMF, LL}}&=D_{\nu\lambda}^{\mathrm{SS}}g^{\mathrm{C, LLSS, SD}}_{\mu\rho\nu\lambda},\\
h_{\mu\rho}^{\mathrm{AMF, LS}}&=-D_{\nu\lambda}^{\mathrm{SL}}g^{\mathrm{C, LLSS, SD}}_{\mu\lambda\nu\rho},\\
h_{\mu\rho}^{\mathrm{AMF, SL}}&=-D_{\nu\lambda}^{\mathrm{LS}}g^{\mathrm{C, SSLL, SD}}_{\mu\lambda\nu\rho},\\
h_{\mu\rho}^{\mathrm{AMF, SS}}&=D_{\nu\lambda}^{\mathrm{SS}}g^{\mathrm{C, SSSS, SD}}_{\mu\rho\nu\lambda}-D_{\nu\lambda}^{\mathrm{SS}}g^{\mathrm{C, SSSS, SD}}_{\mu\lambda\nu\rho},
\end{align}
where the definitions of $g^{\mathrm{C, UVWX, SD}}$ are those given in \Cref{eq:sd_ao_llss,eq:sd_ao_ssll,eq:sd_ao_ssss}\hl{reverting back to non-antisymmetrized integrals again!}. Remember that the SD integrals do not have a LLLL class. The density matrices are the converged atomic DHF ones:
\begin{equation}
    D_{\nu\lambda}^{\mathrm{XY}}=n_ic_{\nu i}^{\mathrm{4c,X}*}c^{\mathrm{4c,Y}}_{\lambda i}.
\end{equation}
These integrals are then picture changed into the two-component form:
\begin{equation}
    h^{\mathrm{2c,AMF}}_{\mu\rho}=[R^{\dagger}(h^{\mathrm{AMF,LL}}+h^{\mathrm{AMF,LS}}X+X^{\dagger}h^{\mathrm{AMF,SL}}+X^{\dagger}h^{\mathrm{AMF,SS}}X)R]_{\mu\rho},
\end{equation}
where the $R$ and $X$ are extracted from the the converged atomic Fock matrix.


\subsection{amfX2C and eamfX2C by Knecht et al. (2022)}
The paper \cite{knechtExactTwocomponentHamiltonians2022} first goes to prove that, provided we start from the correctly transformed set of integrals, we can perform \textit{SCF iterations themselves} in 2c mode and still converge onto the correctly transformed coefficients, orbital energies, as well as the total energy. The glaring problem is that, of course, the decoupling transformation $U$ is not available at the outset. To summarize, the paper showed that 2c Fock matrix can be expressed as
\begin{equation}
\tilde{F}_{\mu\nu}^{\mathrm{2c}}=\tilde{h}^{\mathrm{2c}}_{\mu\nu}+\sum_{\kappa\lambda}\tilde{G}^{\mathrm{2c}}_{\mu\nu,\kappa\lambda}\tilde{D}_{\lambda\kappa}^{\mathrm{2c}},
\end{equation}
and that the correct 4c energy can be expressed in terms of correctly transformed 2c quantities:
\begin{equation}
E^{\mathrm{4c}}=\sum_{\mu\nu}\tilde{h}^{\mathrm{2c}}_{\mu\nu}\tilde{D}_{\nu\mu}^{\mathrm{2c}}+\frac{1}{2}\sum_{\mu\nu\kappa\lambda}\tilde{D}_{\nu\mu}^{\mathrm{2c}}\tilde{G}^{\mathrm{2c}}_{\mu\nu,\kappa\lambda}\tilde{D}_{\lambda\kappa}^{\mathrm{2c}}\equiv\tilde{E}^{\mathrm{2c,1e}}+\tilde{E}^{\mathrm{2c,2e}}.
\end{equation}
As the two-electron picture change is the reason we have to introduce approximations in the first place (\hl{justify!}), the approximation needs to deal with the two-electron picture change effect. In this case, this is done by targeting the differential Fock matrix:
\begin{equation}
\Delta\tilde{F}^{\mathrm{2c}}_{\mu\nu}\equiv\tilde{F}^{\mathrm{2c}}_{\mu\nu}-{F}^{\mathrm{2c}}_{\mu\nu},
\end{equation}
where ${F}^{\mathrm{2c}}_{\mu\nu}$ is the 2c Fock matrix using nonpicture changed two electron integrals:
\begin{equation}
{F}_{\mu\nu}^{\mathrm{2c}}=\tilde{h}^{\mathrm{2c}}_{\mu\nu}+\sum_{\kappa\lambda}{G}^{\mathrm{2c}}_{\mu\nu,\kappa\lambda}\tilde{D}_{\lambda\kappa}^{\mathrm{2c}}.
\end{equation}
This means that the differential Fock matrix is simply
\begin{equation}
\Delta\tilde{F}^{\mathrm{2c}}_{\mu\nu}=\sum_{\kappa\lambda}\Delta\tilde{G}^{\mathrm{2c}}_{\mu\nu,\kappa\lambda}\tilde{D}_{\lambda\kappa}^{\mathrm{2c}},
\end{equation}
and the two-electron HF energy can also be expressed in terms of differential two-electron integrals:
\begin{equation}
\tilde{E}^{\mathrm{2c,2e}}=\frac{1}{2}\sum_{\mu\nu}\tilde{D}^{\mathrm{2c}}_{\nu\mu}\left( {F}^{\mathrm{2c,2e}}_{\mu\nu}+\Delta\tilde{F}^{\mathrm{2c,2e}}_{\mu\nu} \right).
\end{equation}
The differential Fock matrix is approximated by a superposition (\textit{i.e.}, direct sum) of atomic differential Fock matrices, which can be computed exactly from atomic 4c HF calculations:
\begin{equation}
\Delta\tilde{F}^{\mathrm{2c,2e}}\approx\Delta\tilde{F}_{\oplus}^{\mathrm{2c,2e}}\equiv\bigoplus_{K=1}^M\Delta\tilde{F}^{\mathrm{2c}}_K,
\end{equation}
where $K$ rungs over all $M$ atoms in the system. The above equation is in the atomic AO basis rather than the orthonormalized basis to avoid the mixing of basis functions belonging to different atomic centers. The summary of the algorithm is given below:
\begin{enumerate}
    \item For each atomic type (\textit{i.e.}, only once per atom type), perform a 4c SCF calculation, and extract the following quantities:
    \begin{enumerate}
    \item The converged atomic Fock matrix $F^{\mathrm{4c}}_K$ is exactly block-diagonalized to give $\tilde{F}^{\mathrm{2c}}_K$, $\tilde{c}^{\mathrm{2c}}_K$, and $\tilde{D}^{\mathrm{2c}}_K$.
    \item Build the 2c Fock matrix with nonpicture changed two electron integrals, $F^{\mathrm{2c}}_{K}$.
    \item Build the differential Fock matrix $\Delta\tilde{F}^{\mathrm{2c}}_K$.
    \end{enumerate}
    \item From the atomic blocks of $\Delta\tilde{F}^{\mathrm{2c}}_K$ and $F^{\mathrm{4c,2e}}_K$, we form the approximate molecular counterparts by taking the appropriate direct sums:
    \begin{align}
    &\Delta\tilde{F}^{\mathrm{2c,2e}}\approx\Delta\tilde{F}_{\oplus}^{\mathrm{2c,2e}}\equiv\bigoplus_{K=1}^M\Delta\tilde{F}^{\mathrm{2c}}_K,\\
    &{F}^{\mathrm{4c,2e}}\approx{F}_{\oplus}^{\mathrm{4c,2e}}\equiv\bigoplus_{K=1}^M{F}^{\mathrm{4c,2e}}_K.
    \end{align}
    \item The molecular decoupling matrix $U$ is built, proceeding from diagonalizing $\tilde{F}^{\mathrm{4c}}=h^{\mathrm{4c}}+F_{\oplus}^{\mathrm{4c,2e}}$, this is used to picture change $h^{\mathrm{4c}}$. Remember the reason we were doing all this was that the exact molecular Fock matrix was not available, requiring approximations.
    \item Solve the 2c SCF equation with the amfX2C Fock operator:
    \begin{equation}
    F^{\mathrm{2c}}c^{\mathrm{2c}}=c^{\mathrm{2c}}\epsilon^{\mathrm{2c}},
    \end{equation}
    where
    \begin{equation}
    F^{\mathrm{2c}}_{\mu\nu}\equiv F^{\mathrm{amfX2C}}_{\mu\nu}=[U^{\dagger}h^{\mathrm{4c}}U]^{\mathrm{LL}}_{\mu\nu}+\Delta\tilde{F}^{\mathrm{2c,2e}}_{\oplus,\mu\nu}+F^{\mathrm{2c,2e}}_{\mu\nu}.
    \end{equation}
\end{enumerate}

\bibliographystyle{apalike}
\bibliography{main}

\end{document}
